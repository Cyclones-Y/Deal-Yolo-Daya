import io
import importlib.util
import json
import os
import hashlib
import math
import time
import queue
import random
import re
import threading
import shutil
import zipfile
from contextlib import redirect_stdout, redirect_stderr
from datetime import datetime
from pathlib import Path
import logging
import pandas as pd
import streamlit as st
import streamlit.components.v1 as components

# === Merged utils/display.py ===
import json
import math

import pandas as pd
import streamlit as st


def safe_dataframe(df: pd.DataFrame, **kwargs):
    def _coerce_cell(value):
        if value is None:
            return ""
        try:
            if isinstance(value, float) and math.isnan(value):
                return ""
        except Exception:
            pass
        if isinstance(value, (dict, list)):
            return json.dumps(value, ensure_ascii=False)
        if isinstance(value, (bytes, bytearray)):
            try:
                return value.decode("utf-8", errors="ignore")
            except Exception:
                return value.hex()
        return str(value)

    safe_df = df.copy()
    for col in safe_df.columns:
        if safe_df[col].dtype == "object":
            safe_df[col] = safe_df[col].apply(_coerce_cell)
    try:
        st.dataframe(safe_df, **kwargs)
    except Exception:
        fallback = safe_df.astype(str, errors="ignore")
        st.dataframe(fallback, **kwargs)
        st.caption("å·²å°†æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬ä»¥ä¾¿å±•ç¤ºã€‚")
# === Merged process_step.py ===
import json
import copy
import os
from pathlib import Path
from typing import Optional

import pandas as pd
import requests
from PIL import Image, ImageDraw, ImageFont
import re


def merge_all_csv_in_folder(
        folder_path,
        output_file="merged_csv.csv",
        encoding="utf-8-sig",
        chunk_size: int = 100000,
        progress_callback=None,
):
    """
    åˆå¹¶æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰CSVæ–‡ä»¶

    :param folder_path: åŒ…å«CSVæ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„
    :param output_file: åˆå¹¶åçš„è¾“å‡ºæ–‡ä»¶åï¼ˆé»˜è®¤"merged_csv.csv"ï¼‰
    :param encoding: æ–‡ä»¶ç¼–ç ï¼ˆé»˜è®¤"utf-8-sig"ï¼Œå…¼å®¹ä¸­æ–‡å’ŒBOMå¤´ï¼‰
    :param chunk_size: åˆ†å—è¯»å–è¡Œæ•°ï¼ˆé»˜è®¤100000ï¼‰
    :param progress_callback: è¿›åº¦å›è°ƒï¼ˆfile_idx, total_files, filename, total_rows, file_rows, chunk_idxï¼‰
    :return: åˆå¹¶åçš„æ€»è¡Œæ•°
    """
    # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(folder_path):
        raise FileNotFoundError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼š{folder_path}")

    # è·å–æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰CSVæ–‡ä»¶çš„è·¯å¾„
    csv_files = list(Path(folder_path).glob("*.csv"))
    if not csv_files:
        print(f"è­¦å‘Šï¼šæ–‡ä»¶å¤¹ {folder_path} ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")
        return None

    print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶ï¼Œå¼€å§‹åˆå¹¶...")

    output_file = str(output_file)
    Path(output_file).parent.mkdir(parents=True, exist_ok=True)
    header_written = False
    total_rows = 0
    total_bytes = sum(f.stat().st_size for f in csv_files)
    completed_bytes = 0

    for file_idx, csv_file in enumerate(csv_files, start=1):
        try:
            file_size = csv_file.stat().st_size
            if progress_callback:
                progress_callback(
                    file_idx,
                    len(csv_files),
                    csv_file.name,
                    total_rows,
                    0,
                    0,
                    file_size,
                    0,
                    total_bytes,
                    completed_bytes,
                )
            # åˆ†å—è¯»å–CSVæ–‡ä»¶ï¼Œé¿å…ä¸€æ¬¡æ€§å æ»¡å†…å­˜
            file_rows = 0
            with open(csv_file, "r", encoding=encoding, errors="ignore") as f:
                chunk_iter = pd.read_csv(
                    f,
                    parse_dates=False,  # é¿å…è‡ªåŠ¨è§£ææ—¥æœŸå¯¼è‡´é”™è¯¯
                    chunksize=chunk_size,
                )
                for chunk_idx, df in enumerate(chunk_iter, start=1):
                    df["source_file"] = os.path.basename(csv_file)
                    mode = "w" if not header_written else "a"
                    header = not header_written
                    df.to_csv(output_file, index=False, encoding=encoding, mode=mode, header=header)
                    header_written = True
                    rows = len(df)
                    file_rows += rows
                    total_rows += rows
                    file_bytes = f.tell()
                    total_bytes_read = completed_bytes + file_bytes
                    if progress_callback:
                        progress_callback(
                            file_idx,
                            len(csv_files),
                            csv_file.name,
                            total_rows,
                            file_rows,
                            chunk_idx,
                            file_size,
                            file_bytes,
                            total_bytes,
                            total_bytes_read,
                        )
            print(f"æˆåŠŸè¯»å–ï¼š{csv_file.name}ï¼ˆ{file_rows}è¡Œï¼‰")
            completed_bytes += file_size
        except Exception as e:
            print(f"è¯»å–å¤±è´¥ {csv_file.name}ï¼š{str(e)}")
            continue

    if not header_written:
        print("é”™è¯¯ï¼šæ²¡æœ‰å¯åˆå¹¶çš„æœ‰æ•ˆCSVæ•°æ®")
        return None

    print(f"\nåˆå¹¶å®Œæˆï¼å…± {total_rows} è¡Œæ•°æ®")
    print(f"è¾“å‡ºæ–‡ä»¶ï¼š{os.path.abspath(output_file)}")
    return total_rows


# # ---------------------- ä½¿ç”¨ç¤ºä¾‹ ----------------------
# if __name__ == "__main__":
#     # æ›¿æ¢ä¸ºä½ çš„CSVæ–‡ä»¶å¤¹è·¯å¾„
#     csv_folder = "æ ‡æ³¨ç»“æœ 2"
#     # åˆå¹¶CSVï¼ˆè¾“å‡ºæ–‡ä»¶é»˜è®¤åœ¨å½“å‰ç›®å½•ï¼Œå¯è‡ªå®šä¹‰è·¯å¾„ï¼‰
#     merge_all_csv_in_folder(
#         folder_path=csv_folder,
#         output_file="merged_result.csv"
#     )


def deduplicate_csv_by_source(
        csv_path: str,
        output_file: Optional[str] = "deduplicate_result.csv",
        encoding: str = "utf-8-sig",
        keep: str = "first",
        verbose: bool = True
) -> pd.DataFrame:
    """
    è¯»å–CSVæ–‡ä»¶ï¼Œå¹¶æ ¹æ®sourceåˆ—å»é‡

    :param csv_path: CSVæ–‡ä»¶çš„è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹è·¯å¾„ï¼‰
    :param output_file: å»é‡åçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤Noneï¼šä¸ä¿å­˜æ–‡ä»¶ï¼Œä»…è¿”å›DataFrameï¼‰
    :param encoding: æ–‡ä»¶ç¼–ç ï¼ˆé»˜è®¤"utf-8-sig"ï¼Œå…¼å®¹ä¸­æ–‡å’ŒBOMå¤´ï¼›ä¸­æ–‡æ–‡ä»¶å¯å°è¯•"gbk"ï¼‰
    :param keep: å»é‡ä¿ç•™ç­–ç•¥ï¼š"first"ï¼ˆä¿ç•™ç¬¬ä¸€æ¡é‡å¤æ•°æ®ï¼Œé»˜è®¤ï¼‰ã€"last"ï¼ˆä¿ç•™æœ€åä¸€æ¡ï¼‰
    :param verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—ï¼ˆé»˜è®¤Trueï¼‰
    :return: å»é‡åçš„DataFrame
    """
    # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼š{csv_path}")

    # 2. æ£€æŸ¥æ–‡ä»¶åç¼€æ˜¯å¦ä¸ºCSV
    if not csv_path.endswith(".csv"):
        raise ValueError(f"æ–‡ä»¶ä¸æ˜¯CSVæ ¼å¼ï¼š{csv_path}ï¼ˆè¯·ä¼ å…¥.csvåç¼€çš„æ–‡ä»¶ï¼‰")

    # 3. è¯»å–CSVæ–‡ä»¶
    try:
        df = pd.read_csv(
            csv_path,
            encoding=encoding,
            parse_dates=False  # é¿å…è‡ªåŠ¨è§£ææ—¥æœŸå¯¼è‡´é”™è¯¯
        )
        if verbose:
            print(f"æˆåŠŸè¯»å–CSVæ–‡ä»¶ï¼š{os.path.basename(csv_path)}")
            print(f"è¯»å–ååŸå§‹æ•°æ®è¡Œæ•°ï¼š{len(df)}")
    except Exception as e:
        raise Exception(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥ï¼š{str(e)}") from e

    # 4. æ£€æŸ¥sourceåˆ—æ˜¯å¦å­˜åœ¨
    if "source" not in df.columns:
        raise KeyError(f"CSVæ–‡ä»¶ä¸­æœªæ‰¾åˆ°'source'åˆ—ï¼Œè¯·æ£€æŸ¥åˆ—åæ˜¯å¦æ­£ç¡®ï¼ˆå½“å‰åˆ—åï¼š{list(df.columns)}ï¼‰")

    # 5. æ‰§è¡Œå»é‡
    original_count = len(df)
    deduplicated_df = df.drop_duplicates(
        subset=["source"],  # æŒ‰sourceåˆ—å»é‡
        keep=keep,  # ä¿ç•™ç­–ç•¥
        ignore_index=True  # é‡ç½®ç´¢å¼•ï¼ˆé¿å…ç´¢å¼•æ–­è£‚ï¼‰
    )
    duplicate_count = original_count - len(deduplicated_df)

    # 6. è¾“å‡ºå»é‡æ—¥å¿—
    if verbose:
        print(f"å»é‡ç­–ç•¥ï¼šæŒ‰'source'åˆ—ä¿ç•™{keep}æ¡æ•°æ®")
        print(f"å»é™¤é‡å¤æ•°æ®è¡Œæ•°ï¼š{duplicate_count}")
        print(f"å»é‡åå‰©ä½™æ•°æ®è¡Œæ•°ï¼š{len(deduplicated_df)}")

    # 7. ä¿å­˜å»é‡åçš„æ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šoutput_fileï¼‰
    if output_file is not None:
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = os.path.dirname(output_file)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)

            deduplicated_df.to_csv(output_file, index=False, encoding=encoding)
            if verbose:
                print(f"å»é‡åçš„æ–‡ä»¶å·²ä¿å­˜è‡³ï¼š{os.path.abspath(output_file)}")
        except Exception as e:
            raise Exception(f"ä¿å­˜å»é‡æ–‡ä»¶å¤±è´¥ï¼š{str(e)}") from e

    return deduplicated_df

# # # è¯»å–CSVå¹¶æŒ‰sourceåˆ—å»é‡ï¼ˆä¿ç•™ç¬¬ä¸€æ¡ï¼‰
# df = deduplicate_csv_by_source(
#     csv_path="merged_result.csv",  # ä½ çš„CSVæ–‡ä»¶è·¯å¾„
#     keep="first"
# )
# # åç»­å¯å¯¹dfè¿›è¡Œå…¶ä»–æ“ä½œï¼ˆå¦‚ç­›é€‰ã€ç»Ÿè®¡ï¼‰
# print(df["source"].value_counts())  # éªŒè¯å»é‡ç»“æœï¼ˆæ¯ä¸ªsourceä»…å‡ºç°1æ¬¡ï¼‰



def remove_duplicates_between_csv(
        main_csv: str,
        ref_csv: str,
        output_csv: str = "filtered_main.csv",
        compare_col: str = "source",
        encoding: str = "utf-8-sig",
        verbose: bool = True
) -> pd.DataFrame:
    """
    å¯¹æ¯”ä¸¤ä¸ªCSVæ–‡ä»¶ï¼Œå‰”é™¤ã€Œä¸»CSVä¸­åœ¨å‚è€ƒCSVä¸­å‡ºç°è¿‡çš„è®°å½•ã€ï¼Œç”Ÿæˆæ— é‡å¤çš„æ–°CSV

    æ ¸å¿ƒé€»è¾‘ï¼šä»¥ `compare_col`ï¼ˆé»˜è®¤sourceåˆ—ï¼‰ä¸ºåŸºå‡†ï¼Œä¿ç•™ä¸»CSVä¸­è¯¥åˆ—å€¼ä¸åœ¨å‚è€ƒCSVä¸­çš„æ‰€æœ‰è®°å½•

    :param main_csv: ä¸»CSVè·¯å¾„ï¼ˆéœ€è¦å‰”é™¤é‡å¤æ•°æ®çš„CSVï¼‰
    :param ref_csv: å‚è€ƒCSVè·¯å¾„ï¼ˆç”¨äºåˆ¤æ–­é‡å¤çš„åŸºå‡†CSVï¼‰
    :param output_csv: è¾“å‡ºæ— é‡å¤æ•°æ®çš„æ–°CSVè·¯å¾„ï¼ˆé»˜è®¤"filtered_main.csv"ï¼‰
    :param compare_col: ç”¨äºå¯¹æ¯”å»é‡çš„åˆ—åï¼ˆé»˜è®¤"source"ï¼Œéœ€ä¸¤ä¸ªCSVä¸­éƒ½å­˜åœ¨è¯¥åˆ—ï¼‰
    :param encoding: æ–‡ä»¶ç¼–ç ï¼ˆé»˜è®¤"utf-8-sig"ï¼Œå…¼å®¹ä¸­æ–‡å’ŒBOMå¤´ï¼›ä¸­æ–‡å¯å°è¯•"gbk"ï¼‰
    :param verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—ï¼ˆé»˜è®¤Trueï¼‰
    :return: å‰”é™¤é‡å¤åçš„æ•°æ®ï¼ˆDataFrameï¼‰
    """
    # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for csv_path in [main_csv, ref_csv]:
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{csv_path}")
        if not csv_path.endswith(".csv"):
            raise ValueError(f"æ–‡ä»¶ä¸æ˜¯CSVæ ¼å¼ï¼š{csv_path}ï¼ˆè¯·ä¼ å…¥.csvåç¼€æ–‡ä»¶ï¼‰")

    # 2. è¯»å–ä¸¤ä¸ªCSVæ–‡ä»¶
    try:
        # è¯»å–ä¸»CSV
        df_main = pd.read_csv(
            main_csv,
            encoding=encoding,
            parse_dates=False
        )
        # è¯»å–å‚è€ƒCSV
        df_ref = pd.read_csv(
            ref_csv,
            encoding=encoding,
            parse_dates=False
        )
        if verbose:
            print(f"æˆåŠŸè¯»å–æ–‡ä»¶ï¼š")
            print(f"- ä¸»CSVï¼ˆ{os.path.basename(main_csv)}ï¼‰ï¼š{len(df_main)} è¡Œ")
            print(f"- å‚è€ƒCSVï¼ˆ{os.path.basename(ref_csv)}ï¼‰ï¼š{len(df_ref)} è¡Œ")
    except Exception as e:
        raise Exception(f"è¯»å–CSVå¤±è´¥ï¼š{str(e)}") from e

    # 3. æ£€æŸ¥å¯¹æ¯”åˆ—æ˜¯å¦å­˜åœ¨äºä¸¤ä¸ªCSVä¸­
    for df, df_name in [(df_main, "ä¸»CSV"), (df_ref, "å‚è€ƒCSV")]:
        if compare_col not in df.columns:
            raise KeyError(
                f"{df_name}ä¸­æœªæ‰¾åˆ°å¯¹æ¯”åˆ—ã€Œ{compare_col}ã€\n"
                f"{df_name}ç°æœ‰åˆ—åï¼š{list(df.columns)}"
            )

    # 4. æå–å‚è€ƒCSVçš„å¯¹æ¯”åˆ—å”¯ä¸€å€¼ï¼ˆç”¨äºå¿«é€Ÿåˆ¤æ–­é‡å¤ï¼‰
    ref_unique_vals = set(df_ref[compare_col].dropna())  # å»é‡+æ’é™¤NaNå€¼
    if verbose:
        print(f"å‚è€ƒCSVä¸­ã€Œ{compare_col}ã€åˆ—å…±æœ‰ {len(ref_unique_vals)} ä¸ªå”¯ä¸€å€¼")

    # 5. å‰”é™¤ä¸»CSVä¸­ä¸å‚è€ƒCSVé‡å¤çš„è®°å½•
    # ä¿ç•™ï¼šä¸»CSVä¸­å¯¹æ¯”åˆ—å€¼ä¸åœ¨å‚è€ƒCSVä¸­çš„è¡Œ
    df_filtered = df_main[~df_main[compare_col].isin(ref_unique_vals)].reset_index(drop=True)
    duplicate_count = len(df_main) - len(df_filtered)

    # 6. è¾“å‡ºå»é‡ç»Ÿè®¡
    if verbose:
        print(f"\nå»é‡ç»“æœï¼š")
        print(f"- ä¸»CSVåŸå§‹è¡Œæ•°ï¼š{len(df_main)}")
        print(f"- å‰”é™¤é‡å¤è¡Œæ•°ï¼š{duplicate_count}")
        print(f"- å‰©ä½™æœ‰æ•ˆè¡Œæ•°ï¼š{len(df_filtered)}")

    # 7. ä¿å­˜å‰”é™¤é‡å¤åçš„æ–°CSV
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_csv)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)

        df_filtered.to_csv(output_csv, index=False, encoding=encoding)
        if verbose:
            print(f"\næ— é‡å¤æ•°æ®å·²ä¿å­˜è‡³ï¼š{os.path.abspath(output_csv)}")
    except Exception as e:
        raise Exception(f"ä¿å­˜è¾“å‡ºCSVå¤±è´¥ï¼š{str(e)}") from e

    return df_filtered


# # # ä¸»CSVï¼šéœ€è¦å‰”é™¤é‡å¤çš„æ–‡ä»¶
# # å‚è€ƒCSVï¼šç”¨äºåˆ¤æ–­é‡å¤çš„åŸºå‡†æ–‡ä»¶
# # è¾“å‡ºï¼šå‰”é™¤é‡å¤åçš„æ–°æ–‡ä»¶ï¼ˆé»˜è®¤filtered_main.csvï¼‰
# remove_duplicates_between_csv(
#     main_csv="deduplicate_result.csv",
#     ref_csv="reference.csv"
# )


def overwrite_reference_with_result(
        result_csv: str = "deduplicate_result.csv",
        reference_csv: str = "reference.csv",
        encoding: str = "utf-8-sig",
        backup_original: bool = True,
        verbose: bool = True
) -> None:
    """
    æ¸…ç©ºreference.csvåŸæœ‰æ•°æ®ï¼Œå°†deduplicate_result.csvçš„æ•°æ®å®Œæ•´å†™å…¥reference.csv

    æ ¸å¿ƒé€»è¾‘ï¼š
    1. æ ¡éªŒè¾“å…¥æ–‡ä»¶ï¼ˆresult_csvå¿…é¡»å­˜åœ¨ä¸”ä¸ºCSVï¼Œreference_csvä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»ºï¼‰
    2. å¯é€‰ï¼šå¤‡ä»½reference.csvåŸæœ‰æ•°æ®ï¼ˆé¿å…è¯¯æ“ä½œä¸¢å¤±ï¼‰
    3. è¯»å–result_csvæ•°æ®
    4. è¦†ç›–å†™å…¥reference.csvï¼ˆæ¸…ç©ºåŸæœ‰å†…å®¹ï¼‰

    :param result_csv: æ•°æ®æºCSVè·¯å¾„ï¼ˆé»˜è®¤"deduplicate_result.csv"ï¼‰
    :param reference_csv: ç›®æ ‡CSVè·¯å¾„ï¼ˆé»˜è®¤"reference.csv"ï¼‰
    :param encoding: æ–‡ä»¶ç¼–ç ï¼ˆé»˜è®¤"utf-8-sig"ï¼Œå…¼å®¹ä¸­æ–‡å’ŒBOMå¤´ï¼‰
    :param backup_original: æ˜¯å¦å¤‡ä»½reference.csvåŸæœ‰æ•°æ®ï¼ˆé»˜è®¤Trueï¼Œå¤‡ä»½æ–‡ä»¶åä¸ºreference_backup_æ—¶é—´æˆ³.csvï¼‰
    :param verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—ï¼ˆé»˜è®¤Trueï¼‰
    """
    # 1. æ ¡éªŒæ•°æ®æºæ–‡ä»¶ï¼ˆresult_csvå¿…é¡»å­˜åœ¨ä¸”ä¸ºCSVï¼‰
    if not os.path.exists(result_csv):
        raise FileNotFoundError(f"æ•°æ®æºæ–‡ä»¶ä¸å­˜åœ¨ï¼š{result_csv}")
    if not result_csv.endswith(".csv"):
        raise ValueError(f"æ•°æ®æºæ–‡ä»¶ä¸æ˜¯CSVæ ¼å¼ï¼š{result_csv}ï¼ˆè¯·ä¼ å…¥.csvåç¼€æ–‡ä»¶ï¼‰")

    # 2. å¤‡ä»½reference.csvåŸæœ‰æ•°æ®ï¼ˆå¦‚æœéœ€è¦ä¸”æ–‡ä»¶å·²å­˜åœ¨ï¼‰
    if backup_original and os.path.exists(reference_csv):
        from datetime import datetime
        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶åï¼ˆé¿å…è¦†ç›–å†å²å¤‡ä»½ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_csv = f"{os.path.splitext(reference_csv)[0]}_backup_{timestamp}.csv"
        try:
            # å¤åˆ¶åŸæœ‰æ•°æ®åˆ°å¤‡ä»½æ–‡ä»¶
            df_original = pd.read_csv(reference_csv, encoding=encoding, parse_dates=False)
            df_original.to_csv(backup_csv, index=False, encoding=encoding)
            if verbose:
                print(f"âœ… å·²å¤‡ä»½reference.csvåŸæœ‰æ•°æ®è‡³ï¼š{backup_csv}ï¼ˆ{len(df_original)}è¡Œï¼‰")
        except Exception as e:
            raise Exception(f"å¤‡ä»½reference.csvå¤±è´¥ï¼š{str(e)}") from e

    # 3. è¯»å–deduplicate_result.csvæ•°æ®
    try:
        df_result = pd.read_csv(
            result_csv,
            encoding=encoding,
            parse_dates=False
        )
        if verbose:
            print(f"âœ… æˆåŠŸè¯»å–æ•°æ®æºæ–‡ä»¶ï¼š{os.path.basename(result_csv)}ï¼ˆ{len(df_result)}è¡Œæ•°æ®ï¼‰")
    except Exception as e:
        raise Exception(f"è¯»å–æ•°æ®æºæ–‡ä»¶{result_csv}å¤±è´¥ï¼š{str(e)}") from e

    # 4. è¦†ç›–å†™å…¥reference.csvï¼ˆæ¸…ç©ºåŸæœ‰å†…å®¹ï¼Œå†™å…¥æ–°æ•°æ®ï¼‰
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ï¼ˆå¦‚æœreference.csvåœ¨å­ç›®å½•ä¸­ï¼‰
        output_dir = os.path.dirname(reference_csv)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)

        # è¦†ç›–å†™å…¥ï¼ˆindex=Falseä¸ä¿ç•™ç´¢å¼•åˆ—ï¼‰
        df_result.to_csv(reference_csv, index=False, encoding=encoding, mode="w")
        if verbose:
            print(f"âœ… å·²æˆåŠŸè¦†ç›–reference.csvï¼š")
            print(f"   - åŸæ•°æ®å·²{'å¤‡ä»½' if backup_original else 'æœªå¤‡ä»½'}")
            print(f"   - æ–°å†™å…¥æ•°æ®è¡Œæ•°ï¼š{len(df_result)}")
            print(f"   - ç›®æ ‡æ–‡ä»¶è·¯å¾„ï¼š{os.path.abspath(reference_csv)}")
    except Exception as e:
        raise Exception(f"å†™å…¥reference.csvå¤±è´¥ï¼š{str(e)}") from e


# # # ---------------------- æ‰§è¡Œç¨‹åº ----------------------
# if __name__ == "__main__":
#     try:
#         # è°ƒç”¨å‡½æ•°æ‰§è¡Œæ“ä½œï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼Œå¯æ ¹æ®éœ€è¦ä¿®æ”¹è·¯å¾„ï¼‰
#         overwrite_reference_with_result(
#             result_csv="deduplicate_result.csv",  # æ•°æ®æºæ–‡ä»¶ï¼ˆå¯æ”¹ä¸ºç»å¯¹è·¯å¾„ï¼‰
#             reference_csv="reference.csv",  # ç›®æ ‡æ–‡ä»¶ï¼ˆå¯æ”¹ä¸ºç»å¯¹è·¯å¾„ï¼‰
#             encoding="utf-8-sig",  # ç¼–ç ï¼ˆä¸­æ–‡æ–‡ä»¶å¯æ”¹ä¸º"gbk"ï¼‰
#             backup_original=True  # å»ºè®®ä¿ç•™å¤‡ä»½ï¼Œé¿å…æ•°æ®ä¸¢å¤±
#         )
#         print("\nğŸ‰ æ“ä½œå®Œæˆï¼")
#     except Exception as e:
#         print(f"\nâŒ æ“ä½œå¤±è´¥ï¼š{str(e)}")



def process_csv_replace_ptlist(
        input_csv_path,
        output_csv_path="processed_replaced_ptlist.csv",
        excluded_output_file="processed_replaced_ptlist_excluded.csv"
):
    try:
        # 1. è¯»å–åŸå§‹CSVæ–‡ä»¶
        df = pd.read_csv(input_csv_path, encoding="utf-8-sig")

        # 2. æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
        required_cols = ["source", "ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®", "æ˜¯å¦åºŸå¼ƒ"]
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            print(f"é”™è¯¯ï¼šåŸå§‹CSVç¼ºå°‘å¿…è¦åˆ—ï¼š{', '.join(missing_cols)}")
            return

        # 3. ç­›é€‰æ¡ä»¶ï¼š"æ˜¯å¦åºŸå¼ƒ"ä¸º"å¦" ä¸” "ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®"éç©º
        cond_not_discarded = df["æ˜¯å¦åºŸå¼ƒ"] == "å¦"
        cond_has_json = (df["ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®"].notna()) & (df["ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®"] != "")
        filtered_df = df[cond_not_discarded & cond_has_json].copy()

        # 3.1 è®°å½•æœªç­›é€‰æ•°æ®åŠåŸå› 
        excluded_df = df[~(cond_not_discarded & cond_has_json)].copy()
        if not excluded_df.empty:
            reasons = []
            for _, row in excluded_df.iterrows():
                row_reasons = []
                if row.get("æ˜¯å¦åºŸå¼ƒ") != "å¦":
                    row_reasons.append("æ˜¯å¦åºŸå¼ƒä¸ä¸ºå¦")
                val = row.get("ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®")
                if pd.isna(val) or val == "":
                    row_reasons.append("æ ‡æ³¨å­—æ®µä¸ºç©º")
                elif not isinstance(val, str):
                    row_reasons.append("æ ‡æ³¨å­—æ®µéå­—ç¬¦ä¸²")
                if not row_reasons:
                    row_reasons.append("æœªæ»¡è¶³ç­›é€‰æ¡ä»¶")
                reasons.append("ï¼›".join(row_reasons))
            excluded_df["æœªç­›é€‰åŸå› "] = reasons

        # 4. å¤„ç†sourceåˆ—ï¼šæ›¿æ¢å¼€å¤´çš„"oss"ä¸º"http"ï¼ˆä»…åŒ¹é…å¼€å¤´ï¼‰
        filtered_df["source"] = filtered_df["source"].str.replace("^oss", "http", n=1, regex=True)

        # 5. å®šä¹‰å•ä¸ªptListçš„æœ€å°åŒ…å›´ç›’è®¡ç®—å‡½æ•°ï¼ˆè¿”å›ä¸¤ä¸ªç‚¹åæ ‡ï¼‰
        def get_bbox_points(ptlist):
            """è¾“å…¥å•ä¸ªptListï¼Œè¿”å›æœ€å°åŒ…å›´ç›’çš„ä¸¤ä¸ªç‚¹ï¼šå·¦ä¸Š(x1,y1)ã€å³ä¸‹(x2,y2)"""
            if not isinstance(ptlist, list) or len(ptlist) == 0:
                return [{"x": None, "y": None}, {"x": None, "y": None}]

            # æå–æœ‰æ•ˆåæ ‡ç‚¹
            valid_points = [p for p in ptlist if isinstance(p, dict) and "x" in p and "y" in p]
            if not valid_points:
                return [{"x": None, "y": None}, {"x": None, "y": None}]

            # è®¡ç®—åŒ…å›´ç›’åæ ‡
            min_x = min(p["x"] for p in valid_points)
            max_x = max(p["x"] for p in valid_points)
            min_y = min(p["y"] for p in valid_points)
            max_y = max(p["y"] for p in valid_points)

            # è¿”å›ä¸¤ä¸ªç‚¹ï¼ˆå·¦ä¸Šã€å³ä¸‹ï¼‰
            return [{"x": min_x, "y": min_y}, {"x": max_x, "y": max_y}]

        # 6. è§£æåŸå§‹JSONï¼Œæ›¿æ¢æ¯ä¸ªobjectçš„ptListä¸ºåŒ…å›´ç›’ä¸¤ç‚¹ï¼Œä¿ç•™å…¶ä»–å­—æ®µ
        def parse_and_replace_ptlist(json_str):
            """è§£æåŸå§‹JSONï¼Œæ›¿æ¢ptListä¸ºåŒ…å›´ç›’ä¸¤ç‚¹ï¼Œç”Ÿæˆæ–°JSONå­—ç¬¦ä¸²"""
            try:
                if pd.isna(json_str) or not isinstance(json_str, str):
                    return None

                # è§£æåŸå§‹JSONæ•°æ®
                data = json.loads(json_str)
                objects = data.get("objects", [])

                # éå†æ¯ä¸ªobjectï¼Œæ›¿æ¢ptList
                updated_objects = []
                for obj in objects:
                    if isinstance(obj, dict):
                        # æ·±æ‹·è´åŸå§‹objectï¼Œé¿å…ä¿®æ”¹åŸæ•°æ®
                        updated_obj = obj.copy()
                        # è·å–åŸå§‹ptList
                        original_ptlist = obj.get("polygon", {}).get("ptList", [])
                        # è®¡ç®—åŒ…å›´ç›’ä¸¤ç‚¹ï¼Œæ›¿æ¢åŸå§‹ptList
                        updated_ptlist = get_bbox_points(original_ptlist)
                        # æ›´æ–°polygonä¸­çš„ptList
                        if "polygon" not in updated_obj:
                            updated_obj["polygon"] = {}
                        updated_obj["polygon"]["ptList"] = updated_ptlist
                        updated_objects.append(updated_obj)

                # æ›¿æ¢objectsä¸ºæ›´æ–°åçš„æ•°æ®
                data["objects"] = updated_objects
                # è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²è¿”å›
                return json.dumps(data, ensure_ascii=False)

            except json.JSONDecodeError:
                print(f"è­¦å‘Šï¼šJSONè§£æå¤±è´¥ï¼ˆæˆªå–å‰50å­—ç¬¦ï¼‰ï¼š{json_str[:50]}...")
                return None

        # 7. ç”Ÿæˆæ–°çš„"ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®"åˆ—ï¼ˆæ›¿æ¢ptListåï¼‰
        filtered_df["æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®"] = filtered_df["ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®"].apply(
            parse_and_replace_ptlist)

        # 8. æå–widthã€heightå­—æ®µï¼ˆå¯é€‰ï¼Œä¿æŒä¸ä¹‹å‰é€»è¾‘ä¸€è‡´ï¼‰
        def extract_width_height(json_str):
            """ä»JSONä¸­æå–widthå’Œheight"""
            try:
                if pd.isna(json_str) or not isinstance(json_str, str):
                    return {"width": None, "height": None}
                data = json.loads(json_str)
                return {"width": data.get("width"), "height": data.get("height")}
            except:
                return {"width": None, "height": None}

        # åº”ç”¨æå–å‡½æ•°
        wh_data = filtered_df["ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®"].apply(extract_width_height)
        filtered_df["width"] = [item["width"] for item in wh_data]
        filtered_df["height"] = [item["height"] for item in wh_data]

        # 9. å®šä¹‰æœ€ç»ˆä¿ç•™çš„åˆ—ï¼ˆåŸå§‹åˆ—+æ–°JSONåˆ—+æå–å­—æ®µï¼‰
        result_cols = [
            "source",  # å¤„ç†åçš„URLåˆ—
            "ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®",  # åŸå§‹JSONåˆ—ï¼ˆä¿ç•™ï¼‰
            "æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®",  # æ›¿æ¢ptListåçš„æ–°JSONåˆ—
            "width", "height"  # æå–çš„å›¾ç‰‡å°ºå¯¸å­—æ®µ
        ]

        # 10. ä¿å­˜æ–°CSVæ–‡ä»¶
        filtered_df[result_cols].to_csv(output_csv_path, index=False, encoding="utf-8-sig")

        print(f"å¤„ç†å®Œæˆï¼æ–°æ–‡ä»¶å·²ä¿å­˜åˆ°ï¼š{output_csv_path}")
        print(f"ç­›é€‰åæœ‰æ•ˆè¡Œæ•°ï¼š{len(filtered_df)}")
        if excluded_output_file is not None:
            excluded_df.to_csv(excluded_output_file, index=False, encoding="utf-8-sig")
            print(f"æœªç­›é€‰æ•°æ®å·²ä¿å­˜åˆ°ï¼š{excluded_output_file}ï¼ˆ{len(excluded_df)}è¡Œï¼‰")
        print("å…³é”®å˜åŒ–ï¼šæ¯ä¸ªobjectçš„ptListå·²æ›¿æ¢ä¸ºæœ€å°åŒ…å›´ç›’çš„ä¸¤ä¸ªç‚¹ï¼ˆå·¦ä¸Šã€å³ä¸‹ï¼‰")
        print("ä¿ç•™å­—æ®µï¼šåŸå§‹JSONåˆ—ã€å¤„ç†åçš„URLåˆ—ã€widthã€heightï¼Œæ–°JSONåˆ—ç»“æ„ä¸åŸå§‹ä¸€è‡´")

    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶ï¼š{input_csv_path}")
    except Exception as e:
        print(f"å¤„ç†å¼‚å¸¸ï¼š{e}")

    return {
        "filtered_rows": len(filtered_df) if "filtered_df" in locals() else 0,
        "excluded_rows": len(excluded_df) if "excluded_df" in locals() else 0,
        "excluded_output": excluded_output_file,
    }





# ####step1
# # # ---------------------- è¯·ä¿®æ”¹ä»¥ä¸‹å‚æ•° ----------------------
# input_csv_path = "deduplicate_result.csv"  # æ›¿æ¢ä¸ºä½ çš„åŸå§‹CSVæ–‡ä»¶è·¯å¾„
# # ------------------------------------------------------------
#
# # è°ƒç”¨å‡½æ•°ï¼ˆé»˜è®¤è¾“å‡ºæ–‡ä»¶åä¸º processed_result.csvï¼Œå¯è‡ªå®šä¹‰ï¼‰
# process_csv_replace_ptlist(input_csv_path)







def filter_by_box_count_and_iou(input_csv_path,
                                high_iou_csv="high_iou_0.98.csv",
                                other_csv="other_data.csv",
                                min_boxes: int = 2,
                                iou_threshold: float = 0.98):
    """
    æŒ‰ã€Œæ ‡æ³¨æ¡†æ•°é‡ã€å’Œã€Œä¸¤æ¡†IoUé˜ˆå€¼ã€ç­›é€‰æ•°æ®ï¼Œå¹¶æ‹†åˆ†ä¸ºé«˜IoUä¸å…¶ä»–æ•°æ®ã€‚
    """
    # 1. è®¡ç®—ä¸¤ä¸ªçŸ©å½¢æ¡†çš„äº¤å¹¶æ¯”ï¼ˆIoUï¼‰
    def calculate_iou(box1, box2):
        """
        è®¡ç®—ä¸¤ä¸ªçŸ©å½¢æ¡†çš„IoU
        :param box1: ç¬¬ä¸€ä¸ªæ¡†åæ ‡ (x1, y1, x2, y2)
        :param box2: ç¬¬äºŒä¸ªæ¡†åæ ‡ (x1, y1, x2, y2)
        :return: IoUå€¼ï¼ˆ0~1ï¼‰
        """
        # è®¡ç®—é‡å åŒºåŸŸåæ ‡
        x1_inter = max(box1[0], box2[0])
        y1_inter = max(box1[1], box2[1])
        x2_inter = min(box1[2], box2[2])
        y2_inter = min(box1[3], box2[3])

        # é‡å é¢ç§¯ï¼ˆæ— é‡å åˆ™ä¸º0ï¼‰
        intersection = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)
        if intersection == 0:
            return 0.0

        # ä¸¤ä¸ªæ¡†çš„é¢ç§¯
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])

        # è®¡ç®—IoU
        union = area1 + area2 - intersection
        return intersection / union if union != 0 else 0.0

    # 2. ä»JSONä¸­æå–æ ‡æ³¨æ¡†ï¼ˆé€‚é…æ–°ç»“æœå­—æ®µçš„ä¸¤ç‚¹æ ¼å¼ï¼‰
    def extract_boxes(json_str):
        """
        ä»"æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®"æå–æ‰€æœ‰æ ‡æ³¨æ¡†åæ ‡
        :return: æ¡†åˆ—è¡¨ [(x1,y1,x2,y2), ...]ï¼Œä»…ä¿ç•™æœ‰æ•ˆä¸¤ç‚¹æ¡†
        """
        boxes = []
        try:
            if pd.isna(json_str) or not isinstance(json_str, str):
                return boxes
            data = json.loads(json_str)
            objects = data.get("objects", [])

            for obj in objects:
                if not isinstance(obj, dict):
                    continue
                # æå–ä¸¤ç‚¹åæ ‡ï¼ˆæ–°ç»“æœå­—æ®µåº”ä¸ºä¸¤ç‚¹åŒ…å›´ç›’ï¼‰
                ptlist = obj.get("polygon", {}).get("ptList", [])
                if len(ptlist) != 2:
                    continue  # åªå¤„ç†æ ‡å‡†ä¸¤ç‚¹æ¡†
                # è§£æxã€yåæ ‡ï¼ˆç¡®ä¿æœ‰æ•ˆï¼‰
                p1, p2 = ptlist
                if not (isinstance(p1, dict) and isinstance(p2, dict)
                        and "x" in p1 and "y" in p1
                        and "x" in p2 and "y" in p2):
                    continue
                # è§„èŒƒåŒ–åæ ‡ï¼ˆç¡®ä¿x1 < x2ï¼Œy1 < y2ï¼‰
                x1 = min(p1["x"], p2["x"])
                y1 = min(p1["y"], p2["y"])
                x2 = max(p1["x"], p2["x"])
                y2 = max(p1["y"], p2["y"])
                boxes.append((x1, y1, x2, y2))
        except json.JSONDecodeError:
            print(f"è­¦å‘Šï¼šJSONè§£æå¤±è´¥ï¼ˆå‰50å­—ç¬¦ï¼‰ï¼š{str(json_str)[:50]}...")
        except Exception as e:
            print(f"æå–æ¡†å¤±è´¥ï¼š{e}")
        return boxes

    # 3. æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ¡ä»¶ï¼šæ ‡æ³¨æ¡†â‰¥min_boxes ä¸” å­˜åœ¨ä¸¤æ¡†IoUâ‰¥iou_threshold
    def meet_conditions(boxes):
        """
        :param boxes: æ ‡æ³¨æ¡†åˆ—è¡¨
        :return: Trueï¼ˆæ»¡è¶³æ¡ä»¶ï¼‰/ Falseï¼ˆä¸æ»¡è¶³ï¼‰
        """
        # æ¡ä»¶1ï¼šæ ‡æ³¨æ¡†æ•°é‡â‰¥min_boxes
        if len(boxes) < min_boxes:
            return False
        # æ¡ä»¶2ï¼šå­˜åœ¨ä»»æ„ä¸¤æ¡†IoUâ‰¥iou_threshold
        for i in range(len(boxes)):
            for j in range(i + 1, len(boxes)):
                iou = calculate_iou(boxes[i], boxes[j])
                if iou >= iou_threshold:
                    return True
        return False

    # 4. è¯»å–CSVå¹¶ç­›é€‰æ•°æ®
    try:
        df = pd.read_csv(input_csv_path, encoding="utf-8-sig")
        print(f"æˆåŠŸè¯»å–CSVï¼Œå…± {len(df)} è¡Œæ•°æ®")
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°æ–‡ä»¶ {input_csv_path}")
        return
    except Exception as e:
        print(f"è¯»å–å¤±è´¥ï¼š{e}")
        return

    # æ£€æŸ¥å¿…è¦åˆ—
    required_col = "æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®"
    if required_col not in df.columns:
        print(f"é”™è¯¯ï¼šç¼ºå°‘å¿…è¦åˆ— {required_col}")
        return

    # 5. åˆ†ç¦»ç¬¦åˆæ¡ä»¶å’Œä¸ç¬¦åˆæ¡ä»¶çš„æ•°æ®
    high_iou_data = []  # ç¬¦åˆæ¡ä»¶ï¼šæ¡†â‰¥min_boxes ä¸” IoUâ‰¥iou_threshold
    other_data = []  # å…¶ä»–æ•°æ®

    for idx, row in df.iterrows():
        json_str = row[required_col]
        boxes = extract_boxes(json_str)
        if meet_conditions(boxes):
            high_iou_data.append(row)
        else:
            other_data.append(row)

    # 6. ä¿å­˜ç»“æœï¼ˆå…³é”®ä¿®æ”¹ï¼šä¸ºç©ºæ—¶ä¿ç•™è¡¨å¤´ï¼‰
    # ä¿å­˜ç¬¦åˆæ¡ä»¶çš„æ•°æ®ï¼ˆhigh_iou_csvï¼‰
    high_iou_df = pd.DataFrame(high_iou_data, columns=df.columns)  # å¼ºåˆ¶æŒ‡å®šè¡¨å¤´
    high_iou_df.to_csv(high_iou_csv, index=False, encoding="utf-8-sig")

    # ä¿å­˜å…¶ä»–æ•°æ®ï¼ˆother_csvï¼‰ï¼šä¸ºç©ºæ—¶ä»ä¿ç•™è¡¨å¤´
    other_df = pd.DataFrame(other_data, columns=df.columns)  # å¼ºåˆ¶æŒ‡å®šè¡¨å¤´
    other_df.to_csv(other_csv, index=False, encoding="utf-8-sig")

    # è¾“å‡ºç»Ÿè®¡
    print(f"\nç­›é€‰å®Œæˆï¼")
    print(f"ç¬¦åˆæ¡ä»¶ï¼ˆæ¡†â‰¥{min_boxes} ä¸” IoUâ‰¥{iou_threshold}ï¼‰ï¼š{len(high_iou_data)} è¡Œ â†’ {high_iou_csv}")
    print(f"å…¶ä»–æ•°æ®ï¼š{len(other_data)} è¡Œ â†’ {other_csv}")
    print(f"æ³¨ï¼šè‹¥æŸæ–‡ä»¶è¡Œæ•°ä¸º0ï¼Œå·²è‡ªåŠ¨ä¿ç•™åŸCSVè¡¨å¤´")


# # ---------------------- é…ç½®å‚æ•° ----------------------
# input_csv_path = "processed_replaced_ptlist.csv"  # ä½ çš„è¾“å…¥CSVè·¯å¾„
# # ------------------------------------------------------
#
# # æ‰§è¡Œç­›é€‰
# filter_by_box_count_and_iou(input_csv_path)


def download_and_draw_annotations(
        input_csv_path,
        output_dir: Optional[str] = None,
        download_dir: Optional[str] = None,
        result_dir: Optional[str] = None,
        max_images: Optional[int] = None,
        timeout: int = 15
):
    # 1. å®šä¹‰æ–‡ä»¶å¤¹è·¯å¾„ï¼Œè‡ªåŠ¨åˆ›å»ºä¸å­˜åœ¨çš„æ–‡ä»¶å¤¹
    base_dir = Path(output_dir) if output_dir else Path(os.getcwd())
    download_dir = Path(download_dir) if download_dir else (base_dir / "downloaded_images")
    result_dir = Path(result_dir) if result_dir else (base_dir / "annotated_images")
    download_dir.mkdir(parents=True, exist_ok=True)  # è‡ªåŠ¨åˆ›å»ºæ–‡ä»¶å¤¹ï¼Œå·²å­˜åœ¨åˆ™å¿½ç•¥
    result_dir.mkdir(parents=True, exist_ok=True)

    # 2. è¯»å–CSVæ–‡ä»¶
    try:
        df = pd.read_csv(input_csv_path, encoding="utf-8-sig")
        print(f"æˆåŠŸè¯»å–CSVæ–‡ä»¶ï¼Œå…± {len(df)} è¡Œæ•°æ®")
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°CSVæ–‡ä»¶ {input_csv_path}")
        return
    except Exception as e:
        print(f"è¯»å–CSVå¤±è´¥ï¼š{e}")
        return

    # 3. æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
    required_cols = ["source", "ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®", "æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®"]
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        print(f"é”™è¯¯ï¼šCSVç¼ºå°‘å¿…è¦åˆ—ï¼š{', '.join(missing_cols)}")
        return

    # 4. å®šä¹‰å­—ä½“ï¼ˆç”¨äºç»˜åˆ¶ç±»åˆ«åç§°ï¼Œé¿å…ä¸­æ–‡ä¹±ç ï¼‰
    def get_font():
        """å°è¯•åŠ è½½ç³»ç»Ÿå­—ä½“ï¼Œå¤±è´¥åˆ™ä½¿ç”¨é»˜è®¤å­—ä½“"""
        try:
            # Windowsç³»ç»Ÿ
            return ImageFont.truetype("simhei.ttf", 48)
        except:
            try:
                # Macç³»ç»Ÿ
                return ImageFont.truetype("Arial Unicode.ttf", 48)
            except:
                #  fallbackï¼šä½¿ç”¨é»˜è®¤å­—ä½“
                return ImageFont.load_default()

    font = get_font()

    # 5. å®šä¹‰ç»˜åˆ¶æ ‡æ³¨æ¡†çš„å‡½æ•°
    def draw_annotation_boxes(image, json_str, color, draw):
        """
        åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶æ ‡æ³¨æ¡†å’Œç±»åˆ«åç§°
        :param image: PIL.Imageå¯¹è±¡
        :param json_str: æ ‡æ³¨JSONå­—ç¬¦ä¸²
        :param color: æ ‡æ³¨æ¡†é¢œè‰²ï¼ˆRGBå…ƒç»„ï¼‰
        :param draw: ImageDrawå¯¹è±¡
        """
        try:
            if pd.isna(json_str) or not isinstance(json_str, str):
                return
            data = json.loads(json_str)
            objects = data.get("objects", [])

            for obj in objects:
                if not isinstance(obj, dict):
                    continue
                # æå–ç±»åˆ«åç§°
                name = obj.get("name", "æœªçŸ¥ç±»åˆ«")
                # æå–ptListï¼ˆåæ ‡ç‚¹ï¼‰
                ptlist = obj.get("polygon", {}).get("ptList", [])
                if not ptlist or len(ptlist) < 2:
                    continue

                # è§£æåæ ‡ç‚¹ï¼ˆé€‚é…åŸå§‹å¤šè¾¹å½¢/æ–°åŒ…å›´ç›’ä¸¤ç‚¹ï¼‰
                points = []
                for p in ptlist:
                    if (isinstance(p, dict) and "x" in p and "y" in p
                            and p["x"] is not None and p["y"] is not None):
                        points.append((p["x"], p["y"]))

                if len(points) < 2:
                    continue

                # ç»˜åˆ¶æ ‡æ³¨æ¡†ï¼šä¸¤ç‚¹åˆ™ç”»çŸ©å½¢ï¼Œå¤šç‚¹åˆ™ç”»å¤šè¾¹å½¢
                if len(points) == 2:
                    # ä¸¤ç‚¹æ¨¡å¼ï¼ˆå·¦ä¸Šã€å³ä¸‹ï¼‰ï¼šç”»çŸ©å½¢
                    x1, y1 = points[0]
                    x2, y2 = points[1]
                    draw.rectangle([x1, y1, x2, y2], outline=color, width=2)
                    # åœ¨çŸ©å½¢å·¦ä¸Šè§’ç»˜åˆ¶ç±»åˆ«åç§°ï¼ˆèƒŒæ™¯åŠé€æ˜ï¼‰
                    text_bbox = draw.textbbox((x1, y1 - 20), name, font=font)
                    draw.rectangle(text_bbox, fill=(255, 255, 255, 180))  # ç™½è‰²åŠé€æ˜èƒŒæ™¯
                    draw.text((x1, y1 - 20), name, font=font, fill=color)
                else:
                    # å¤šç‚¹æ¨¡å¼ï¼ˆåŸå§‹å¤šè¾¹å½¢ï¼‰ï¼šç”»å¤šè¾¹å½¢
                    draw.polygon(points, outline=color, width=2)
                    # åœ¨å¤šè¾¹å½¢å·¦ä¸Šè§’é™„è¿‘ç»˜åˆ¶ç±»åˆ«åç§°
                    min_x = min(p[0] for p in points)
                    min_y = min(p[1] for p in points)
                    text_bbox = draw.textbbox((min_x, min_y - 20), name, font=font)
                    draw.rectangle(text_bbox, fill=(255, 255, 255, 180))
                    draw.text((min_x, min_y - 20), name, font=font, fill=color)

        except json.JSONDecodeError:
            print(f"è­¦å‘Šï¼šæ ‡æ³¨JSONè§£æå¤±è´¥")
        except Exception as e:
            print(f"ç»˜åˆ¶æ ‡æ³¨å¤±è´¥ï¼š{e}")

    # 6. éå†æ¯è¡Œæ•°æ®ï¼Œä¸‹è½½å›¾ç‰‡å¹¶ç»˜åˆ¶æ ‡æ³¨
    success_count = 0
    fail_count = 0

    for idx, row in df.iterrows():
        processed_count = success_count + fail_count
        if max_images is not None and processed_count >= max_images:
            print(f"\nå·²è¾¾åˆ°æœ€å¤§å¤„ç†æ•°é‡ï¼š{max_images}")
            break
        source_url = row["source"]
        original_anno = row["ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®"]
        new_anno = row["æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®"]

        # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶åï¼ˆä»URLæå–æˆ–ç”¨ç´¢å¼•å‘½åï¼‰
        img_filename = source_url.split("/")[-1] if "/" in source_url else f"image_{idx}.jpg"
        download_path = download_dir / img_filename
        result_path = result_dir / img_filename

        # è·³è¿‡å·²ä¸‹è½½çš„å›¾ç‰‡ï¼ˆé¿å…é‡å¤ä¸‹è½½ï¼‰
        if not os.path.exists(download_path):
            print(f"\næ­£åœ¨ä¸‹è½½å›¾ç‰‡ï¼š{source_url}")
            try:
                # ä¸‹è½½å›¾ç‰‡
                response = requests.get(source_url, stream=True, timeout=timeout)
                response.raise_for_status()  # æ•è·HTTPé”™è¯¯
                with open(download_path, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                print(f"ä¸‹è½½æˆåŠŸï¼š{img_filename}")
            except requests.exceptions.RequestException as e:
                print(f"ä¸‹è½½å¤±è´¥ï¼š{e}")
                fail_count += 1
                continue
        else:
            print(f"\nå›¾ç‰‡å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ï¼š{img_filename}")

        # æ‰“å¼€å›¾ç‰‡å¹¶ç»˜åˆ¶æ ‡æ³¨
        try:
            with Image.open(download_path) as img:
                draw = ImageDraw.Draw(img)
                # ç»˜åˆ¶åŸå§‹æ ‡æ³¨æ¡†ï¼ˆçº¢è‰²ï¼šRGB(255,0,0)ï¼‰
                draw_annotation_boxes(img, original_anno, (255, 0, 0), draw)
                # ç»˜åˆ¶æ–°æ ‡æ³¨æ¡†ï¼ˆç»¿è‰²ï¼šRGB(0,255,0)ï¼‰
                draw_annotation_boxes(img, new_anno, (0, 255, 0), draw)
                # ä¿å­˜æ ‡æ³¨åçš„å›¾ç‰‡
                img.save(result_path)
            print(f"æ ‡æ³¨å®Œæˆï¼š{img_filename}")
            success_count += 1
        except Exception as e:
            print(f"å›¾ç‰‡å¤„ç†å¤±è´¥ï¼š{e}")
            fail_count += 1

    # 7. è¾“å‡ºç»Ÿè®¡ç»“æœ
    print(f"\nå¤„ç†å®Œæˆï¼")
    print(f"æˆåŠŸå¤„ç†ï¼š{success_count} å¼ å›¾ç‰‡")
    print(f"å¤±è´¥ï¼š{fail_count} å¼ å›¾ç‰‡")
    print(f"ä¸‹è½½å›¾ç‰‡è·¯å¾„ï¼š{download_dir}")
    print(f"æ ‡æ³¨å›¾ç‰‡è·¯å¾„ï¼š{result_dir}")


def _safe_filename(name: str) -> str:
    name = str(name).strip()
    name = re.sub(r"[\\\\/:*?\"<>|]", "_", name)
    name = re.sub(r"\\s+", "_", name)
    return name or "unknown"


def _to_pinyin_slug(text: str) -> Optional[str]:
    try:
        from pypinyin import lazy_pinyin
    except Exception:
        return None
    parts = []
    for ch in str(text):
        if re.match(r"[\u4e00-\u9fff]", ch):
            py = lazy_pinyin(ch)
            if py:
                parts.append(py[0])
        else:
            parts.append(ch)
    slug = "".join(parts).strip()
    slug = re.sub(r"\s+", "_", slug)
    slug = re.sub(r"[^A-Za-z0-9._-]", "_", slug)
    slug = re.sub(r"_+", "_", slug).strip("_")
    return slug.lower() if slug else None


def _ascii_safe_filename(name: str, fallback: str) -> str:
    base = _safe_filename(name)
    ascii_name = re.sub(r"[^A-Za-z0-9._-]", "_", base)
    ascii_name = re.sub(r"_+", "_", ascii_name).strip("_")
    return ascii_name or fallback


def _safe_dataset_dir_name(name: str, fallback: str) -> str:
    text = str(name).strip()
    if re.search(r"[\u4e00-\u9fff]", text):
        pinyin_slug = _to_pinyin_slug(text)
        if pinyin_slug:
            return pinyin_slug
    return _ascii_safe_filename(text, fallback)


def _split_label_cell(value: str) -> list:
    if value is None:
        return []
    text = str(value).strip()
    if not text or text.lower() == "nan":
        return []
    # æ¯ä¸ªå•å…ƒæ ¼å³ä¸€ä¸ªæ ‡ç­¾ï¼Œä¸åšæ‹†åˆ†
    return [text]


def _parse_objects(json_str: str):
    if pd.isna(json_str) or not isinstance(json_str, str):
        return None, "æ ‡æ³¨å­—æ®µä¸ºç©ºæˆ–éå­—ç¬¦ä¸²"
    try:
        data = json.loads(json_str)
    except json.JSONDecodeError:
        return None, "æ ‡æ³¨å­—æ®µJSONè§£æå¤±è´¥"
    objects = data.get("objects")
    if objects is None:
        return None, "æ ‡æ³¨å­—æ®µç¼ºå°‘objects"
    if not isinstance(objects, list) or len(objects) == 0:
        return [], "æ ‡æ³¨å­—æ®µobjectsä¸ºç©º"
    return objects, None


def _parse_data_objects(json_str: str):
    if pd.isna(json_str) or not isinstance(json_str, str):
        return None, None, "æ ‡æ³¨å­—æ®µä¸ºç©ºæˆ–éå­—ç¬¦ä¸²"
    try:
        data = json.loads(json_str)
    except json.JSONDecodeError:
        return None, None, "æ ‡æ³¨å­—æ®µJSONè§£æå¤±è´¥"
    objects = data.get("objects")
    if objects is None:
        return data, None, "æ ‡æ³¨å­—æ®µç¼ºå°‘objects"
    if not isinstance(objects, list) or len(objects) == 0:
        return data, [], "æ ‡æ³¨å­—æ®µobjectsä¸ºç©º"
    return data, objects, None


def _split_object_labels(label_str: str) -> list:
    if label_str is None:
        return []
    text = str(label_str).strip()
    if not text or text.lower() == "nan":
        return []
    # ä»…æ”¯æŒä¸­æ–‡é€—å·/è‹±æ–‡é€—å·åˆ†éš”çš„å¤šæ ‡ç­¾
    parts = re.split(r"[ï¼Œ,]+", text)
    return [p.strip() for p in parts if p.strip()]


def _replace_label_tokens(raw_name: str, mapping: dict) -> tuple:
    if raw_name is None:
        return raw_name, 0, 0
    text = str(raw_name).strip()
    if not text:
        return raw_name, 0, 0
    labels = _split_object_labels(text)
    if not labels:
        return raw_name, 0, 0
    replaced = 0
    new_labels = []
    for lbl in labels:
        new_lbl = mapping.get(lbl, lbl)
        if new_lbl != lbl:
            replaced += 1
        new_labels.append(new_lbl)
    if "ï¼Œ" in text:
        joiner = "ï¼Œ"
    elif "," in text:
        joiner = ","
    else:
        joiner = "ï¼Œ"
    return joiner.join(new_labels), replaced, len(labels)


def _filter_json_by_label(json_str: str, label: str):
    if pd.isna(json_str) or not isinstance(json_str, str):
        return None
    try:
        data = json.loads(json_str)
    except json.JSONDecodeError:
        return None
    objects = data.get("objects", [])
    filtered = [obj for obj in objects if isinstance(obj, dict) and obj.get("name") == label]
    if not filtered:
        return None
    data["objects"] = filtered
    return json.dumps(data, ensure_ascii=False)


def replace_labels_by_mapping(
        input_csv_path: str,
        mapping_excel_path: str,
        output_csv_path: str,
        sheet_name: Optional[str] = None,
        old_col: Optional[str] = None,
        new_col: Optional[str] = None,
        json_columns: Optional[list] = None,
        diff_excel_path: Optional[str] = None,
        unmatched_excel_path: Optional[str] = None,
        sample_size: int = 30,
):
    """
    ä½¿ç”¨æ–°æ—§æ ‡ç­¾å¯¹ç…§è¡¨æ›¿æ¢æ ‡æ³¨ä¸­çš„æ ‡ç­¾åç§°ã€‚
    æ”¯æŒåœ¨æ ‡æ³¨å­—æ®µçš„ objects[].name ä¸­æ›¿æ¢ï¼Œè¾“å‡ºæ–°çš„CSVã€‚
    """
    df = pd.read_csv(input_csv_path, encoding="utf-8-sig")

    mapping_df = pd.read_excel(mapping_excel_path, sheet_name=sheet_name) if sheet_name else pd.read_excel(mapping_excel_path)
    if mapping_df.empty:
        raise ValueError("æ ‡ç­¾å¯¹ç…§è¡¨ä¸ºç©º")

    if not old_col or not new_col:
        cols = list(mapping_df.columns)
        if len(cols) < 2:
            raise ValueError("æ ‡ç­¾å¯¹ç…§è¡¨è‡³å°‘éœ€è¦ä¸¤åˆ—ï¼ˆæ—§æ ‡ç­¾ã€æ–°æ ‡ç­¾ï¼‰")
        old_col = old_col or cols[0]
        new_col = new_col or cols[1]

    label_map = {}
    for _, row in mapping_df.iterrows():
        old_label = str(row.get(old_col, "")).strip()
        new_label = str(row.get(new_col, "")).strip()
        if not old_label or old_label.lower() == "nan":
            continue
        if not new_label or new_label.lower() == "nan":
            continue
        label_map[old_label] = new_label
    if not label_map:
        raise ValueError("æ ‡ç­¾å¯¹ç…§è¡¨æœªåŒ…å«æœ‰æ•ˆæ˜ å°„")

    if json_columns is None:
        json_columns = []
        if "æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®" in df.columns:
            json_columns.append("æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®")
        if "ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®" in df.columns:
            json_columns.append("ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®")
    if not json_columns:
        raise KeyError("è¾“å…¥CSVä¸­æœªæ‰¾åˆ°æ ‡æ³¨å­—æ®µåˆ—")

    total_rows = len(df)
    total_objects = 0
    total_labels = 0
    replaced_labels = 0
    replaced_objects = 0
    replaced_rows = 0
    invalid_json_rows = 0
    missing_name_objects = 0
    unmatched_counter = {}
    diff_rows = []

    for idx, row in df.iterrows():
        row_replaced = False
        for col in json_columns:
            if col not in df.columns:
                continue
            json_str = row.get(col)
            if pd.isna(json_str) or not isinstance(json_str, str) or not json_str:
                continue
            try:
                data = json.loads(json_str)
            except json.JSONDecodeError:
                invalid_json_rows += 1
                continue
            objects = data.get("objects")
            if not isinstance(objects, list):
                continue
            row_diff = []
            for obj in objects:
                if not isinstance(obj, dict):
                    continue
                total_objects += 1
                raw_name = obj.get("name")
                if raw_name is None:
                    missing_name_objects += 1
                    continue
                labels = _split_object_labels(raw_name)
                for lbl in labels:
                    if lbl not in label_map:
                        unmatched_counter[lbl] = unmatched_counter.get(lbl, 0) + 1
                new_name, replaced, label_count = _replace_label_tokens(raw_name, label_map)
                total_labels += label_count
                if replaced > 0:
                    obj["name"] = new_name
                    replaced_labels += replaced
                    replaced_objects += 1
                    row_replaced = True
                if raw_name != new_name:
                    row_diff.append((raw_name, new_name))
            data["objects"] = objects
            df.at[idx, col] = json.dumps(data, ensure_ascii=False)
            if row_diff:
                diff_rows.append({
                    "source": row.get("source"),
                    "column": col,
                    "before": "ï¼›".join([p[0] for p in row_diff]),
                    "after": "ï¼›".join([p[1] for p in row_diff]),
                })
        if row_replaced:
            replaced_rows += 1

    output_csv_path = Path(output_csv_path)
    output_csv_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(output_csv_path, index=False, encoding="utf-8-sig")

    diff_path = None
    if diff_excel_path:
        diff_path = Path(diff_excel_path)
        diff_path.parent.mkdir(parents=True, exist_ok=True)
        diff_df = pd.DataFrame(diff_rows)
        diff_df.to_excel(diff_path, index=False)

    unmatched_path = None
    if unmatched_excel_path:
        unmatched_path = Path(unmatched_excel_path)
        unmatched_path.parent.mkdir(parents=True, exist_ok=True)
        if unmatched_counter:
            unmatched_df = pd.DataFrame(
                [{"æ ‡ç­¾": k, "æ•°é‡": v} for k, v in unmatched_counter.items()]
            ).sort_values("æ•°é‡", ascending=False)
        else:
            unmatched_df = pd.DataFrame(columns=["æ ‡ç­¾", "æ•°é‡"])
        unmatched_df.to_excel(unmatched_path, index=False)

    sample_diff = []
    for item in diff_rows[: sample_size or 0]:
        sample_diff.append(item)

    summary = {
        "total_rows": total_rows,
        "replaced_rows": replaced_rows,
        "total_objects": total_objects,
        "replaced_objects": replaced_objects,
        "total_labels": total_labels,
        "replaced_labels": replaced_labels,
        "invalid_json_rows": invalid_json_rows,
        "missing_name_objects": missing_name_objects,
        "mapping_size": len(label_map),
        "unmatched_labels": len(unmatched_counter),
    }
    return {
        "output_csv": output_csv_path,
        "summary": summary,
        "diff": diff_path,
        "unmatched": unmatched_path,
        "sample_diff": sample_diff,
    }


def split_dataset_by_rules(
        input_csv_path: str,
        rules_excel_path: str,
        output_dir: str,
        rule_mode: str = "wide",
        sheet_name: Optional[str] = None,
        label_col: Optional[str] = None,
        category_col: Optional[str] = None,
        json_columns: Optional[list] = None,
        train_ratio: float = 0.8,
        val_ratio: float = 0.1,
        test_ratio: float = 0.1,
        random_seed: int = 42,
):
    """
    æ ¹æ®åˆ†ç±»è§„åˆ™å°†æ•°æ®æ‹†åˆ†ä¸ºå¤šä¸ªç±»åˆ«Excelï¼Œå¹¶ç”Ÿæˆ train/val/testã€‚
    - æ”¯æŒå®½è¡¨ï¼ˆç±»åˆ«ä¸ºåˆ—ï¼‰å’Œä¸¤åˆ—æ˜ å°„ï¼ˆæ ‡ç­¾-ç±»åˆ«ï¼‰ã€‚
    - å¤šæ ‡ç­¾æ•°æ®ä¼šç”Ÿæˆå¤šæ¡è®°å½•ï¼Œæ¯æ¡åªä¿ç•™ä¸€ä¸ªæ ‡ç­¾çš„æ ‡æ³¨æ¡†ã€‚
    - æ— æ³•åˆ†ç±»çš„æ•°æ®å•ç‹¬è¾“å‡ºå¹¶é™„åŸå› ã€‚
    """
    if not os.path.exists(input_csv_path):
        raise FileNotFoundError(f"è¾“å…¥CSVä¸å­˜åœ¨ï¼š{input_csv_path}")
    if not os.path.exists(rules_excel_path):
        raise FileNotFoundError(f"è§„åˆ™Excelä¸å­˜åœ¨ï¼š{rules_excel_path}")

    if train_ratio + val_ratio + test_ratio <= 0:
        raise ValueError("è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ¯”ä¾‹ä¹‹å’Œå¿…é¡»å¤§äº0")

    ratio_sum = train_ratio + val_ratio + test_ratio
    train_ratio /= ratio_sum
    val_ratio /= ratio_sum
    test_ratio /= ratio_sum

    df = pd.read_csv(input_csv_path, encoding="utf-8-sig")

    if json_columns is None:
        json_columns = []
        if "æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®" in df.columns:
            json_columns.append("æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®")
        if "ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®" in df.columns:
            json_columns.append("ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®")
    if not json_columns:
        raise KeyError("è¾“å…¥CSVä¸­æœªæ‰¾åˆ°æ ‡æ³¨å­—æ®µåˆ—")

    rules_df = pd.read_excel(rules_excel_path, sheet_name=sheet_name) if sheet_name else pd.read_excel(rules_excel_path)

    label_to_category = {}
    label_conflicts = {}

    if rule_mode == "wide":
        for col in rules_df.columns:
            category = str(col).strip()
            if not category:
                continue
            for cell in rules_df[col].dropna():
                labels = _split_label_cell(cell)
                for label in labels:
                    if label in label_to_category and label_to_category[label] != category:
                        label_conflicts.setdefault(label, set()).update([label_to_category[label], category])
                    else:
                        label_to_category[label] = category
    elif rule_mode == "two_column":
        if not label_col or not category_col:
            raise ValueError("ä¸¤åˆ—æ˜ å°„æ¨¡å¼éœ€è¦æä¾› label_col å’Œ category_col")
        for _, row in rules_df.iterrows():
            label = str(row.get(label_col, "")).strip()
            category = str(row.get(category_col, "")).strip()
            if not label or not category or label.lower() == "nan" or category.lower() == "nan":
                continue
            if label in label_to_category and label_to_category[label] != category:
                label_conflicts.setdefault(label, set()).update([label_to_category[label], category])
            else:
                label_to_category[label] = category
    else:
        raise ValueError("rule_mode ä»…æ”¯æŒ wide æˆ– two_column")

    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    category_rows = {}
    unclassified_rows = []
    split_counts_rows = []

    for _, row in df.iterrows():
        json_str = None
        for col in json_columns:
            if col in row and isinstance(row[col], str) and row[col]:
                json_str = row[col]
                break

        data, objects, error = _parse_data_objects(json_str)
        if error:
            row_copy = row.copy()
            row_copy["æ— æ³•åˆ†ç±»åŸå› "] = error
            unclassified_rows.append(row_copy)
            split_counts_rows.append({
                "source": row.get("source"),
                "åŸå§‹æ ‡ç­¾ç»„åˆ": "",
                "æ‹†åˆ†æ¡æ•°": 0,
                "æ˜¯å¦å¯åˆ†ç±»": "å¦",
                "æ— æ³•åˆ†ç±»åŸå› ": error,
            })
            continue

        if not objects:
            row_copy = row.copy()
            row_copy["æ— æ³•åˆ†ç±»åŸå› "] = "æ ‡æ³¨å­—æ®µobjectsä¸ºç©º"
            unclassified_rows.append(row_copy)
            split_counts_rows.append({
                "source": row.get("source"),
                "åŸå§‹æ ‡ç­¾ç»„åˆ": "",
                "æ‹†åˆ†æ¡æ•°": 0,
                "æ˜¯å¦å¯åˆ†ç±»": "å¦",
                "æ— æ³•åˆ†ç±»åŸå› ": "æ ‡æ³¨å­—æ®µobjectsä¸ºç©º",
            })
            continue

        raw_label_set = set()
        for obj in objects:
            if isinstance(obj, dict) and obj.get("name"):
                raw_label_set.update(_split_object_labels(obj.get("name")))
        raw_label_combo = "ï¼Œ".join(sorted(raw_label_set)) if raw_label_set else ""
        row_expand_count = 0
        row_reason_set = set()

        any_classified = False
        for obj in objects:
            if not isinstance(obj, dict):
                continue
            raw_name = obj.get("name")
            labels = _split_object_labels(raw_name)
            if not labels:
                row_copy = row.copy()
                row_copy["æ— æ³•åˆ†ç±»åŸå› "] = "æ ‡æ³¨æ¡†ç¼ºå°‘nameå­—æ®µ"
                row_copy["æ— æ³•åˆ†ç±»æ ‡ç­¾"] = ""
                unclassified_rows.append(row_copy)
                row_reason_set.add("æ ‡æ³¨æ¡†ç¼ºå°‘nameå­—æ®µ")
                continue

            for label in labels:
                # if label in label_conflicts:
                #     row_copy = row.copy()
                #     row_copy["æ— æ³•åˆ†ç±»åŸå› "] = f"æ ‡ç­¾{label}åœ¨è§„åˆ™ä¸­æ˜ å°„å¤šä¸ªç±»åˆ«"
                #     row_copy["æ— æ³•åˆ†ç±»æ ‡ç­¾"] = label
                #     unclassified_rows.append(row_copy)
                #     row_reason_set.add(f"æ ‡ç­¾{label}åœ¨è§„åˆ™ä¸­æ˜ å°„å¤šä¸ªç±»åˆ«")
                #     continue
                if label not in label_to_category:
                    row_copy = row.copy()
                    row_copy["æ— æ³•åˆ†ç±»åŸå› "] = f"æ ‡ç­¾{label}æœªåœ¨è§„åˆ™ä¸­å®šä¹‰"
                    row_copy["æ— æ³•åˆ†ç±»æ ‡ç­¾"] = label
                    unclassified_rows.append(row_copy)
                    row_reason_set.add(f"æ ‡ç­¾{label}æœªåœ¨è§„åˆ™ä¸­å®šä¹‰")
                    continue

                category = label_to_category[label]
                new_row = row.copy()
                obj_copy = copy.deepcopy(obj)
                obj_copy["name"] = label
                new_data = {k: v for k, v in data.items() if k != "objects"}
                new_data["objects"] = [obj_copy]
                new_json = json.dumps(new_data, ensure_ascii=False)
                for col in json_columns:
                    if col in df.columns:
                        new_row[col] = new_json
                new_row["åˆ†ç±»æ ‡ç­¾"] = label
                new_row["åˆ†ç±»ç±»åˆ«"] = category
                new_row["åŸå§‹æ ‡ç­¾ç»„åˆ"] = raw_label_combo
                category_rows.setdefault(category, []).append(new_row)
                any_classified = True
                row_expand_count += 1

        if not any_classified:
            row_copy = row.copy()
            reason_msg = "ï¼›".join(sorted(row_reason_set)) if row_reason_set else "æ ‡ç­¾æ— æ³•åŒ¹é…è§„åˆ™"
            row_copy["æ— æ³•åˆ†ç±»åŸå› "] = reason_msg
            unclassified_rows.append(row_copy)

        reason_msg = "ï¼›".join(sorted(row_reason_set)) if row_reason_set else ""
        if any_classified:
            status = "éƒ¨åˆ†å¯åˆ†ç±»" if reason_msg else "æ˜¯"
        else:
            status = "å¦"
            if not reason_msg:
                reason_msg = "æ ‡ç­¾æ— æ³•åŒ¹é…è§„åˆ™"
        split_counts_rows.append({
            "source": row.get("source"),
            "åŸå§‹æ ‡ç­¾ç»„åˆ": raw_label_combo,
            "æ‹†åˆ†æ¡æ•°": row_expand_count,
            "æ˜¯å¦å¯åˆ†ç±»": status,
            "æ— æ³•åˆ†ç±»åŸå› ": reason_msg,
        })

    category_files = []
    category_counts = {}
    for category, rows in category_rows.items():
        if not rows:
            continue
        category_counts[category] = len(rows)
        cat_df = pd.DataFrame(rows)
        cat_df = cat_df.sample(frac=1, random_state=random_seed).reset_index(drop=True)
        n_total = len(cat_df)
        n_train = int(n_total * train_ratio)
        n_val = int(n_total * val_ratio)
        n_test = n_total - n_train - n_val
        train_df = cat_df.iloc[:n_train]
        val_df = cat_df.iloc[n_train:n_train + n_val]
        test_df = cat_df.iloc[n_train + n_val:]
        safe_name = _safe_filename(category)
        out_path = output_dir / f"{safe_name}.xlsx"
        with pd.ExcelWriter(out_path) as writer:
            train_df.to_excel(writer, sheet_name="train", index=False)
            val_df.to_excel(writer, sheet_name="val", index=False)
            test_df.to_excel(writer, sheet_name="test", index=False)
        category_files.append(out_path)

    unclassified_path = output_dir / "unclassified.xlsx"
    if unclassified_rows:
        unclassified_df = pd.DataFrame(unclassified_rows)
        unclassified_df.to_excel(unclassified_path, index=False)
    else:
        pd.DataFrame(columns=list(df.columns) + ["æ— æ³•åˆ†ç±»åŸå› "]).to_excel(unclassified_path, index=False)

    split_counts_path = output_dir / "split_counts.xlsx"
    if split_counts_rows:
        pd.DataFrame(split_counts_rows).to_excel(split_counts_path, index=False)
    else:
        pd.DataFrame(columns=["source", "åŸå§‹æ ‡ç­¾ç»„åˆ", "æ‹†åˆ†æ¡æ•°", "æ˜¯å¦å¯åˆ†ç±»", "æ— æ³•åˆ†ç±»åŸå› "]).to_excel(split_counts_path, index=False)

    classified_total = sum(category_counts.values())
    return {
        "output_dir": output_dir,
        "category_files": category_files,
        "unclassified": unclassified_path,
        "split_counts": split_counts_path,
        "summary": {
            "categories": len(category_rows),
            "classified": classified_total,
            "unclassified": len(unclassified_rows),
            "category_counts": category_counts,
        },
    }


def summarize_unclassified(
        unclassified_excel_path: str,
        output_dir: str,
        json_columns: Optional[list] = None,
):
    """
    æ±‡æ€»æ— æ³•åˆ†ç±»æ•°æ®çš„åŸå› ä¸æ ‡ç­¾ç»Ÿè®¡ï¼Œè¾“å‡ºåˆ°å•ä¸ªExcelæ–‡ä»¶ã€‚
    """
    if not os.path.exists(unclassified_excel_path):
        raise FileNotFoundError(f"æ— æ³•åˆ†ç±»æ–‡ä»¶ä¸å­˜åœ¨ï¼š{unclassified_excel_path}")

    df = pd.read_excel(unclassified_excel_path)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    if json_columns is None:
        json_columns = []
        if "æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®" in df.columns:
            json_columns.append("æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®")
        if "ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®" in df.columns:
            json_columns.append("ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®")

    reason_col = "æ— æ³•åˆ†ç±»åŸå› "
    if reason_col not in df.columns:
        df[reason_col] = "æœªçŸ¥åŸå› "

    reason_counts = df[reason_col].fillna("æœªçŸ¥åŸå› ").value_counts().reset_index()
    reason_counts.columns = ["åŸå› ", "æ•°é‡"]

    label_counter = {}
    reason_label_counter = {}

    # reason_label_pattern = re.compile(r"^æ ‡ç­¾(.+?)(æœªåœ¨è§„åˆ™ä¸­å®šä¹‰|åœ¨è§„åˆ™ä¸­æ˜ å°„å¤šä¸ªç±»åˆ«)$")
    reason_label_pattern = re.compile(r"^æ ‡ç­¾(.+?)(æœªåœ¨è§„åˆ™ä¸­å®šä¹‰)$")

    for _, row in df.iterrows():
        reason = row.get(reason_col, "æœªçŸ¥åŸå› ")
        json_str = None
        for col in json_columns:
            if col in row and isinstance(row[col], str) and row[col]:
                json_str = row[col]
                break

        # ä¼˜å…ˆä½¿ç”¨â€œæ— æ³•åˆ†ç±»æ ‡ç­¾â€åˆ—
        labels = []
        if "æ— æ³•åˆ†ç±»æ ‡ç­¾" in df.columns:
            raw_unclassified = row.get("æ— æ³•åˆ†ç±»æ ‡ç­¾")
            labels = _split_object_labels(raw_unclassified)

        if not labels:
            objects, error = _parse_objects(json_str)
            if objects is None or error:
                # å°è¯•ä»åŸå› ä¸­æå–æ ‡ç­¾
                match = reason_label_pattern.match(str(reason))
                if match:
                    labels = [match.group(1)]
                else:
                    label_counter["æ— æ ‡ç­¾"] = label_counter.get("æ— æ ‡ç­¾", 0) + 1
                    reason_label_counter[("æ— æ ‡ç­¾", reason)] = reason_label_counter.get(("æ— æ ‡ç­¾", reason), 0) + 1
                    continue
            else:
                for obj in objects:
                    if isinstance(obj, dict) and obj.get("name"):
                        labels.extend(_split_object_labels(obj.get("name")))
                labels = list(dict.fromkeys(labels))
                if not labels:
                    label_counter["æ— æ ‡ç­¾"] = label_counter.get("æ— æ ‡ç­¾", 0) + 1
                    reason_label_counter[("æ— æ ‡ç­¾", reason)] = reason_label_counter.get(("æ— æ ‡ç­¾", reason), 0) + 1
                    continue

        for label in labels:
            label_counter[label] = label_counter.get(label, 0) + 1
            reason_label_counter[(label, reason)] = reason_label_counter.get((label, reason), 0) + 1

    label_summary = pd.DataFrame(
        [{"æ ‡ç­¾": k, "æ•°é‡": v} for k, v in label_counter.items()]
    ).sort_values("æ•°é‡", ascending=False)

    reason_label_summary = pd.DataFrame(
        [{"æ ‡ç­¾": k[0], "åŸå› ": k[1], "æ•°é‡": v} for k, v in reason_label_counter.items()]
    ).sort_values("æ•°é‡", ascending=False)

    out_path = output_dir / "unclassified_summary.xlsx"
    with pd.ExcelWriter(out_path) as writer:
        reason_counts.to_excel(writer, sheet_name="reason_summary", index=False)
        label_summary.to_excel(writer, sheet_name="label_summary", index=False)
        reason_label_summary.to_excel(writer, sheet_name="reason_label", index=False)

    return out_path


def _extract_boxes_with_labels(json_str: str):
    if pd.isna(json_str) or not isinstance(json_str, str):
        return []
    try:
        data = json.loads(json_str)
    except json.JSONDecodeError:
        return []
    objects = data.get("objects", [])
    boxes = []
    for obj in objects:
        if not isinstance(obj, dict):
            continue
        label = obj.get("name")
        ptlist = obj.get("polygon", {}).get("ptList", [])
        points = []
        for p in ptlist:
            if isinstance(p, dict) and "x" in p and "y" in p and p["x"] is not None and p["y"] is not None:
                points.append((p["x"], p["y"]))
        if len(points) < 2:
            continue
        xs = [p[0] for p in points]
        ys = [p[1] for p in points]
        x1, x2 = min(xs), max(xs)
        y1, y2 = min(ys), max(ys)
        boxes.append((label, x1, y1, x2, y2))
    return boxes


def _safe_image_stem(source: str, idx: int) -> str:
    if not source:
        return f"image_{idx}"
    name = str(source).split("/")[-1].split("?")[0]
    name = os.path.splitext(name)[0]
    name = _safe_filename(name)
    return f"{idx:06d}_{name}"


def _ensure_image_cached(source: str, cache_dir: Path, timeout: int = 15):
    cache_dir.mkdir(parents=True, exist_ok=True)
    filename = str(source).split("/")[-1].split("?")[0]
    if not filename:
        filename = f"image_{abs(hash(source))}.jpg"
    filename = _safe_filename(filename)
    cache_path = cache_dir / filename
    if cache_path.exists():
        return cache_path
    if str(source).startswith("http"):
        try:
            resp = requests.get(source, stream=True, timeout=timeout)
            resp.raise_for_status()
            with open(cache_path, "wb") as f:
                for chunk in resp.iter_content(chunk_size=8192):
                    f.write(chunk)
            return cache_path
        except Exception:
            return None
    else:
        src_path = Path(source)
        if src_path.exists():
            try:
                cache_path.write_bytes(src_path.read_bytes())
                return cache_path
            except Exception:
                return None
    return None


def generate_yolo_datasets_from_excels(
        category_excels: list,
        output_dir: str,
        image_cache_dir: Optional[str] = None,
        source_col: str = "source",
        label_col: str = "åˆ†ç±»æ ‡ç­¾",
        json_col_primary: str = "æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®",
        json_col_fallback: str = "ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®",
        width_col: str = "width",
        height_col: str = "height",
        download_images: bool = True,
        random_seed: int = 42,
        class_order: Optional[list] = None,
        resume: bool = True,
        progress_callback=None,
):
    """
    æ ¹æ®åˆ†ç±»åçš„Excelç”ŸæˆYOLOæ ¼å¼æ•°æ®é›†ã€‚
    æ¯ä¸ªç±»åˆ«Excelç”Ÿæˆä¸€ä¸ªæ•°æ®é›†ç›®å½•ï¼ŒåŒ…å« images/labels/train|val|test å’Œ data.yamlã€‚
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    cache_dir = Path(image_cache_dir) if image_cache_dir else (output_dir / "image_cache")
    cache_dir.mkdir(parents=True, exist_ok=True)

    datasets = []
    dataset_name_map = {}
    skipped = []
    dataset_stats = {}
    total_rows = 0
    processed_rows = 0
    downloaded_images = 0

    current_category = None
    current_split = None
    current_file = None
    current_label = None
    current_excel = None
    current_row_idx = None

    used_dir_names = set()
    for idx_excel, excel_path in enumerate(category_excels):
        if not excel_path or not Path(excel_path).exists():
            continue
        excel_path = Path(excel_path)
        current_excel = excel_path.name
        category_name = excel_path.stem
        current_category = category_name
        base_dir_name = _safe_dataset_dir_name(category_name, f"category_{idx_excel:03d}")
        dir_name = base_dir_name
        suffix = 1
        while dir_name in used_dir_names:
            dir_name = f"{base_dir_name}_{suffix}"
            suffix += 1
        used_dir_names.add(dir_name)
        dataset_dir = output_dir / dir_name
        dataset_name_map[dataset_dir.name] = category_name
        images_root = dataset_dir / "images"
        labels_root = dataset_dir / "labels"
        for split in ["train", "val", "test"]:
            (images_root / split).mkdir(parents=True, exist_ok=True)
            (labels_root / split).mkdir(parents=True, exist_ok=True)

        xls = pd.ExcelFile(excel_path)
        split_sheets = [s for s in ["train", "val", "test"] if s in xls.sheet_names]
        if not split_sheets:
            continue

        all_labels = []
        split_dfs = {}
        for split in split_sheets:
            df_split = pd.read_excel(excel_path, sheet_name=split)
            split_dfs[split] = df_split
            if label_col in df_split.columns:
                all_labels.extend([str(v) for v in df_split[label_col].dropna()])
            total_rows += len(df_split)

        classes = sorted(list(dict.fromkeys(all_labels)))
        if class_order:
            ordered = [c for c in class_order if c in classes]
            remaining = [c for c in classes if c not in ordered]
            classes = ordered + remaining
        class_to_id = {name: i for i, name in enumerate(classes)}

        dataset_stats[category_name] = {"train": 0, "val": 0, "test": 0}
        if progress_callback:
            progress_callback(processed_rows, total_rows, downloaded_images, current_category, current_split, current_file, current_label, current_excel, current_row_idx)
        for split in split_sheets:
            current_split = split
            df_split = split_dfs[split]
            df_split = df_split.sample(frac=1, random_state=random_seed).reset_index(drop=True)
            for idx, row in df_split.iterrows():
                current_row_idx = idx
                current_file = _safe_image_stem(str(row.get(source_col)), idx)
                source = row.get(source_col)
                if not source:
                    skipped.append({"category": category_name, "reason": "ç¼ºå°‘source", "split": split})
                    processed_rows += 1
                    if progress_callback and (processed_rows % 50 == 0 or processed_rows == total_rows):
                        progress_callback(processed_rows, total_rows, downloaded_images, current_category, current_split, current_file, current_label, current_excel, current_row_idx)
                    continue

                label_value = row.get(label_col)
                if not label_value:
                    skipped.append({"category": category_name, "reason": "ç¼ºå°‘åˆ†ç±»æ ‡ç­¾", "split": split})
                    processed_rows += 1
                    if progress_callback and (processed_rows % 50 == 0 or processed_rows == total_rows):
                        progress_callback(processed_rows, total_rows, downloaded_images, current_category, current_split, current_file, current_label, current_excel, current_row_idx)
                    continue
                label_value = str(label_value)
                current_label = label_value
                if label_value not in class_to_id:
                    skipped.append({"category": category_name, "reason": "æ ‡ç­¾æœªåœ¨ç±»åˆ«åˆ—è¡¨ä¸­", "split": split})
                    processed_rows += 1
                    if progress_callback and (processed_rows % 50 == 0 or processed_rows == total_rows):
                        progress_callback(processed_rows, total_rows, downloaded_images, current_category, current_split, current_file, current_label, current_excel, current_row_idx)
                    continue

                image_stem = _safe_image_stem(str(source), idx)
                current_file = image_stem
                label_path = labels_root / split / f"{image_stem}.txt"
                if resume and label_path.exists() and label_path.stat().st_size > 0:
                    existing_images = list((images_root / split).glob(f"{image_stem}.*"))
                    if existing_images:
                        dataset_stats[category_name][split] += 1
                        processed_rows += 1
                        if progress_callback and (processed_rows % 50 == 0 or processed_rows == total_rows):
                            progress_callback(processed_rows, total_rows, downloaded_images, current_category, current_split, current_file, current_label, current_excel, current_row_idx)
                        continue

                json_str = None
                if json_col_primary in row and isinstance(row[json_col_primary], str) and row[json_col_primary]:
                    json_str = row[json_col_primary]
                elif json_col_fallback in row and isinstance(row[json_col_fallback], str) and row[json_col_fallback]:
                    json_str = row[json_col_fallback]

                boxes = _extract_boxes_with_labels(json_str)
                if not boxes:
                    skipped.append({"category": category_name, "reason": "æ ‡æ³¨æ¡†ä¸ºç©º", "split": split})
                    processed_rows += 1
                    if progress_callback and (processed_rows % 50 == 0 or processed_rows == total_rows):
                        progress_callback(processed_rows, total_rows, downloaded_images, current_category, current_split, current_file, current_label, current_excel, current_row_idx)
                    continue

                filtered_boxes = [b for b in boxes if b[0] == label_value]
                if not filtered_boxes:
                    skipped.append({"category": category_name, "reason": "æ— åŒ¹é…æ ‡ç­¾æ¡†", "split": split})
                    processed_rows += 1
                    if progress_callback and (processed_rows % 50 == 0 or processed_rows == total_rows):
                        progress_callback(processed_rows, total_rows, downloaded_images, current_category, current_split, current_file, current_label, current_excel, current_row_idx)
                    continue

                width = row.get(width_col)
                height = row.get(height_col)

                if (not width or not height) and json_str:
                    try:
                        data = json.loads(json_str)
                        width = width or data.get("width")
                        height = height or data.get("height")
                    except Exception:
                        pass

                image_path = None
                if download_images:
                    image_path = _ensure_image_cached(str(source), cache_dir)
                else:
                    if Path(str(source)).exists():
                        image_path = Path(str(source))

                if image_path and (not width or not height):
                    try:
                        with Image.open(image_path) as img:
                            width, height = img.size
                    except Exception:
                        pass

                if not width or not height:
                    skipped.append({"category": category_name, "reason": "ç¼ºå°‘å›¾åƒå°ºå¯¸", "split": split})
                    processed_rows += 1
                    if progress_callback and (processed_rows % 50 == 0 or processed_rows == total_rows):
                        progress_callback(processed_rows, total_rows, downloaded_images, current_category, current_split, current_file, current_label, current_excel, current_row_idx)
                    continue

                image_suffix = ".jpg"
                if image_path:
                    image_suffix = image_path.suffix if image_path.suffix else ".jpg"
                image_name = f"{image_stem}{image_suffix}"
                out_image = images_root / split / image_name

                if image_path:
                    try:
                        wrote_new = False
                        if not out_image.exists():
                            out_image.write_bytes(Path(image_path).read_bytes())
                            wrote_new = True
                        if wrote_new:
                            downloaded_images += 1
                    except Exception:
                        skipped.append({"category": category_name, "reason": "å›¾ç‰‡å†™å…¥å¤±è´¥", "split": split})
                        processed_rows += 1
                        if progress_callback and (processed_rows % 50 == 0 or processed_rows == total_rows):
                            progress_callback(processed_rows, total_rows, downloaded_images, current_category, current_split, current_file, current_label, current_excel, current_row_idx)
                        continue
                else:
                    skipped.append({"category": category_name, "reason": "å›¾ç‰‡ä¸‹è½½å¤±è´¥", "split": split})
                    processed_rows += 1
                    if progress_callback and (processed_rows % 50 == 0 or processed_rows == total_rows):
                        progress_callback(processed_rows, total_rows, downloaded_images, current_category, current_split, current_file, current_label, current_excel, current_row_idx)
                    continue

                label_lines = []
                for _, x1, y1, x2, y2 in filtered_boxes:
                    x1, x2 = min(x1, x2), max(x1, x2)
                    y1, y2 = min(y1, y2), max(y1, y2)
                    bw = max(x2 - x1, 0.0)
                    bh = max(y2 - y1, 0.0)
                    if bw <= 0 or bh <= 0:
                        continue
                    cx = (x1 + x2) / 2 / width
                    cy = (y1 + y2) / 2 / height
                    bw_n = bw / width
                    bh_n = bh / height
                    class_id = class_to_id[label_value]
                    label_lines.append(f"{class_id} {cx:.6f} {cy:.6f} {bw_n:.6f} {bh_n:.6f}")

                if not label_lines:
                    skipped.append({"category": category_name, "reason": "æ ‡æ³¨æ¡†æ— æ•ˆ", "split": split})
                    processed_rows += 1
                    if progress_callback and (processed_rows % 50 == 0 or processed_rows == total_rows):
                        progress_callback(processed_rows, total_rows, downloaded_images, current_category, current_split, current_file, current_label, current_excel, current_row_idx)
                    continue

                label_path = labels_root / split / f"{image_stem}.txt"
                label_path.write_text("\n".join(label_lines), encoding="utf-8")
                dataset_stats[category_name][split] += 1
                processed_rows += 1
                if progress_callback and (processed_rows % 50 == 0 or processed_rows == total_rows):
                    progress_callback(processed_rows, total_rows, downloaded_images, current_category, current_split, current_file, current_label, current_excel, current_row_idx)

        data_yaml = dataset_dir / "data.yaml"
        names_json = json.dumps(classes, ensure_ascii=False)
        data_yaml.write_text(
            "\n".join([
                f"path: {dataset_dir}",
                "train: images/train",
                "val: images/val",
                "test: images/test",
                f"nc: {len(classes)}",
                f"names: {names_json}",
            ]),
            encoding="utf-8",
        )

        datasets.append(dataset_dir)

    skipped_path = output_dir / "yolo_skipped.xlsx"
    if skipped:
        pd.DataFrame(skipped).to_excel(skipped_path, index=False)
    else:
        pd.DataFrame(columns=["category", "reason", "split"]).to_excel(skipped_path, index=False)

    if progress_callback:
        progress_callback(processed_rows, total_rows, downloaded_images, current_category, current_split, current_file, current_label, current_excel, current_row_idx)
    return {
        "datasets": datasets,
        "skipped": skipped_path,
        "stats": dataset_stats,
        "total": total_rows,
        "processed": processed_rows,
        "downloaded": downloaded_images,
        "dataset_name_map": dataset_name_map,
    }
# # # ---------------------- éœ€æ‰‹åŠ¨ä¿®æ”¹çš„å‚æ•° ----------------------
# input_csv_path = "other_data.csv"  # æ›¿æ¢ä¸ºä½ ç”Ÿæˆçš„CSVæ–‡ä»¶è·¯å¾„
# # --------------------------------------------------------------
#
# # è°ƒç”¨å‡½æ•°
# download_and_draw_annotations(input_csv_path)




# import pandas as pd
# import requests
# import json
# import os
# from pathlib import Path
# from sklearn.model_selection import train_test_split
# import yaml  # éœ€è¦é¢å¤–å®‰è£…pyyamlåº“
#
#
# def generate_yolo_dataset(input_csv_path,
#                           yolo_root="yolo_dataset",
#                           val_split=0.2,
#                           class_mapping=None):
#     # 1. åˆå§‹åŒ–ç±»åˆ«æ˜ å°„å’ŒID
#     if class_mapping is None:
#         class_mapping = {}
#         class_id = 0  # è‡ªåŠ¨åˆ†é…IDæ—¶åˆå§‹åŒ–
#     else:
#         # æ‰‹åŠ¨æŒ‡å®šæ˜ å°„æ—¶ï¼Œä»æœ€å¤§ID+1å¼€å§‹
#         class_id = max(class_mapping.values(), default=-1) + 1
#
#     # åˆ›å»ºæ–‡ä»¶å¤¹ç»“æ„
#     train_img_dir = os.path.join(yolo_root, "train", "images")
#     train_label_dir = os.path.join(yolo_root, "train", "labels")
#     val_img_dir = os.path.join(yolo_root, "val", "images")
#     val_label_dir = os.path.join(yolo_root, "val", "labels")
#     for dir_path in [train_img_dir, train_label_dir, val_img_dir, val_label_dir]:
#         Path(dir_path).mkdir(parents=True, exist_ok=True)
#
#     # 2. å›¾ç‰‡ä¸‹è½½å‡½æ•°
#     def download_image(url, save_path):
#         if os.path.exists(save_path):
#             return True
#         try:
#             response = requests.get(url, stream=True, timeout=15)
#             response.raise_for_status()
#             with open(save_path, "wb") as f:
#                 f.write(response.content)
#             return True
#         except Exception as e:
#             print(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥ {url}ï¼š{e}")
#             return False
#
#     # 3. æ ‡æ³¨è½¬æ¢å‡½æ•°
#     def json_to_yolo_annotation(json_str, img_width, img_height):
#         nonlocal class_mapping, class_id
#         yolo_lines = []
#         try:
#             if pd.isna(json_str) or not isinstance(json_str, str):
#                 return ""
#             data = json.loads(json_str)
#             objects = data.get("objects", [])
#             img_width = img_width or data.get("width", 1)
#             img_height = img_height or data.get("height", 1)
#
#             for obj in objects:
#                 if not isinstance(obj, dict):
#                     continue
#                 # å¤„ç†ç±»åˆ«ID
#                 obj_name = obj.get("name", "unknown")
#                 if obj_name not in class_mapping:
#                     class_mapping[obj_name] = class_id
#                     class_id += 1
#                 cid = class_mapping[obj_name]
#
#                 # å¤„ç†åæ ‡
#                 ptlist = obj.get("polygon", {}).get("ptList", [])
#                 if len(ptlist) != 2:
#                     continue
#                 p1, p2 = ptlist
#                 x1, y1 = min(p1["x"], p2["x"]), min(p1["y"], p2["y"])
#                 x2, y2 = max(p1["x"], p2["x"]), max(p1["y"], p2["y"])
#
#                 # è½¬æ¢ä¸ºYOLOæ ¼å¼ï¼ˆå½’ä¸€åŒ–ï¼‰
#                 x_center = (x1 + x2) / 2 / img_width
#                 y_center = (y1 + y2) / 2 / img_height
#                 width = (x2 - x1) / img_width
#                 height = (y2 - y1) / img_height
#
#                 # é™åˆ¶èŒƒå›´åœ¨0~1ä¹‹é—´
#                 x_center = max(0.001, min(0.999, x_center))
#                 y_center = max(0.001, min(0.999, y_center))
#                 width = max(0.001, min(0.999, width))
#                 height = max(0.001, min(0.999, height))
#
#                 yolo_lines.append(f"{cid} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}")
#
#         except json.JSONDecodeError:
#             print(f"JSONè§£æå¤±è´¥ï¼ˆå‰50å­—ç¬¦ï¼‰ï¼š{str(json_str)[:50]}...")
#         except Exception as e:
#             print(f"æ ‡æ³¨è½¬æ¢å¤±è´¥ï¼š{e}")
#
#         return "\n".join(yolo_lines)
#
#     # 4. è¯»å–å¹¶å¤„ç†CSV
#     try:
#         df = pd.read_csv(input_csv_path, encoding="utf-8-sig")
#         print(f"æˆåŠŸè¯»å–CSVï¼Œå…± {len(df)} è¡Œæ•°æ®")
#     except Exception as e:
#         print(f"CSVå¤„ç†å¤±è´¥ï¼š{e}")
#         return
#
#     # 5. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
#     train_df, val_df = train_test_split(df, test_size=val_split, random_state=42)
#     print(f"è®­ç»ƒé›†ï¼š{len(train_df)} æ¡ | éªŒè¯é›†ï¼š{len(val_df)} æ¡")
#
#     # 6. å¤„ç†æ•°æ®é›†ï¼ˆå›¾ç‰‡+æ ‡æ³¨ï¼‰
#     def process_split(df, img_dir, label_dir, split_name):
#         success = 0
#         fail = 0
#         for idx, row in df.iterrows():
#             img_url = row["source"]
#             json_anno = row["æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®"]
#             img_width = row.get("width")
#             img_height = row.get("height")
#
#             # ç”Ÿæˆæ–‡ä»¶å
#             img_filename = os.path.splitext(os.path.basename(img_url))[0] + ".jpg"
#             label_filename = os.path.splitext(img_filename)[0] + ".txt"
#
#             # ä¸‹è½½å›¾ç‰‡
#             img_path = os.path.join(img_dir, img_filename)
#             if not download_image(img_url, img_path):
#                 fail += 1
#                 continue
#
#             # ç”Ÿæˆæ ‡æ³¨
#             yolo_anno = json_to_yolo_annotation(json_anno, img_width, img_height)
#             with open(os.path.join(label_dir, label_filename), "w", encoding="utf-8") as f:
#                 f.write(yolo_anno)
#
#             success += 1
#         print(f"{split_name}é›†ï¼šæˆåŠŸ {success} æ¡ | å¤±è´¥ {fail} æ¡")
#
#     process_split(train_df, train_img_dir, train_label_dir, "è®­ç»ƒ")
#     process_split(val_df, val_img_dir, val_label_dir, "éªŒè¯")
#
#     # 7. ç”Ÿæˆç±»åˆ«æ˜ å°„æ–‡ä»¶ï¼ˆyolo_classes.txtï¼‰
#     class_file = os.path.join(yolo_root, "yolo_classes.txt")
#     with open(class_file, "w", encoding="utf-8") as f:
#         for cls_name, cls_id in sorted(class_mapping.items(), key=lambda x: x[1]):
#             f.write(f"{cls_id} {cls_name}\n")
#
#     # 8. ç”ŸæˆYOLOè®­ç»ƒæ‰€éœ€çš„yamlæ–‡ä»¶ï¼ˆå…³é”®æ–°å¢åŠŸèƒ½ï¼‰
#     yaml_data = {
#         "path": os.path.abspath(yolo_root),  # æ•°æ®é›†æ ¹ç›®å½•ç»å¯¹è·¯å¾„
#         "train": "train/images",  # è®­ç»ƒé›†å›¾ç‰‡ç›¸å¯¹è·¯å¾„
#         "val": "val/images",  # éªŒè¯é›†å›¾ç‰‡ç›¸å¯¹è·¯å¾„
#         "nc": len(class_mapping),  # ç±»åˆ«æ•°é‡
#         "names": [cls_name for cls_name, _ in sorted(class_mapping.items(), key=lambda x: x[1])]  # ç±»åˆ«åç§°åˆ—è¡¨
#     }
#
#     yaml_path = os.path.join(yolo_root, "dataset.yaml")
#     with open(yaml_path, "w", encoding="utf-8") as f:
#         yaml.dump(yaml_data, f, sort_keys=False, allow_unicode=True)  # ä¿ç•™ä¸­æ–‡ä¸”ä¸æ’åºé”®
#
#     # 9. è¾“å‡ºæœ€ç»ˆç»“æœ
#     print(f"\næ•°æ®é›†ç”Ÿæˆå®Œæˆï¼æ ¹ç›®å½•ï¼š{os.path.abspath(yolo_root)}")
#     print(f"ç±»åˆ«æ•°ï¼š{len(class_mapping)} | ç±»åˆ«æ–‡ä»¶ï¼š{class_file}")
#     print(f"YOLOé…ç½®æ–‡ä»¶ï¼š{yaml_path}ï¼ˆå¯ç›´æ¥ç”¨äºè®­ç»ƒï¼‰")
#
#
# # ---------------------- é…ç½®å‚æ•° ----------------------
# input_csv_path = "other_data.csv"  # æ›¿æ¢ä¸ºä½ çš„CSVè·¯å¾„
# val_split = 0.2  # éªŒè¯é›†æ¯”ä¾‹
# class_mapping = {
#     # ç¤ºä¾‹ï¼šæ‰‹åŠ¨æŒ‡å®šç±»åˆ«IDï¼ˆå¯é€‰ï¼‰
#     # "å›½æ§|ç”Ÿç‰©åœˆ|è™«å®³|å›½æ§å°ºè –|ä¸¥é‡": 0,
#     # "å›½æ§|ç”Ÿç‰©åœˆ|è™«å®³|å›½æ§å°ºè –|æ˜æ˜¾": 1
# }
# # ------------------------------------------------------
#
# # å®‰è£…ä¾èµ–ï¼ˆè‹¥æœªå®‰è£…ï¼‰
# # pip install pandas requests pillow scikit-learn pyyaml
#
# # ç”Ÿæˆæ•°æ®é›†
# generate_yolo_dataset(input_csv_path, val_split=val_split, class_mapping=class_mapping)

import csv
import json
from collections import defaultdict
from typing import Optional, List


def process_detection_data(
        input_csv: str,
        output_matched: str,
        output_unmatched: str,
        target_classes: Optional[List[str]] = None,
        min_total_count: int = 0  # æ”¹ä¸ºâ€œå…¨é‡æ•°æ®ä¸­ç±»åˆ«æ€»æ¬¡æ•°>é˜ˆå€¼â€
) -> None:
    """
    å¤„ç†ç›®æ ‡æ£€æµ‹æ•°æ®ï¼šç»Ÿè®¡ç±»åˆ«æ•°é‡ + æŒ‰æ¡ä»¶ç­›é€‰æ•°æ®
    ç­›é€‰é€»è¾‘ï¼šå…ˆåˆ¤æ–­ç›®æ ‡ç±»åˆ«åœ¨å…¨é‡æ•°æ®ä¸­çš„æ€»æ¬¡æ•°æ˜¯å¦>min_total_countï¼Œ
              å†ç­›é€‰å‡ºåŒ…å«è¿™äº›â€œè¾¾æ ‡ç±»åˆ«â€çš„æ‰€æœ‰è¡Œ

    Args:
        input_csv: è¾“å…¥CSVæ–‡ä»¶è·¯å¾„ï¼ˆother_data.csvï¼‰
        output_matched: ç¬¦åˆæ¡ä»¶çš„æ•°æ®ä¿å­˜è·¯å¾„
        output_unmatched: ä¸ç¬¦åˆæ¡ä»¶çš„æ•°æ®ä¿å­˜è·¯å¾„
        target_classes: ç›®æ ‡æ£€æµ‹ç±»åˆ«åˆ—è¡¨ï¼ˆå¦‚["å›½æ§|æœ¬ä½“|ä¸»å¹²", "å›½æ§|æœ¬ä½“|æ ‘å† "]ï¼‰ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶ç±»åˆ«
        min_total_count: å…¨é‡æ•°æ®ä¸­ç±»åˆ«çš„æœ€å°æ€»æ¬¡æ•°ï¼ˆéœ€å¤§äºè¯¥å€¼ï¼‰
    """
    # æ ¡éªŒå‚æ•°
    if min_total_count < 0:
        raise ValueError("æœ€å°æ€»æ¬¡æ•°é˜ˆå€¼ä¸èƒ½ä¸ºè´Ÿæ•°")
    if target_classes is None:
        target_classes = []

    # ç¬¬ä¸€æ­¥ï¼šç»Ÿè®¡æ‰€æœ‰ç±»åˆ«çš„å…¨é‡æ€»æ¬¡æ•°
    class_counter = defaultdict(int)
    total_rows = 0
    with open(input_csv, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        if 'æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®' not in reader.fieldnames:
            raise KeyError("CSVæ–‡ä»¶ä¸­ç¼ºå°‘'æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®'åˆ—")

        for row in reader:
            total_rows += 1
            try:
                config_str = row['æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®'].strip() or '{}'
                config = json.loads(config_str)
                objects = config.get('objects', [])
                # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å…¨é‡æ€»æ¬¡æ•°ï¼ˆä¸é™åˆ¶å•è¡Œé‡å¤ï¼‰
                for obj in objects:
                    class_name = obj.get('name', 'æœªçŸ¥ç±»åˆ«').strip()
                    if class_name:
                        class_counter[class_name] += 1
            except json.JSONDecodeError:
                print(f"è­¦å‘Šï¼šç¬¬{total_rows}è¡ŒJSONæ ¼å¼é”™è¯¯ï¼Œè·³è¿‡ç»Ÿè®¡")
            except Exception as e:
                print(f"è­¦å‘Šï¼šç¬¬{total_rows}è¡Œç»Ÿè®¡å¤±è´¥ï¼ˆ{str(e)}ï¼‰ï¼Œè·³è¿‡")

    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print("\n===== æ£€æµ‹ç›®æ ‡ç±»åˆ«ç»Ÿè®¡ =====")
    print(f"æ€»æ•°æ®è¡Œæ•°ï¼š{total_rows}")
    valid_classes = {k: v for k, v in class_counter.items() if k.strip()}
    print(f"æ£€æµ‹åˆ°çš„æœ‰æ•ˆç±»åˆ«æ€»æ•°ï¼š{len(valid_classes)}")
    for class_name, count in sorted(valid_classes.items(), key=lambda x: x[1], reverse=True):
        print(f"  {class_name}: {count}æ¬¡")
    print("===========================\n")

    # ç¬¬äºŒæ­¥ï¼šç¡®å®šâ€œè¾¾æ ‡ç±»åˆ«â€ï¼ˆç›®æ ‡ç±»åˆ«ä¸­æ€»æ¬¡æ•°>min_total_countçš„ç±»åˆ«ï¼‰
    qualified_classes = []
    if target_classes:
        # ç­›é€‰å‡ºç›®æ ‡ç±»åˆ«ä¸­æ€»æ¬¡æ•°è¾¾æ ‡çš„ç±»åˆ«
        for cls in target_classes:
            cls = cls.strip()
            total_count = class_counter.get(cls, 0)
            if total_count > min_total_count:
                qualified_classes.append(cls)
                print(f"ç±»åˆ«ã€Œ{cls}ã€è¾¾æ ‡ï¼ˆæ€»æ¬¡æ•°={total_count} > {min_total_count}ï¼‰")
            else:
                print(f"ç±»åˆ«ã€Œ{cls}ã€æœªè¾¾æ ‡ï¼ˆæ€»æ¬¡æ•°={total_count} â‰¤ {min_total_count}ï¼‰")
    else:
        # ä¸é™åˆ¶ç±»åˆ«æ—¶ï¼Œæ‰€æœ‰æ€»æ¬¡æ•°>min_total_countçš„ç±»åˆ«éƒ½ç®—è¾¾æ ‡
        qualified_classes = [cls for cls, count in class_counter.items() if count > min_total_count]
        print(f"ä¸é™åˆ¶ç±»åˆ«ï¼Œè¾¾æ ‡ç±»åˆ«æ•°é‡ï¼š{len(qualified_classes)}ï¼ˆæ€»æ¬¡æ•°>{min_total_count}ï¼‰")

    if not qualified_classes:
        print(f"\næ— è¾¾æ ‡ç±»åˆ«ï¼Œæ‰€æœ‰æ•°æ®å½’å…¥æœªåŒ¹é…æ–‡ä»¶")
        # ç›´æ¥å¤åˆ¶æ‰€æœ‰æ•°æ®åˆ°æœªåŒ¹é…æ–‡ä»¶
        with open(input_csv, 'r', encoding='utf-8') as infile, \
                open(output_unmatched, 'w', encoding='utf-8', newline='') as unmatchfile:
            reader = csv.DictReader(infile)
            writer = csv.DictWriter(unmatchfile, fieldnames=reader.fieldnames)
            writer.writeheader()
            writer.writerows(reader)
        return

    # ç¬¬ä¸‰æ­¥ï¼šç­›é€‰å‡ºåŒ…å«â€œè¾¾æ ‡ç±»åˆ«â€çš„æ‰€æœ‰è¡Œ
    with open(input_csv, 'r', encoding='utf-8') as infile, \
            open(output_matched, 'w', encoding='utf-8', newline='') as matchfile, \
            open(output_unmatched, 'w', encoding='utf-8', newline='') as unmatchfile:

        reader = csv.DictReader(infile)
        fieldnames = reader.fieldnames
        match_writer = csv.DictWriter(matchfile, fieldnames=fieldnames)
        unmatch_writer = csv.DictWriter(unmatchfile, fieldnames=fieldnames)
        match_writer.writeheader()
        unmatch_writer.writeheader()

        matched_count = 0

        for row in reader:
            try:
                config_str = row['æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®'].strip() or '{}'
                config = json.loads(config_str)
                objects = config.get('objects', [])
                # æ£€æŸ¥å½“å‰è¡Œæ˜¯å¦åŒ…å«ä»»ä½•ä¸€ä¸ªè¾¾æ ‡ç±»åˆ«
                row_has_qualified = False
                for obj in objects:
                    class_name = obj.get('name', '').strip()
                    if class_name in qualified_classes:
                        row_has_qualified = True
                        break  # æ‰¾åˆ°ä¸€ä¸ªå°±å¤Ÿï¼Œæ— éœ€ç»§ç»­æ£€æŸ¥

                if row_has_qualified:
                    match_writer.writerow(row)
                    matched_count += 1
                else:
                    unmatch_writer.writerow(row)

            except json.JSONDecodeError:
                unmatch_writer.writerow(row)
                print(f"è­¦å‘Šï¼šç¬¬{reader.line_num}è¡ŒJSONæ ¼å¼é”™è¯¯ï¼Œå½’å…¥æœªåŒ¹é…æ•°æ®")
            except Exception as e:
                unmatch_writer.writerow(row)
                print(f"è­¦å‘Šï¼šç¬¬{reader.line_num}è¡Œç­›é€‰å¤±è´¥ï¼ˆ{str(e)}ï¼‰ï¼Œå½’å…¥æœªåŒ¹é…æ•°æ®")

    # è¾“å‡ºç­›é€‰ç»“æœç»Ÿè®¡
    print(f"\nç­›é€‰å®Œæˆï¼š")
    print(f"è¾¾æ ‡ç±»åˆ«ï¼š{qualified_classes}")
    print(f"ç¬¦åˆæ¡ä»¶çš„æ•°æ®ï¼ˆåŒ…å«è¾¾æ ‡ç±»åˆ«ï¼‰ï¼š{matched_count}æ¡ï¼ˆå·²ä¿å­˜è‡³{output_matched}ï¼‰")
    print(f"ä¸ç¬¦åˆæ¡ä»¶çš„æ•°æ®ï¼š{total_rows - matched_count}æ¡ï¼ˆå·²ä¿å­˜è‡³{output_unmatched}ï¼‰")


# ä½¿ç”¨ç¤ºä¾‹ï¼ˆä½ çš„éœ€æ±‚ï¼šç±»åˆ«=['å›½æ§|æœ¬ä½“|ä¸»å¹²', 'å›½æ§|æœ¬ä½“|æ ‘å† ']ï¼Œæ€»æ¬¡æ•°>500ï¼‰
# if __name__ == "__main__":
#     process_detection_data(
#         input_csv="other_data.csv",
#         output_matched="matched_data.csv",
#         output_unmatched="unmatched_data.csv",
#         target_classes=[],#"å›½æ§|æœ¬ä½“|ä¸»å¹²", "å›½æ§|æœ¬ä½“|æ ‘å† "
#         min_total_count=0  # å…¨é‡æ€»æ¬¡æ•°>500
#     )

#
# import pandas as pd
# import requests
# import json
# import os
# from pathlib import Path
# from sklearn.model_selection import train_test_split
# import yaml
# from tqdm import tqdm  # ç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡
#
# def generate_yolo_dataset(input_csv_path,
#                           yolo_root="yolo_dataset",
#                           val_split=0.2,
#                           class_mapping=None):
#     # 1. åˆå§‹åŒ–ç±»åˆ«æ˜ å°„å’ŒID
#     if class_mapping is None:
#         class_mapping = {}
#         class_id = 0  # è‡ªåŠ¨åˆ†é…IDæ—¶åˆå§‹åŒ–
#     else:
#         # æ‰‹åŠ¨æŒ‡å®šæ˜ å°„æ—¶ï¼Œä»æœ€å¤§ID+1å¼€å§‹
#         class_id = max(class_mapping.values(), default=-1) + 1
#
#     # åˆ›å»ºæ–‡ä»¶å¤¹ç»“æ„
#     train_img_dir = os.path.join(yolo_root, "train", "images")
#     train_label_dir = os.path.join(yolo_root, "train", "labels")
#     val_img_dir = os.path.join(yolo_root, "val", "images")
#     val_label_dir = os.path.join(yolo_root, "val", "labels")
#     for dir_path in [train_img_dir, train_label_dir, val_img_dir, val_label_dir]:
#         Path(dir_path).mkdir(parents=True, exist_ok=True)
#
#     # 2. å›¾ç‰‡ä¸‹è½½å‡½æ•°
#     def download_image(url, save_path):
#         if os.path.exists(save_path):
#             return True
#         try:
#             response = requests.get(url, stream=True, timeout=15)
#             response.raise_for_status()
#             with open(save_path, "wb") as f:
#                 f.write(response.content)
#             return True
#         except Exception as e:
#             print(f"\nå›¾ç‰‡ä¸‹è½½å¤±è´¥ {url}ï¼š{e}")  # æ¢è¡Œé¿å…æ‰“æ–­è¿›åº¦æ¡
#             return False
#
#     # 3. æ ‡æ³¨è½¬æ¢å‡½æ•°
#     def json_to_yolo_annotation(json_str, img_width, img_height):
#         nonlocal class_mapping, class_id
#         yolo_lines = []
#         try:
#             if pd.isna(json_str) or not isinstance(json_str, str):
#                 return ""
#             data = json.loads(json_str)
#             objects = data.get("objects", [])
#             img_width = img_width or data.get("width", 1)
#             img_height = img_height or data.get("height", 1)
#
#             for obj in objects:
#                 if not isinstance(obj, dict):
#                     continue
#                 # å¤„ç†ç±»åˆ«ID
#                 obj_name = obj.get("name", "unknown")
#                 if obj_name not in class_mapping:
#                     class_mapping[obj_name] = class_id
#                     class_id += 1
#                 cid = class_mapping[obj_name]
#
#                 # å¤„ç†åæ ‡
#                 ptlist = obj.get("polygon", {}).get("ptList", [])
#                 if len(ptlist) != 2:
#                     continue
#                 p1, p2 = ptlist
#                 x1, y1 = min(p1["x"], p2["x"]), min(p1["y"], p2["y"])
#                 x2, y2 = max(p1["x"], p2["x"]), max(p1["y"], p2["y"])
#
#                 # è½¬æ¢ä¸ºYOLOæ ¼å¼ï¼ˆå½’ä¸€åŒ–ï¼‰
#                 x_center = (x1 + x2) / 2 / img_width
#                 y_center = (y1 + y2) / 2 / img_height
#                 width = (x2 - x1) / img_width
#                 height = (y2 - y1) / img_height
#
#                 # é™åˆ¶èŒƒå›´åœ¨0~1ä¹‹é—´
#                 x_center = max(0.001, min(0.999, x_center))
#                 y_center = max(0.001, min(0.999, y_center))
#                 width = max(0.001, min(0.999, width))
#                 height = max(0.001, min(0.999, height))
#
#                 yolo_lines.append(f"{cid} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}")
#
#         except json.JSONDecodeError:
#             print(f"\nJSONè§£æå¤±è´¥ï¼ˆå‰50å­—ç¬¦ï¼‰ï¼š{str(json_str)[:50]}...")
#         except Exception as e:
#             print(f"\næ ‡æ³¨è½¬æ¢å¤±è´¥ï¼š{e}")
#
#         return "\n".join(yolo_lines)
#
#     # 4. è¯»å–å¹¶å¤„ç†CSV
#     try:
#         df = pd.read_csv(input_csv_path, encoding="utf-8-sig")
#         print(f"æˆåŠŸè¯»å–CSVï¼Œå…± {len(df)} è¡Œæ•°æ®")
#     except Exception as e:
#         print(f"CSVå¤„ç†å¤±è´¥ï¼š{e}")
#         return
#
#     # 5. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
#     train_df, val_df = train_test_split(df, test_size=val_split, random_state=42)
#     print(f"è®­ç»ƒé›†ï¼š{len(train_df)} æ¡ | éªŒè¯é›†ï¼š{len(val_df)} æ¡")
#
#     # 6. å¤„ç†æ•°æ®é›†ï¼ˆå›¾ç‰‡+æ ‡æ³¨ï¼‰ï¼Œæ·»åŠ è¿›åº¦æ¡
#     def process_split(df, img_dir, label_dir, split_name):
#         success = 0
#         fail = 0
#         # ä½¿ç”¨tqdmåˆ›å»ºè¿›åº¦æ¡ï¼Œdescæ˜¾ç¤ºå½“å‰å¤„ç†é˜¶æ®µ
#         for idx, row in tqdm(enumerate(df.iterrows()),
#                              total=len(df),
#                              desc=f"å¤„ç†{split_name}é›†",
#                              unit="æ¡"):
#             _, row = row  # è§£åŒ…iterrowsè¿”å›çš„(index, row)
#             img_url = row["source"]
#             json_anno = row["æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®"]
#             img_width = row.get("width")
#             img_height = row.get("height")
#
#             # ç”Ÿæˆæ–‡ä»¶å
#             img_filename = os.path.splitext(os.path.basename(img_url))[0] + ".jpg"
#             label_filename = os.path.splitext(img_filename)[0] + ".txt"
#
#             # ä¸‹è½½å›¾ç‰‡
#             img_path = os.path.join(img_dir, img_filename)
#             if not download_image(img_url, img_path):
#                 fail += 1
#                 continue
#
#             # ç”Ÿæˆæ ‡æ³¨
#             yolo_anno = json_to_yolo_annotation(json_anno, img_width, img_height)
#             with open(os.path.join(label_dir, label_filename), "w", encoding="utf-8") as f:
#                 f.write(yolo_anno)
#
#             success += 1
#         print(f"{split_name}é›†ï¼šæˆåŠŸ {success} æ¡ | å¤±è´¥ {fail} æ¡")
#
#     process_split(train_df, train_img_dir, train_label_dir, "è®­ç»ƒ")
#     process_split(val_df, val_img_dir, val_label_dir, "éªŒè¯")
#
#     # 7. ç”Ÿæˆç±»åˆ«æ˜ å°„æ–‡ä»¶ï¼ˆyolo_classes.txtï¼‰
#     class_file = os.path.join(yolo_root, "yolo_classes.txt")
#     with open(class_file, "w", encoding="utf-8") as f:
#         for cls_name, cls_id in sorted(class_mapping.items(), key=lambda x: x[1]):
#             f.write(f"{cls_id} {cls_name}\n")
#
#     # 8. ç”ŸæˆYOLOè®­ç»ƒæ‰€éœ€çš„yamlæ–‡ä»¶
#     yaml_data = {
#         "path": os.path.abspath(yolo_root),
#         "train": "train/images",
#         "val": "val/images",
#         "nc": len(class_mapping),
#         "names": [cls_name for cls_name, _ in sorted(class_mapping.items(), key=lambda x: x[1])]
#     }
#
#     yaml_path = os.path.join(yolo_root, "dataset.yaml")
#     with open(yaml_path, "w", encoding="utf-8") as f:
#         yaml.dump(yaml_data, f, sort_keys=False, allow_unicode=True)
#
#     # 9. è¾“å‡ºæœ€ç»ˆç»“æœ
#     print(f"\næ•°æ®é›†ç”Ÿæˆå®Œæˆï¼æ ¹ç›®å½•ï¼š{os.path.abspath(yolo_root)}")
#     print(f"ç±»åˆ«æ•°ï¼š{len(class_mapping)} | ç±»åˆ«æ–‡ä»¶ï¼š{class_file}")
#     print(f"YOLOé…ç½®æ–‡ä»¶ï¼š{yaml_path}ï¼ˆå¯ç›´æ¥ç”¨äºè®­ç»ƒï¼‰")
#
#
# # # ---------------------- é…ç½®å‚æ•° ----------------------
# input_csv_path = "matched_data.csv"  # æ›¿æ¢ä¸ºä½ çš„CSVè·¯å¾„
# val_split = 0.2  # éªŒè¯é›†æ¯”ä¾‹
# class_mapping = {
#     # ç¤ºä¾‹ï¼šæ‰‹åŠ¨æŒ‡å®šç±»åˆ«IDï¼ˆå¯é€‰ï¼‰
#     # "å›½æ§|ç”Ÿç‰©åœˆ|è™«å®³|å›½æ§å°ºè –|ä¸¥é‡": 0,
#     # "å›½æ§|ç”Ÿç‰©åœˆ|è™«å®³|å›½æ§å°ºè –|æ˜æ˜¾": 1
# }
# # ------------------------------------------------------
#
# # å®‰è£…ä¾èµ–ï¼ˆè‹¥æœªå®‰è£…ï¼‰
# # pip install pandas requests pillow scikit-learn pyyaml tqdm
#
# # ç”Ÿæˆæ•°æ®é›†
# generate_yolo_dataset(input_csv_path, val_split=val_split, class_mapping=class_mapping)

import pandas as pd
import requests
import json
import os
import shutil
from pathlib import Path
from sklearn.model_selection import train_test_split
import yaml
from tqdm import tqdm
from typing import Dict, Optional, List, Tuple


# ---------------------- æ ¸å¿ƒå‡½æ•°ï¼šä»CSVæŒ‡å®šåˆ—è¯»å–ç›®æ ‡æ ‡ç­¾ ----------------------
def load_target_classes_from_csv(csv_file: str, target_column: str) -> List[str]:
    """
    ä»CSVæ–‡ä»¶çš„æŒ‡å®šåˆ—è¯»å–ç›®æ ‡æ ‡ç­¾ï¼Œè‡ªåŠ¨å»é‡ã€è¿‡æ»¤ç©ºå€¼å’Œæ— æ•ˆæ ‡ç­¾
    :param csv_file: æ ‡ç­¾CSVæ–‡ä»¶è·¯å¾„
    :param target_column: è¦è¯»å–çš„æ ‡ç­¾åˆ—åï¼ˆå¿…é¡»ä¸CSVè¡¨å¤´ä¸€è‡´ï¼‰
    :return: å»é‡åçš„æœ‰æ•ˆæ ‡ç­¾åˆ—è¡¨ï¼ˆä¸ºç©ºåˆ™ä¿ç•™æ‰€æœ‰æ ‡ç­¾ï¼‰
    """
    # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(csv_file):
        print(f"âš ï¸  è­¦å‘Šï¼šæ ‡ç­¾CSVæ–‡ä»¶ä¸å­˜åœ¨ â†’ {csv_file}")
        print("ğŸ“Œ å°†ä¿ç•™æ‰€æœ‰æ ‡ç­¾ï¼ˆç­‰ä»·äºTARGET_CLASSES = []ï¼‰")
        return []

    try:
        # 2. è¯»å–CSVï¼Œä»…åŠ è½½æŒ‡å®šåˆ—
        df = pd.read_csv(csv_file, encoding="utf-8-sig", usecols=[target_column])

        # 3. æ•°æ®æ¸…æ´—ï¼šè¿‡æ»¤ç©ºå€¼ã€å»é‡ã€å»é™¤ç©ºå­—ç¬¦ä¸²
        target_classes = (
            df[target_column]
            .dropna()  # è¿‡æ»¤NaNå€¼
            .unique()  # å»é‡
            .tolist()  # è½¬ä¸ºåˆ—è¡¨
        )
        # è¿›ä¸€æ­¥è¿‡æ»¤ç©ºå­—ç¬¦ä¸²å’Œçº¯ç©ºæ ¼æ ‡ç­¾
        target_classes = [
            str(cls).strip() for cls in target_classes
            if isinstance(cls, (str, int, float)) and str(cls).strip() != ""
        ]

        # 4. è¾“å‡ºç»“æœæ—¥å¿—
        if len(target_classes) > 0:
            print(f"âœ… æˆåŠŸä»CSVè¯»å–æ ‡ç­¾ï¼š")
            print(f"   - æ–‡ä»¶è·¯å¾„ï¼š{csv_file}")
            print(f"   - è¯»å–åˆ—åï¼š{target_column}")
            print(f"   - æœ‰æ•ˆæ ‡ç­¾æ•°ï¼š{len(target_classes)}")
            print(f"   - æ ‡ç­¾åˆ—è¡¨ï¼š{target_classes}")
        else:
            print(f"âš ï¸  è­¦å‘Šï¼šCSVæ–‡ä»¶ {csv_file} çš„ {target_column} åˆ—æ— æœ‰æ•ˆæ ‡ç­¾")
            print("ğŸ“Œ å°†ä¿ç•™æ‰€æœ‰æ ‡ç­¾ï¼ˆç­‰ä»·äºTARGET_CLASSES = []ï¼‰")

        return target_classes

    except KeyError:
        print(f"âŒ é”™è¯¯ï¼šCSVæ–‡ä»¶ {csv_file} ä¸­æœªæ‰¾åˆ°åˆ—å â†’ {target_column}")
        print("ğŸ“Œ å°†ä¿ç•™æ‰€æœ‰æ ‡ç­¾ï¼ˆç­‰ä»·äºTARGET_CLASSES = []ï¼‰")
        return []
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šè¯»å–æ ‡ç­¾CSVå¤±è´¥ â†’ {str(e)}")
        print("ğŸ“Œ å°†ä¿ç•™æ‰€æœ‰æ ‡ç­¾ï¼ˆç­‰ä»·äºTARGET_CLASSES = []ï¼‰")
        return []


# ---------------------- æ­¥éª¤1ï¼šä¸‹è½½æ ‡æ³¨æ•°æ®é›†ï¼ˆä»CSVæå–å¹¶ä¿å­˜åŸå§‹æ•°æ®ï¼‰ ----------------------
def download_annotation_dataset(
        input_csv_path: str,
        raw_data_root: str = "raw_dataset",
        class_mapping: Optional[Dict[str, int]] = None
) -> Tuple[Dict[str, int], str]:
    """
    æ­¥éª¤1ï¼šä»CSVæ–‡ä»¶ä¸‹è½½å›¾ç‰‡å’Œç”ŸæˆåŸå§‹æ ‡æ³¨ï¼Œä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹
    æ ¸å¿ƒé€»è¾‘ï¼šå›¾ç‰‡å­˜åœ¨åˆ™è·³è¿‡ä¸‹è½½ï¼Œä¸å­˜åœ¨æ‰ä¸‹è½½
    :param input_csv_path: è¾“å…¥CSVè·¯å¾„ï¼ˆåŒ…å«sourceå›¾ç‰‡URLã€æ ‡æ³¨å­—æ®µç­‰ï¼‰
    :param raw_data_root: åŸå§‹æ•°æ®ä¿å­˜æ ¹ç›®å½•ï¼ˆé»˜è®¤raw_datasetï¼‰
    :param class_mapping: åˆå§‹ç±»åˆ«æ˜ å°„ï¼ˆå¯é€‰ï¼Œå¦‚{"ç±»åˆ«1":0, "ç±»åˆ«2":1}ï¼‰
    :return: (æœ€ç»ˆç±»åˆ«æ˜ å°„, åŸå§‹æ•°æ®æ ¹ç›®å½•è·¯å¾„)
    """
    # åˆå§‹åŒ–ç±»åˆ«æ˜ å°„
    if class_mapping is None:
        class_mapping = {}
        class_id = 0
    else:
        class_id = max(class_mapping.values(), default=-1) + 1

    # åˆ›å»ºåŸå§‹æ•°æ®æ–‡ä»¶å¤¹ï¼ˆå›¾ç‰‡+æ ‡æ³¨ï¼‰
    raw_img_dir = os.path.join(raw_data_root, "images")
    raw_label_dir = os.path.join(raw_data_root, "labels")
    for dir_path in [raw_img_dir, raw_label_dir]:
        Path(dir_path).mkdir(parents=True, exist_ok=True)

    # å›¾ç‰‡ä¸‹è½½å‡½æ•°ï¼ˆå­˜åœ¨åˆ™è·³è¿‡ï¼‰
    def download_image(url: str, save_path: str) -> bool:
        if os.path.exists(save_path):
            return True
        try:
            response = requests.get(url, stream=True, timeout=15)
            response.raise_for_status()
            with open(save_path, "wb") as f:
                f.write(response.content)
            return True
        except Exception as e:
            print(f"\nâŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥ {url}ï¼š{e}")
            return False

    # æ ‡æ³¨è½¬æ¢å‡½æ•°ï¼ˆç”ŸæˆåŸå§‹æ ‡æ³¨ï¼Œä¿ç•™æ‰€æœ‰ç±»åˆ«ï¼‰
    def json_to_yolo_annotation(
            json_str: str,
            img_width: Optional[float],
            img_height: Optional[float]
    ) -> str:
        nonlocal class_mapping, class_id
        yolo_lines = []
        try:
            if pd.isna(json_str) or not isinstance(json_str, str):
                return ""
            data = json.loads(json_str)
            objects = data.get("objects", [])
            img_width = img_width or data.get("width", 1)
            img_height = img_height or data.get("height", 1)

            for obj in objects:
                if not isinstance(obj, dict):
                    continue
                # å¤„ç†ç±»åˆ«IDï¼ˆä¿ç•™æ‰€æœ‰åŸå§‹ç±»åˆ«ï¼‰
                obj_name = obj.get("name", "unknown")
                if obj_name not in class_mapping:
                    class_mapping[obj_name] = class_id
                    class_id += 1
                cid = class_mapping[obj_name]

                # å¤„ç†åæ ‡ï¼ˆYOLOæ ¼å¼ï¼šå½’ä¸€åŒ–ä¸­å¿ƒåæ ‡+å®½é«˜ï¼‰
                ptlist = obj.get("polygon", {}).get("ptList", [])
                if len(ptlist) != 2:
                    continue
                p1, p2 = ptlist
                x1, y1 = min(p1["x"], p2["x"]), min(p1["y"], p2["y"])
                x2, y2 = max(p1["x"], p2["x"]), max(p1["y"], p2["y"])

                x_center = (x1 + x2) / 2 / img_width
                y_center = (y1 + y2) / 2 / img_height
                width = (x2 - x1) / img_width
                height = (y2 - y1) / img_height

                # é™åˆ¶èŒƒå›´åœ¨0~1ä¹‹é—´
                x_center = max(0.001, min(0.999, x_center))
                y_center = max(0.001, min(0.999, y_center))
                width = max(0.001, min(0.999, width))
                height = max(0.001, min(0.999, height))

                yolo_lines.append(f"{cid} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}")
        except json.JSONDecodeError:
            print(f"\nâŒ JSONè§£æå¤±è´¥ï¼ˆå‰50å­—ç¬¦ï¼‰ï¼š{str(json_str)[:50]}...")
        except Exception as e:
            print(f"\nâŒ æ ‡æ³¨è½¬æ¢å¤±è´¥ï¼š{e}")
        return "\n".join(yolo_lines)

    # è¯»å–CSVå¹¶å¤„ç†
    try:
        df = pd.read_csv(input_csv_path, encoding="utf-8-sig")
        print(f"æ­¥éª¤1ï¼šæˆåŠŸè¯»å–CSVï¼Œå…± {len(df)} è¡Œæ•°æ®")
    except Exception as e:
        raise Exception(f"æ­¥éª¤1ï¼šCSVè¯»å–å¤±è´¥ï¼š{e}") from e

    # æ‰¹é‡ä¸‹è½½å›¾ç‰‡å’Œç”Ÿæˆæ ‡æ³¨
    success_count = 0
    fail_count = 0
    skip_count = 0
    for idx, row in tqdm(enumerate(df.iterrows()), total=len(df), desc="æ­¥éª¤1ï¼šä¸‹è½½å¹¶ç”ŸæˆåŸå§‹æ•°æ®", unit="æ¡"):
        _, row = row
        img_url = row["source"]
        json_anno = row["æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®"]
        img_width = row.get("width")
        img_height = row.get("height")

        # ç”Ÿæˆæ–‡ä»¶åï¼ˆå¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
        img_basename = os.path.splitext(os.path.basename(img_url))[0]
        img_basename = img_basename.replace("?", "").replace("&", "_").replace("/", "_")
        img_filename = f"{img_basename}.jpg"
        label_filename = f"{img_basename}.txt"

        # ä¸‹è½½å›¾ç‰‡ï¼ˆè‡ªåŠ¨åˆ¤æ–­æ˜¯å¦å­˜åœ¨ï¼‰
        img_path = os.path.join(raw_img_dir, img_filename)
        download_result = download_image(img_url, img_path)
        if not download_result:
            fail_count += 1
            continue
        if os.path.exists(img_path):
            skip_count += 1

        # ç”ŸæˆåŸå§‹æ ‡æ³¨
        yolo_anno = json_to_yolo_annotation(json_anno, img_width, img_height)
        label_path = os.path.join(raw_label_dir, label_filename)
        with open(label_path, "w", encoding="utf-8") as f:
            f.write(yolo_anno)

        success_count += 1

    # è¾“å‡ºæ­¥éª¤1ç»“æœ
    print(f"\næ­¥éª¤1ï¼šåŸå§‹æ•°æ®ç”Ÿæˆå®Œæˆï¼")
    print(f" - æ€»å¤„ç†æ•°æ®ï¼š{len(df)} æ¡")
    print(f" - æˆåŠŸï¼ˆä¸‹è½½/è·³è¿‡+æ ‡æ³¨ï¼‰ï¼š{success_count} æ¡")
    print(f" - ä¸‹è½½å¤±è´¥ï¼š{fail_count} æ¡")
    print(f" - è·³è¿‡å·²å­˜åœ¨å›¾ç‰‡ï¼š{skip_count - (success_count - fail_count)} æ¡")
    print(f" - åŸå§‹å›¾ç‰‡ç›®å½•ï¼š{raw_img_dir}")
    print(f" - åŸå§‹æ ‡æ³¨ç›®å½•ï¼š{raw_label_dir}")
    print(f" - è¯†åˆ«åˆ°çš„æ‰€æœ‰ç±»åˆ«ï¼š{sorted(class_mapping.items(), key=lambda x: x[1])}")

    return class_mapping, raw_data_root


# ---------------------- æ­¥éª¤2ï¼šç­›é€‰æ•°æ®é›†ï¼ˆä¿ç•™æŒ‡å®šæ ‡ç­¾ï¼Œåˆ é™¤å…¶ä»–æ ‡æ³¨ï¼‰ ----------------------
def filter_dataset(
        raw_data_root: str,
        filtered_data_root: str = "filtered_dataset",
        target_classes: List[str] = None,
        class_mapping: Dict[str, int] = None
) -> str:
    """
    æ­¥éª¤2ï¼šç­›é€‰ç¬¦åˆè¦æ±‚çš„æ•°æ®é›†ï¼ˆä¿ç•™æŒ‡å®šæ ‡ç­¾ï¼Œåˆ é™¤å…¶ä»–æ ‡æ³¨ä¿¡æ¯ï¼‰
    æ ¸å¿ƒä¿®æ”¹1ï¼šåªä¿ç•™ç›®æ ‡æ ‡ç­¾ä¸­åœ¨æ•°æ®é›†ä¸­å®é™…å­˜åœ¨çš„æ ‡ç­¾ï¼Œè¿‡æ»¤æ— æ•ˆæ ‡ç­¾
    æ ¸å¿ƒä¿®æ”¹2ï¼šæ¯æ¬¡ç­›é€‰å‰æ¸…ç©ºfiltered_datasetç›®å½•çš„æ—§æ•°æ®ï¼Œåªä¿ç•™æ–°æ•°æ®
    é€»è¾‘ï¼šå½“target_classesä¸ºç©ºï¼ˆæˆ–None/ç©ºåˆ—è¡¨ï¼‰æ—¶ï¼Œä¿ç•™æ‰€æœ‰æ ‡ç­¾ï¼ˆä¸ç­›é€‰ï¼‰
    :param raw_data_root: æ­¥éª¤1ç”Ÿæˆçš„åŸå§‹æ•°æ®æ ¹ç›®å½•
    :param filtered_data_root: ç­›é€‰åæ•°æ®ä¿å­˜ç›®å½•ï¼ˆé»˜è®¤filtered_datasetï¼‰
    :param target_classes: éœ€ä¿ç•™çš„ç›®æ ‡æ ‡ç­¾åˆ—è¡¨ï¼ˆä¸ºç©ºåˆ™ä¿ç•™æ‰€æœ‰æ ‡ç­¾ï¼‰
    :param class_mapping: æ­¥éª¤1ç”Ÿæˆçš„ç±»åˆ«æ˜ å°„ï¼ˆå¿…é¡»ä¼ å…¥ï¼‰
    :return: ç­›é€‰åæ•°æ®æ ¹ç›®å½•è·¯å¾„
    """
    # æ ¡éªŒclass_mapping
    if class_mapping is None or len(class_mapping) == 0:
        raise ValueError("æ­¥éª¤2ï¼šclass_mappingä¸èƒ½ä¸ºç©ºï¼Œè¯·ä¼ å…¥æ­¥éª¤1ç”Ÿæˆçš„ç±»åˆ«æ˜ å°„")

    # ---------------------- æ–°å¢ï¼šæ¸…ç©ºç­›é€‰ç›®å½•çš„æ—§æ•°æ® ----------------------
    filtered_img_dir = os.path.join(filtered_data_root, "images")
    filtered_label_dir = os.path.join(filtered_data_root, "labels")
    # è‹¥ç›®å½•å·²å­˜åœ¨ï¼Œåˆ é™¤æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬å­æ–‡ä»¶å’Œå­ç›®å½•ï¼‰
    if os.path.exists(filtered_data_root):
        print(f"ğŸ—‘ï¸  æ¸…ç©ºæ—§ç­›é€‰æ•°æ®ï¼š{filtered_data_root}")
        # åˆ é™¤æ•´ä¸ªç›®å½•ï¼ˆåŒ…æ‹¬å†…å®¹ï¼‰ï¼Œç„¶åé‡æ–°åˆ›å»ºç©ºç›®å½•
        shutil.rmtree(filtered_data_root)
    # é‡æ–°åˆ›å»ºç©ºçš„ç­›é€‰ç›®å½•
    for dir_path in [filtered_img_dir, filtered_label_dir]:
        Path(dir_path).mkdir(parents=True, exist_ok=True)
    # --------------------------------------------------------------------------

    # å¤„ç†target_classesï¼šåªä¿ç•™æ•°æ®é›†ä¸­å­˜åœ¨çš„æ ‡ç­¾
    dataset_existing_classes = set(class_mapping.keys())  # æ•°æ®é›†ä¸­å®é™…å­˜åœ¨çš„æ‰€æœ‰æ ‡ç­¾
    if target_classes is None or len(target_classes) == 0:
        print("æ­¥éª¤2ï¼štarget_classesä¸ºç©ºï¼Œå°†ä¿ç•™æ‰€æœ‰æ ‡ç­¾ï¼Œä¸è¿›è¡Œç­›é€‰")
        target_class_ids = list(class_mapping.values())
        target_classes = list(class_mapping.keys())
    else:
        # æ ¸å¿ƒä¿®æ”¹ï¼šç­›é€‰å‡º"ç›®æ ‡æ ‡ç­¾"å’Œ"æ•°æ®é›†å­˜åœ¨æ ‡ç­¾"çš„äº¤é›†
        target_classes = [cls for cls in target_classes if cls in dataset_existing_classes]
        if len(target_classes) > 0:
            print(f"âœ… ç­›é€‰å‡ºæ•°æ®é›†ä¸­å®é™…å­˜åœ¨çš„ç›®æ ‡æ ‡ç­¾ï¼š{target_classes}")
            target_class_ids = [class_mapping[cls] for cls in target_classes]
        else:
            print("âš ï¸  è­¦å‘Šï¼šæ‰€æœ‰ç›®æ ‡æ ‡ç­¾åœ¨æ•°æ®é›†ä¸­å‡ä¸å­˜åœ¨ï¼Œå°†ä¿ç•™æ‰€æœ‰æ ‡ç­¾")
            target_class_ids = list(class_mapping.values())
            target_classes = list(class_mapping.keys())

    # è·å–åŸå§‹æ•°æ®çš„æ‰€æœ‰å›¾ç‰‡å’Œæ ‡æ³¨æ–‡ä»¶
    raw_img_dir = os.path.join(raw_data_root, "images")
    raw_label_dir = os.path.join(raw_data_root, "labels")
    if not os.path.exists(raw_img_dir) or not os.path.exists(raw_label_dir):
        raise FileNotFoundError(f"æ­¥éª¤2ï¼šåŸå§‹æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼š{raw_img_dir} æˆ– {raw_label_dir}")

    # ç­›é€‰é€»è¾‘ï¼šä¿ç•™ç›®æ ‡æ ‡ç­¾ï¼ˆæˆ–æ‰€æœ‰æ ‡ç­¾ï¼‰
    success_count = 0
    no_target_count = 0
    label_files = list(Path(raw_label_dir).glob("*.txt"))

    for label_file in tqdm(label_files, desc="æ­¥éª¤2ï¼šç­›é€‰æ•°æ®é›†", unit="ä¸ª"):
        label_basename = label_file.stem
        img_filename = f"{label_basename}.jpg"
        raw_img_path = os.path.join(raw_img_dir, img_filename)
        filtered_img_path = os.path.join(filtered_img_dir, img_filename)
        filtered_label_path = os.path.join(filtered_label_dir, label_file.name)

        # è·³è¿‡ä¸å­˜åœ¨çš„å›¾ç‰‡
        if not os.path.exists(raw_img_path):
            print(f"\næ­¥éª¤2ï¼šå›¾ç‰‡ä¸å­˜åœ¨ï¼Œè·³è¿‡ï¼š{raw_img_path}")
            continue

        # è¯»å–åŸå§‹æ ‡æ³¨ï¼Œç­›é€‰ç›®æ ‡æ ‡ç­¾
        with open(label_file, "r", encoding="utf-8") as f:
            lines = f.readlines()
        filtered_lines = []
        for line in lines:
            line = line.strip()
            if not line:
                continue
            parts = line.split()
            if len(parts) != 5:
                continue
            cid = int(parts[0])
            if cid in target_class_ids:
                filtered_lines.append(line)

        # ä¿ç•™æœ‰æœ‰æ•ˆæ ‡æ³¨çš„æ–‡ä»¶
        if len(filtered_lines) > 0:
            shutil.copy2(raw_img_path, filtered_img_path)
            with open(filtered_label_path, "w", encoding="utf-8") as f:
                f.write("\n".join(filtered_lines))
            success_count += 1
        else:
            no_target_count += 1

    # è¾“å‡ºæ­¥éª¤2ç»“æœ
    print(f"\næ­¥éª¤2ï¼šæ•°æ®é›†ç­›é€‰å®Œæˆï¼")
    print(f" - ä¿ç•™çš„æ ‡ç­¾ï¼š{target_classes}ï¼ˆå¯¹åº”IDï¼š{target_class_ids}ï¼‰")
    print(f" - ä¿ç•™æœ‰æ•ˆæ•°æ®ï¼š{success_count} æ¡")
    print(f" - æ— æœ‰æ•ˆæ ‡æ³¨è¢«è¿‡æ»¤ï¼š{no_target_count} æ¡")
    print(f" - ç­›é€‰åå›¾ç‰‡ç›®å½•ï¼š{filtered_img_dir}")
    print(f" - ç­›é€‰åæ ‡æ³¨ç›®å½•ï¼š{filtered_label_dir}")

    return filtered_data_root


# ---------------------- æ­¥éª¤3ï¼šåˆ’åˆ†æ•°æ®é›†ï¼ˆç”ŸæˆYOLOv11è®­ç»ƒæ ¼å¼ï¼‰ ----------------------
def split_yolov11_dataset(
        filtered_data_root: str,
        yolo_root: str = "yolo_dataset",
        train_split: float = 0.7,
        val_split: float = 0.2,
        test_split: float = 0.1,
        target_classes: List[str] = None,
        class_mapping: Dict[str, int] = None
) -> str:
    """
    æ­¥éª¤3ï¼šåˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†ï¼Œç”ŸæˆYOLOv11æ‰€éœ€çš„æ•°æ®é›†ç»“æ„å’Œé…ç½®æ–‡ä»¶
    æ ¸å¿ƒä¿®æ”¹ï¼šæ¯æ¬¡åˆ’åˆ†å‰æ¸…ç©ºyolo_datasetç›®å½•çš„æ—§æ•°æ®ï¼Œåªä¿ç•™æ–°æ•°æ®
    :param filtered_data_root: æ­¥éª¤2ç”Ÿæˆçš„ç­›é€‰åæ•°æ®æ ¹ç›®å½•
    :param yolo_root: YOLOv11æ•°æ®é›†æ ¹ç›®å½•ï¼ˆé»˜è®¤yolo_datasetï¼‰
    :param train_split: è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.7ï¼‰
    :param val_split: éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.2ï¼‰
    :param test_split: æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.1ï¼‰
    :param target_classes: ä¿ç•™çš„ç›®æ ‡æ ‡ç­¾åˆ—è¡¨ï¼ˆä¸ºç©ºåˆ™ä½¿ç”¨æ‰€æœ‰ç±»åˆ«ï¼‰
    :param class_mapping: æ­¥éª¤1ç”Ÿæˆçš„ç±»åˆ«æ˜ å°„
    :return: YOLOv11æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
    """
    # éªŒè¯æ¯”ä¾‹ä¹‹å’Œä¸º1
    if not abs(train_split + val_split + test_split - 1.0) < 1e-6:
        raise ValueError(f"æ­¥éª¤3ï¼šæ•°æ®é›†æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1ï¼Œå½“å‰ï¼š{train_split}+{val_split}+{test_split}")
    if class_mapping is None or len(class_mapping) == 0:
        raise ValueError("æ­¥éª¤3ï¼šclass_mappingä¸èƒ½ä¸ºç©ºï¼Œè¯·ä¼ å…¥æ­¥éª¤1ç”Ÿæˆçš„ç±»åˆ«æ˜ å°„")

    # ---------------------- æ–°å¢ï¼šæ¸…ç©ºYOLOæ•°æ®é›†ç›®å½•çš„æ—§æ•°æ® ----------------------
    if os.path.exists(yolo_root):
        print(f"ğŸ—‘ï¸  æ¸…ç©ºæ—§YOLOæ•°æ®é›†ï¼š{yolo_root}")
        shutil.rmtree(yolo_root)
    # --------------------------------------------------------------------------

    # å¤„ç†target_classesï¼šåªä¿ç•™æ•°æ®é›†ä¸­å­˜åœ¨çš„æ ‡ç­¾ï¼ˆä¸æ­¥éª¤2ä¿æŒä¸€è‡´ï¼‰
    dataset_existing_classes = set(class_mapping.keys())
    if target_classes is None or len(target_classes) == 0:
        target_classes = list(class_mapping.keys())
        print(f"æ­¥éª¤3ï¼štarget_classesä¸ºç©ºï¼Œä½¿ç”¨æ‰€æœ‰ç±»åˆ«ï¼š{target_classes}")
    else:
        target_classes = [cls for cls in target_classes if cls in dataset_existing_classes]
        if len(target_classes) == 0:
            target_classes = list(class_mapping.keys())
            print(f"æ­¥éª¤3ï¼šæ‰€æœ‰ç›®æ ‡æ ‡ç­¾åœ¨æ•°æ®é›†ä¸­å‡ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ‰€æœ‰ç±»åˆ«ï¼š{target_classes}")
        else:
            print(f"æ­¥éª¤3ï¼šä½¿ç”¨æ•°æ®é›†ä¸­å­˜åœ¨çš„ç›®æ ‡æ ‡ç­¾ï¼š{target_classes}")

    # æå–ç­›é€‰åçš„å›¾ç‰‡å’Œæ ‡æ³¨æ–‡ä»¶
    filtered_img_dir = os.path.join(filtered_data_root, "images")
    filtered_label_dir = os.path.join(filtered_data_root, "labels")
    if not os.path.exists(filtered_img_dir) or not os.path.exists(filtered_label_dir):
        raise FileNotFoundError(f"æ­¥éª¤3ï¼šç­›é€‰åæ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼š{filtered_img_dir} æˆ– {filtered_label_dir}")

    # è·å–æ‰€æœ‰æœ‰æ•ˆæ•°æ®ï¼ˆå›¾ç‰‡+æ ‡æ³¨æˆå¯¹å­˜åœ¨ï¼‰
    data_files = []
    img_files = list(Path(filtered_img_dir).glob("*.jpg"))
    for img_file in img_files:
        img_basename = img_file.stem
        label_file = Path(filtered_label_dir) / f"{img_basename}.txt"
        if label_file.exists():
            data_files.append((str(img_file), str(label_file)))

    if len(data_files) == 0:
        raise ValueError("æ­¥éª¤3ï¼šç­›é€‰åçš„æ•°æ®ä¸ºç©ºï¼Œæ— æ³•åˆ’åˆ†æ•°æ®é›†")

    print(f"æ­¥éª¤3ï¼šå…±è·å– {len(data_files)} æ¡æœ‰æ•ˆæ•°æ®ï¼Œå¼€å§‹åˆ’åˆ†...")

    # åˆ›å»ºYOLOv11æ•°æ®é›†æ–‡ä»¶å¤¹ç»“æ„
    train_img_dir = os.path.join(yolo_root, "train", "images")
    train_label_dir = os.path.join(yolo_root, "train", "labels")
    val_img_dir = os.path.join(yolo_root, "val", "images")
    val_label_dir = os.path.join(yolo_root, "val", "labels")
    test_img_dir = os.path.join(yolo_root, "test", "images")
    test_label_dir = os.path.join(yolo_root, "test", "labels")

    for dir_path in [train_img_dir, train_label_dir, val_img_dir, val_label_dir, test_img_dir, test_label_dir]:
        Path(dir_path).mkdir(parents=True, exist_ok=True)

    # åˆ’åˆ†æ•°æ®é›†
    train_val_files, test_files = train_test_split(data_files, test_size=test_split, random_state=42)
    train_files, val_files = train_test_split(train_val_files, test_size=val_split / (train_split + val_split),
                                              random_state=42)

    # å¤åˆ¶æ–‡ä»¶åˆ°å¯¹åº”ç›®å½•
    def copy_files(files: List[tuple], img_dir: str, label_dir: str, split_name: str):
        for img_path, label_path in tqdm(files, desc=f"æ­¥éª¤3ï¼šå¤åˆ¶{split_name}é›†", unit="æ¡"):
            img_dst = os.path.join(img_dir, os.path.basename(img_path))
            label_dst = os.path.join(label_dir, os.path.basename(label_path))
            shutil.copy2(img_path, img_dst)
            shutil.copy2(label_path, label_dst)

    copy_files(train_files, train_img_dir, train_label_dir, "è®­ç»ƒ")
    copy_files(val_files, val_img_dir, val_label_dir, "éªŒè¯")
    copy_files(test_files, test_img_dir, test_label_dir, "æµ‹è¯•")

    # ç”ŸæˆYOLOv11æ‰€éœ€çš„ç±»åˆ«æ–‡ä»¶å’Œyamlé…ç½®æ–‡ä»¶
    target_class_mapping = {cls: class_mapping[cls] for cls in target_classes}
    sorted_target_classes = sorted(target_class_mapping.items(), key=lambda x: x[1])
    class_file = os.path.join(yolo_root, "yolo_classes.txt")
    with open(class_file, "w", encoding="utf-8") as f:
        for cls_name, cls_id in sorted_target_classes:
            f.write(f"{cls_id} {cls_name}\n")

    # ç”Ÿæˆdataset.yaml
    yaml_data = {
        "path": os.path.abspath(yolo_root),
        "train": "train/images",
        "val": "val/images",
        "test": "test/images",
        "nc": len(target_classes),
        "names": [cls_name for cls_name, _ in sorted_target_classes]
    }
    yaml_path = os.path.join(yolo_root, "dataset.yaml")
    with open(yaml_path, "w", encoding="utf-8") as f:
        yaml.dump(yaml_data, f, sort_keys=False, allow_unicode=True)

    # è¾“å‡ºæ­¥éª¤3ç»“æœ
    print(f"\næ­¥éª¤3ï¼šYOLOv11æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼")
    print(f" - æ•°æ®é›†æ ¹ç›®å½•ï¼š{os.path.abspath(yolo_root)}")
    print(f" - è®­ç»ƒé›†ï¼š{len(train_files)} æ¡ | éªŒè¯é›†ï¼š{len(val_files)} æ¡ | æµ‹è¯•é›†ï¼š{len(test_files)} æ¡")
    print(f" - ç±»åˆ«æ•°ï¼š{len(target_classes)} | ç±»åˆ«æ–‡ä»¶ï¼š{class_file}")
    print(f" - YOLOé…ç½®æ–‡ä»¶ï¼š{yaml_path}ï¼ˆå¯ç›´æ¥ç”¨äºYOLOv11è®­ç»ƒï¼‰")

    return yolo_root


# è¯´æ˜ï¼š
# ä¸Šé¢çš„å‡½æ•°åŸæœ¬åœ¨ process_step.py ä¸­å¸¦æœ‰ CLI ç¤ºä¾‹å…¥å£ã€‚
# åˆå¹¶ä¸ºå•æ–‡ä»¶åï¼ŒStreamlit è¿è¡Œæ—¶ __name__ == "__main__"ï¼Œä¼šè¯¯è§¦å‘è¯¥ç¤ºä¾‹ã€‚
# ä¸ºé¿å…æ— å…³æŠ¥é”™ï¼ˆä¾‹å¦‚ missing matched_data.csvï¼‰ï¼Œå·²ç§»é™¤ç¤ºä¾‹å…¥å£ã€‚


st.set_page_config(page_title="YOLO æ•°æ®å¤„ç†æµæ°´çº¿", layout="wide")


def inject_style():
    st.markdown(
        """
<style>
:root {
  --bg: #f5f7fb;
  --bg-2: #eef2f7;
  --card: #ffffff;
  --text: #0f172a;
  --muted: #64748b;
  --border: #e2e8f0;
  --accent: #2563eb;
  --accent-2: #60a5fa;
  --accent-3: #38bdf8;
  --success: #22c55e;
  --warning: #f59e0b;
  --danger: #ef4444;
}

.stApp {
  background:
    radial-gradient(900px circle at 15% 10%, rgba(96, 165, 250, 0.2), transparent 55%),
    radial-gradient(800px circle at 85% 0%, rgba(56, 189, 248, 0.18), transparent 55%),
    linear-gradient(180deg, #f6f8fc 0%, #eef2f7 45%, #f5f7fb 100%);
  color: var(--text);
  font-family: "Avenir Next", "Source Sans Pro", "Noto Sans", sans-serif;
}

.stApp::before {
  content: "";
  position: fixed;
  inset: 0;
  background-image:
    linear-gradient(rgba(59, 130, 246, 0.08) 1px, transparent 1px),
    linear-gradient(90deg, rgba(59, 130, 246, 0.08) 1px, transparent 1px);
  background-size: 36px 36px;
  pointer-events: none;
  z-index: 0;
}

.hero-title {
  font-size: 2.1rem;
  font-weight: 800;
  letter-spacing: 0.5px;
  background: linear-gradient(90deg, #3b82f6, #60a5fa, #2563eb, #3b82f6);
  background-size: 200% auto;
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  animation: titleGlow 6s linear infinite;
  text-shadow: 0 0 18px rgba(59, 130, 246, 0.25);
}

@keyframes titleGlow {
  0% { background-position: 0% 50%; }
  100% { background-position: 200% 50%; }
}

.glow-frame {
  position: relative;
  border-radius: 16px;
  padding: 2px;
  background: linear-gradient(120deg, rgba(59, 130, 246, 0.18), rgba(37, 99, 235, 0.25), rgba(59, 130, 246, 0.18));
  background-size: 200% 200%;
  animation: borderFlow 8s ease infinite;
}

.glow-frame > .glow-inner {
  border-radius: 14px;
  background: rgba(255, 255, 255, 0.92);
  padding: 14px 16px;
}

@keyframes borderFlow {
  0% { background-position: 0% 50%; }
  50% { background-position: 100% 50%; }
  100% { background-position: 0% 50%; }
}

.busy-indicator {
  display: inline-flex;
  align-items: center;
  gap: 8px;
  font-size: 0.8rem;
  color: #3b82f6;
}

.busy-dots span {
  display: inline-block;
  width: 6px;
  height: 6px;
  margin-left: 3px;
  border-radius: 50%;
  background: #3b82f6;
  animation: pulse 1.2s infinite ease-in-out;
}

.busy-dots span:nth-child(2) { animation-delay: 0.2s; }
.busy-dots span:nth-child(3) { animation-delay: 0.4s; }

@keyframes pulse {
  0%, 80%, 100% { transform: scale(0.6); opacity: 0.4; }
  40% { transform: scale(1); opacity: 1; }
}

section[data-testid="stSidebar"] {
  background: linear-gradient(180deg, #ffffff 0%, #f1f5f9 100%);
  border-right: 1px solid var(--border);
}

.sidebar-title {
  font-size: 1.05rem;
  font-weight: 700;
  color: var(--text);
  margin-bottom: 0.5rem;
}

.panel {
  background: linear-gradient(135deg, rgba(96, 165, 250, 0.12), #ffffff);
  border: 1px solid rgba(37, 99, 235, 0.15);
  border-radius: 16px;
  padding: 16px 18px;
  box-shadow: 0 12px 24px rgba(15, 23, 42, 0.08);
  backdrop-filter: blur(6px);
}

.kpi {
  font-size: 0.85rem;
  color: var(--muted);
}

.chip {
  display: inline-flex;
  align-items: center;
  gap: 6px;
  padding: 4px 10px;
  border-radius: 999px;
  font-size: 0.75rem;
  font-weight: 600;
  border: 1px solid transparent;
}

.chip-wait {
  background: #f1f5f9;
  color: #64748b;
  border-color: #e2e8f0;
}

.chip-done {
  background: #dcfce7;
  color: #166534;
  border-color: #86efac;
}

.chip-skip {
  background: #fef3c7;
  color: #92400e;
  border-color: #fde68a;
}

.file-card {
  background: #f8fafc;
  border: 1px solid rgba(37, 99, 235, 0.15);
  border-radius: 12px;
  padding: 10px 12px;
  min-height: 70px;
  box-shadow: inset 0 0 0 1px rgba(37, 99, 235, 0.06);
}

.file-name {
  font-weight: 600;
  font-size: 0.85rem;
  color: var(--text);
  word-break: break-all;
}

.file-meta {
  font-size: 0.75rem;
  color: var(--muted);
}

button[kind="primary"] {
  background: linear-gradient(135deg, #2563eb 0%, #60a5fa 100%);
  border: none;
  box-shadow: 0 8px 18px rgba(37, 99, 235, 0.25);
}

div[data-baseweb="input"] > div,
div[data-baseweb="input"] input,
div[data-baseweb="textarea"] textarea,
div[data-baseweb="select"] > div {
  background: #ffffff !important;
  color: #0f172a !important;
  border-color: rgba(37, 99, 235, 0.2) !important;
}

div[data-baseweb="input"] input::placeholder,
div[data-baseweb="textarea"] textarea::placeholder {
  color: rgba(100, 116, 139, 0.8) !important;
}

div[data-testid="stDataFrame"] {
  border: 1px solid rgba(37, 99, 235, 0.18);
  border-radius: 12px;
  overflow: hidden;
}

div[data-testid="stExpander"] {
  border-radius: 14px;
  border: 1px solid rgba(37, 99, 235, 0.12);
  background: #ffffff;
  box-shadow: 0 10px 20px rgba(15, 23, 42, 0.08);
  backdrop-filter: blur(6px);
}

div[data-testid="stExpander"] > details > summary {
  padding: 0.4rem 1rem;
  font-weight: 600;
  color: var(--text);
}

.stepper {
  display: flex;
  align-items: center;
  gap: 8px;
  flex-wrap: wrap;
}

.step {
  display: flex;
  align-items: center;
  gap: 8px;
}

.step-circle {
  width: 28px;
  height: 28px;
  border-radius: 50%;
  display: inline-flex;
  align-items: center;
  justify-content: center;
  font-size: 0.75rem;
  font-weight: 700;
  border: 1px solid rgba(37, 99, 235, 0.2);
  background: #ffffff;
  color: #64748b;
}

.step-circle.done {
  background: rgba(34, 197, 94, 0.2);
  color: #86efac;
  border-color: rgba(34, 197, 94, 0.5);
}

.step-circle.active {
  background: #dbeafe;
  color: #1d4ed8;
  border-color: #93c5fd;
}

.step-circle.locked {
  background: #f1f5f9;
  color: #94a3b8;
}

.step-circle.skipped {
  background: #fef3c7;
  color: #92400e;
  border-color: #fde68a;
}

.step-label {
  font-size: 0.8rem;
  color: var(--text);
  font-weight: 600;
}

.step-line {
  flex: 1;
  height: 2px;
  min-width: 24px;
  background: #e2e8f0;
}

.step-line.line-done {
  background: linear-gradient(90deg, #22c55e, #16a34a);
}

.step-line.line-skip {
  background: repeating-linear-gradient(90deg, #f59e0b 0 6px, #fde68a 6px 12px);
}

.step-line.line-lock {
  background: #e2e8f0;
}

.stat-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(160px, 1fr));
  gap: 12px;
  margin-top: 8px;
}

.stat-card {
  background: #ffffff;
  border: 1px solid rgba(37, 99, 235, 0.12);
  border-radius: 12px;
  padding: 12px;
  box-shadow: inset 0 0 0 1px rgba(37, 99, 235, 0.05);
}

.stat-label {
  font-size: 0.75rem;
  color: var(--muted);
}

.stat-value {
  font-size: 1.1rem;
  font-weight: 700;
  margin-top: 4px;
  color: var(--text);
}

.stat-hint {
  font-size: 0.7rem;
  color: var(--muted);
  margin-top: 2px;
}

.dependency-card {
  background: #ffffff;
  border: 1px solid rgba(37, 99, 235, 0.12);
  border-radius: 14px;
  padding: 12px 16px;
  box-shadow: 0 10px 20px rgba(15, 23, 42, 0.08);
  backdrop-filter: blur(6px);
}

.file-manager {
  border: 1px solid rgba(37, 99, 235, 0.15);
  border-radius: 14px;
  padding: 12px;
  background: linear-gradient(180deg, #ffffff 0%, #f8fafc 100%);
  box-shadow: inset 0 0 0 1px rgba(37, 99, 235, 0.05);
}

.fm-node {
  display: flex;
  align-items: center;
  gap: 8px;
  padding: 6px 8px;
  border-radius: 8px;
}

.fm-selected {
  background: rgba(37, 99, 235, 0.12);
  border: 1px solid rgba(37, 99, 235, 0.3);
}

.fm-node:hover {
  background: rgba(59, 130, 246, 0.08);
}

.fm-icon {
  width: 18px;
  height: 14px;
  display: inline-flex;
  align-items: center;
  justify-content: center;
}

.fm-icon svg {
  width: 18px;
  height: 14px;
  fill: currentColor;
  color: #2563eb;
}

.fm-depth-0 .fm-icon svg { color: #0284c7; }
.fm-depth-1 .fm-icon svg { color: #2563eb; }
.fm-depth-2 .fm-icon svg { color: #7c3aed; }
.fm-depth-3 .fm-icon svg { color: #059669; }
.fm-depth-4 .fm-icon svg { color: #f59e0b; }

.fm-name {
  font-size: 0.82rem;
  color: #0f172a;
  font-weight: 600;
}

.fm-path {
  font-size: 0.75rem;
  color: #64748b;
}

.drop-zone {
  border: 1px dashed rgba(37, 99, 235, 0.4);
  border-radius: 12px;
  padding: 10px;
  text-align: center;
  color: #2563eb;
  background: rgba(59, 130, 246, 0.05);
  font-size: 0.8rem;
}

hr {
  border: none;
  border-top: 1px solid rgba(37, 99, 235, 0.12);
}
</style>
        """,
        unsafe_allow_html=True,
    )


def parse_kv_lines(text: str):
    options = {}
    errors = []
    if not text:
        return options, errors
    for raw_line in text.splitlines():
        line = raw_line.strip()
        if not line or line.startswith("#"):
            continue
        if "=" not in line:
            errors.append(f"æ— æ³•è§£æï¼š{raw_line}")
            continue
        key, raw_value = line.split("=", 1)
        key = key.strip()
        raw_value = raw_value.strip()
        if not key:
            errors.append(f"å‚æ•°åä¸ºç©ºï¼š{raw_line}")
            continue
        value = raw_value
        try:
            value = json.loads(raw_value)
        except Exception:
            lowered = raw_value.lower()
            if lowered in {"true", "false"}:
                value = lowered == "true"
            elif lowered in {"none", "null"}:
                value = None
            else:
                try:
                    if "." in raw_value:
                        value = float(raw_value)
                    else:
                        value = int(raw_value)
                except Exception:
                    value = raw_value
        options[key] = value
    return options, errors


def trigger_rerun():
    if hasattr(st, "rerun"):
        st.rerun()
    else:
        st.experimental_rerun()


def clear_output_root(output_root: Path, keep_inputs: bool = True, keep_files=None):
    keep = set(keep_files or [])
    if keep_inputs:
        keep.add("input_csvs")
    for item in output_root.iterdir() if output_root.exists() else []:
        if item.name in keep:
            continue
        try:
            if item.is_dir():
                shutil.rmtree(item)
            else:
                item.unlink()
        except Exception:
            pass


def show_confirm_dialog(state_key: str, title: str, body: str, on_confirm):
    def _handle_confirm():
        on_confirm()
        st.session_state[state_key] = False
        trigger_rerun()

    def _handle_cancel():
        st.session_state[state_key] = False
        trigger_rerun()

    if hasattr(st, "dialog"):
        @st.dialog(title)
        def _dialog():
            st.write(body)
            col1, col2 = st.columns(2)
            if col1.button("ç¡®è®¤åˆ é™¤", key=f"{state_key}_confirm", width='stretch'):
                _handle_confirm()
            if col2.button("å–æ¶ˆ", key=f"{state_key}_cancel", width='stretch'):
                _handle_cancel()
        _dialog()
    else:
        st.warning(body)
        col1, col2 = st.columns(2)
        if col1.button("ç¡®è®¤åˆ é™¤", key=f"{state_key}_confirm", width='stretch'):
            _handle_confirm()
        if col2.button("å–æ¶ˆ", key=f"{state_key}_cancel", width='stretch'):
            _handle_cancel()


def build_train_template_payload():
    saved_data_yaml = st.session_state.get("train_dataset_manual") or st.session_state.get("train_dataset_choice") or ""
    return {
        "dataset_root": st.session_state.get("train_dataset_root"),
        "data_yaml": saved_data_yaml,
        "model_path": st.session_state.get("train_model_path"),
        "project": st.session_state.get("train_project_input"),
        "name": st.session_state.get("train_name_input"),
        "exist_ok": st.session_state.get("train_exist_ok"),
        "epochs": st.session_state.get("train_epochs"),
        "imgsz": st.session_state.get("train_imgsz"),
        "batch": st.session_state.get("train_batch"),
        "workers": st.session_state.get("train_workers"),
        "device": st.session_state.get("train_device"),
        "amp": st.session_state.get("train_amp"),
        "cache": st.session_state.get("train_cache"),
        "resume": st.session_state.get("train_resume"),
        "optimizer": st.session_state.get("train_optimizer"),
        "seed": st.session_state.get("train_seed"),
        "patience": st.session_state.get("train_patience"),
        "cos_lr": st.session_state.get("train_cos_lr"),
        "close_mosaic": st.session_state.get("train_close_mosaic"),
        "save_period": st.session_state.get("train_save_period"),
        "advanced_text": st.session_state.get("train_advanced"),
        "cuda_visible_devices": st.session_state.get("train_cuda_visible"),
        "scan_yaml": st.session_state.get("train_scan_yaml"),
    }


def save_template_file(target: Path, payload: dict):
    target.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")


def check_train_dependencies():
    missing = []
    if importlib.util.find_spec("ultralytics") is None:
        missing.append("ultralytics")
    if importlib.util.find_spec("torch") is None:
        missing.append("torch")
    return missing


@st.cache_data(show_spinner=False)
def scan_dataset_configs(root_str: str):
    root = Path(root_str) if root_str else None
    if not root or not root.exists():
        return []
    patterns = ["data.yaml", "dataset.yaml", "data.yml", "dataset.yml"]
    found = []
    for pattern in patterns:
        found.extend(root.rglob(pattern))
    unique = sorted({p.resolve() for p in found if p.is_file()})
    return unique


def load_dataset_yaml(path_str: str):
    if not path_str:
        return None, "è·¯å¾„ä¸ºç©º"
    path = Path(path_str)
    if not path.exists():
        return None, "æœªæ‰¾åˆ°æ•°æ®é›†é…ç½®æ–‡ä»¶"
    try:
        import yaml
    except Exception:
        return None, "æœªå®‰è£… pyyamlï¼Œæ— æ³•è¯»å–æ•°æ®é›†è¯¦æƒ…"
    try:
        content = path.read_text(encoding="utf-8")
        data = yaml.safe_load(content)
        return data, None
    except Exception as exc:
        return None, f"è¯»å–å¤±è´¥ï¼š{exc}"


def count_images_in_dir(dir_path: Path):
    if not dir_path or not dir_path.exists():
        return None
    exts = {".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff", ".webp"}
    try:
        return sum(1 for p in dir_path.iterdir() if p.is_file() and p.suffix.lower() in exts)
    except Exception:
        return None


def format_int_safe(value):
    return "-" if value is None else f"{value:,}"


def summarize_dataset(path_str: str):
    data, err = load_dataset_yaml(path_str)
    if err:
        return {"error": err}
    base_path = Path(path_str).parent
    root_value = data.get("path")
    if root_value:
        root_value = Path(root_value)
        root_path = root_value if root_value.is_absolute() else (base_path / root_value).resolve()
    else:
        root_path = base_path
    train_dir = root_path / str(data.get("train", ""))
    val_dir = root_path / str(data.get("val", ""))
    test_dir = root_path / str(data.get("test", ""))
    return {
        "nc": data.get("nc"),
        "names": data.get("names"),
        "path": str(root_path),
        "train_dir": str(train_dir),
        "val_dir": str(val_dir),
        "test_dir": str(test_dir),
        "train_images": count_images_in_dir(train_dir),
        "val_images": count_images_in_dir(val_dir),
        "test_images": count_images_in_dir(test_dir),
    }


def get_cuda_summary():
    try:
        import torch
    except Exception:
        return {"available": False, "detail": "æœªå®‰è£… torch"}
    available = torch.cuda.is_available()
    if not available:
        return {"available": False, "detail": "CUDA ä¸å¯ç”¨"}
    devices = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]
    return {"available": True, "detail": f"{len(devices)} å¼ GPU", "devices": devices}


def list_dataset_roots_from_configs(config_paths):
    roots = []
    for path in config_paths:
        try:
            data, err = load_dataset_yaml(str(path))
            if err or not data:
                roots.append(Path(path).parent.resolve())
                continue
            base_path = Path(path).parent
            root_value = data.get("path")
            if root_value:
                root_value = Path(root_value)
                root_path = root_value if root_value.is_absolute() else (base_path / root_value).resolve()
            else:
                root_path = base_path
            roots.append(root_path)
        except Exception:
            roots.append(Path(path).parent.resolve())
    unique = []
    seen = set()
    for item in roots:
        if str(item) not in seen:
            unique.append(item)
            seen.add(str(item))
    return unique


def collect_image_files(dir_path: Path, max_images: int = 24, shuffle: bool = True, recursive: bool = True):
    if not dir_path or not dir_path.exists():
        return []
    exts = {".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff", ".webp"}
    iterator = dir_path.rglob("*") if recursive else dir_path.iterdir()
    files = [p for p in iterator if p.is_file() and p.suffix.lower() in exts]
    if not files:
        return []
    if shuffle:
        random.shuffle(files)
    return files[:max_images]


def list_subdirectories(path_str: str, include_hidden: bool = False, max_items: int = 200):
    if not path_str:
        return []
    base = Path(path_str)
    if not base.exists() or not base.is_dir():
        return []
    items = []
    for p in base.iterdir():
        if not p.is_dir():
            continue
        if not include_hidden and p.name.startswith("."):
            continue
        items.append(p)
    items = sorted(items, key=lambda x: x.name.lower())
    return items[:max_items]


def list_yaml_files(path_str: str, max_items: int = 300):
    if not path_str:
        return []
    base = Path(path_str)
    if not base.exists():
        return []
    if base.is_file() and base.suffix.lower() in {".yaml", ".yml"}:
        return [base]
    patterns = ["data.yaml", "dataset.yaml", "data.yml", "dataset.yml"]
    files = []
    for pattern in patterns:
        files.extend(base.rglob(pattern))
    files = sorted({p.resolve() for p in files if p.is_file()})
    return files[:max_items]


@st.cache_data(show_spinner=False)
def list_image_files_for_preview(path_str: str, recursive: bool, max_files: int):
    base = Path(path_str)
    if not base.exists() or not base.is_dir():
        return []
    exts = {".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff", ".webp"}
    files = []
    if recursive:
        for root, _, filenames in os.walk(base):
            for name in filenames:
                file_path = Path(root) / name
                if file_path.suffix.lower() in exts:
                    try:
                        stat = file_path.stat()
                        files.append({"path": str(file_path), "size": stat.st_size, "mtime": stat.st_mtime})
                    except Exception:
                        files.append({"path": str(file_path), "size": 0, "mtime": 0})
                    if len(files) >= max_files:
                        return files
    else:
        for p in base.iterdir():
            if p.is_file() and p.suffix.lower() in exts:
                try:
                    stat = p.stat()
                    files.append({"path": str(p), "size": stat.st_size, "mtime": stat.st_mtime})
                except Exception:
                    files.append({"path": str(p), "size": 0, "mtime": 0})
                if len(files) >= max_files:
                    break
    return files


def get_immediate_children_sizes(path: Path, max_items: int = 10):
    if not path.exists() or not path.is_dir():
        return [], []
    dir_sizes = []
    file_sizes = []
    for entry in path.iterdir():
        if entry.is_dir():
            size = 0
            try:
                for item in entry.iterdir():
                    if item.is_file():
                        try:
                            size += item.stat().st_size
                        except Exception:
                            pass
            except Exception:
                pass
            dir_sizes.append((entry.name, size))
        elif entry.is_file():
            try:
                size = entry.stat().st_size
            except Exception:
                size = 0
            file_sizes.append((entry.name, size))
    dir_sizes = sorted(dir_sizes, key=lambda x: x[1], reverse=True)[:max_items]
    file_sizes = sorted(file_sizes, key=lambda x: x[1], reverse=True)[:max_items]
    return dir_sizes, file_sizes


def build_tree_flat(root: Path, include_hidden: bool, max_depth: int, max_nodes: int):
    nodes = []
    root_str = str(root)

    def _walk(path: Path, parent: str, depth: int):
        if depth > max_depth or len(nodes) >= max_nodes:
            return
        try:
            children = [
                p for p in path.iterdir()
                if p.is_dir() and (include_hidden or not p.name.startswith("."))
            ]
        except Exception:
            return
        children = sorted(children, key=lambda x: x.name.lower())
        for child in children:
            if len(nodes) >= max_nodes:
                break
            child_str = str(child)
            try:
                has_children = any(
                    p.is_dir() and (include_hidden or not p.name.startswith("."))
                    for p in child.iterdir()
                )
            except Exception:
                has_children = False
            nodes.append({
                "id": child_str,
                "parent": parent,
                "name": child.name,
                "path": child_str,
                "depth": depth,
                "has_children": has_children,
            })
            _walk(child, child_str, depth + 1)

    _walk(root, root_str, 0)
    return nodes, root_str


def filter_tree_nodes(nodes, query: str):
    if not query:
        return nodes
    key = query.lower()
    by_id = {n["id"]: n for n in nodes}
    parent_map = {n["id"]: n["parent"] for n in nodes}
    keep = set()
    for n in nodes:
        if key in n["name"].lower():
            current = n["id"]
            while current and current in parent_map:
                keep.add(current)
                current = parent_map.get(current)
    return [n for n in nodes if n["id"] in keep]


def render_advanced_tree_component(nodes, root_id: str, expanded, selected):
    payload = json.dumps({
        "nodes": nodes,
        "root": root_id,
        "expanded": expanded or [],
        "selected": selected or "",
    }, ensure_ascii=False)
    html = f"""
    <style>
      .adv-tree {{ font-family: inherit; font-size: 13px; color: #0f172a; }}
      .adv-row {{ display: flex; align-items: center; gap: 6px; padding: 4px 6px; border-radius: 6px; }}
      .adv-row:hover {{ background: rgba(59,130,246,0.08); }}
      .adv-selected {{ background: rgba(37,99,235,0.12); border: 1px solid rgba(37,99,235,0.3); }}
      .adv-toggle {{ width: 14px; text-align: center; cursor: pointer; }}
      .adv-name {{ font-weight: 600; }}
      .adv-icon svg {{ width: 16px; height: 12px; color: #2563eb; }}
      .adv-menu {{ position: fixed; background: #fff; border: 1px solid rgba(15,23,42,0.15); box-shadow: 0 10px 24px rgba(15,23,42,0.15); border-radius: 8px; padding: 6px; display: none; z-index: 9999; }}
      .adv-menu button {{ width: 100%; margin: 2px 0; padding: 6px 10px; border-radius: 6px; border: 1px solid #e2e8f0; background: #f8fafc; cursor: pointer; }}
    </style>
    <div class="adv-tree" id="adv-tree"></div>
    <div class="adv-menu" id="adv-menu">
      <button data-action="preview">é¢„è§ˆ</button>
      <button data-action="set_root">è®¾ä¸ºæ ¹ç›®å½•</button>
      <button data-action="copy">å¤åˆ¶è·¯å¾„</button>
    </div>
    <script>
      const payload = {payload};
      const nodes = payload.nodes || [];
      const rootId = payload.root;
      const expanded = new Set(payload.expanded || []);
      let selected = payload.selected || '';
      const treeEl = document.getElementById('adv-tree');
      const menuEl = document.getElementById('adv-menu');
      let menuPath = '';

      const byParent = {{}};
      nodes.forEach(n => {{
        if (!byParent[n.parent]) byParent[n.parent] = [];
        byParent[n.parent].push(n);
      }});

      function send(action, path) {{
        const msg = {{
          action,
          path,
          expanded: Array.from(expanded),
          selected: path
        }};
        window.parent.postMessage({{
          isStreamlitMessage: true,
          type: 'streamlit:setComponentValue',
          value: JSON.stringify(msg)
        }}, '*');
      }}

      function render(parentId, container, depth) {{
        const children = byParent[parentId] || [];
        children.forEach(node => {{
          const row = document.createElement('div');
          row.className = 'adv-row' + (node.path === selected ? ' adv-selected' : '');
          row.style.marginLeft = (depth * 14) + 'px';
          const toggle = document.createElement('div');
          toggle.className = 'adv-toggle';
          toggle.textContent = node.has_children ? (expanded.has(node.path) ? 'â–¾' : 'â–¸') : '';
          toggle.onclick = (e) => {{
            e.stopPropagation();
            if (!node.has_children) return;
            if (expanded.has(node.path)) expanded.delete(node.path);
            else expanded.add(node.path);
            renderTree();
          }};
          const icon = document.createElement('span');
          icon.className = 'adv-icon';
          icon.innerHTML = "<svg viewBox='0 0 24 16'><path d='M2 3.5C2 2.7 2.7 2 3.5 2h5.2c.4 0 .8.2 1 .5l.9 1.3c.2.3.6.5 1 .5h8.9c.8 0 1.5.7 1.5 1.5v6.2c0 .8-.7 1.5-1.5 1.5H3.5c-.8 0-1.5-.7-1.5-1.5V3.5z'/></svg>";
          const name = document.createElement('span');
          name.className = 'adv-name';
          name.textContent = node.name;
          row.appendChild(toggle);
          row.appendChild(icon);
          row.appendChild(name);
          row.onclick = () => {{
            selected = node.path;
            send('select', node.path);
          }};
          row.oncontextmenu = (e) => {{
            e.preventDefault();
            menuPath = node.path;
            menuEl.style.display = 'block';
            menuEl.style.left = e.clientX + 'px';
            menuEl.style.top = e.clientY + 'px';
          }};
          container.appendChild(row);
          if (node.has_children && expanded.has(node.path)) {{
            render(node.path, container, depth + 1);
          }}
        }});
      }}

      function renderTree() {{
        treeEl.innerHTML = '';
        render(rootId, treeEl, 0);
      }}

      document.addEventListener('click', (e) => {{
        if (!menuEl.contains(e.target)) {{
          menuEl.style.display = 'none';
        }}
      }});

      menuEl.addEventListener('click', (e) => {{
        const action = e.target.getAttribute('data-action');
        if (!action) return;
        if (action === 'copy') {{
          try {{ navigator.clipboard.writeText(menuPath); }} catch (err) {{}}
        }} else {{
          send(action, menuPath);
        }}
        menuEl.style.display = 'none';
      }});

      renderTree();
    </script>
    """
    return components.html(html, height=520, scrolling=True)

def list_immediate_dirs(path_str: str, include_hidden: bool = False):
    base = Path(path_str)
    if not base.exists() or not base.is_dir():
        return []
    items = [
        p for p in base.iterdir()
        if p.is_dir() and (include_hidden or not p.name.startswith("."))
    ]
    return sorted(items, key=lambda x: x.name.lower())


def get_path_suggestions(current_value: str, include_hidden: bool = False, max_items: int = 50):
    if not current_value:
        return []
    expanded = os.path.expanduser(current_value)
    candidate = Path(expanded)
    parent = candidate if candidate.is_dir() else candidate.parent
    if not parent.exists():
        return []
    items = list_immediate_dirs(str(parent), include_hidden=include_hidden)
    suggestions = [str(p) for p in items]
    if current_value not in suggestions:
        suggestions.insert(0, current_value)
    return suggestions[:max_items]


def ensure_favorite_groups(default_group: str = "é»˜è®¤"):
    if "train_favorite_groups" not in st.session_state:
        groups = {}
        legacy = st.session_state.get("train_favorite_paths", [])
        groups[default_group] = list(legacy) if legacy else []
        st.session_state["train_favorite_groups"] = groups
    return st.session_state.get("train_favorite_groups", {})


def add_favorite_path(path_str: str, group: str = "é»˜è®¤", max_items: int = 12):
    if not path_str:
        return
    groups = ensure_favorite_groups()
    if group not in groups:
        groups[group] = []
    if path_str in groups[group]:
        return
    groups[group].append(path_str)
    groups[group] = groups[group][:max_items]
    st.session_state["train_favorite_groups"] = groups


def remove_favorite_path(path_str: str, group: str):
    groups = ensure_favorite_groups()
    if group in groups and path_str in groups[group]:
        groups[group].remove(path_str)
        st.session_state["train_favorite_groups"] = groups


def add_favorite_group(name: str):
    groups = ensure_favorite_groups()
    if not name:
        return
    if name not in groups:
        groups[name] = []
        st.session_state["train_favorite_groups"] = groups


def delete_favorite_group(name: str, default_group: str = "é»˜è®¤"):
    groups = ensure_favorite_groups()
    if name in groups and name != default_group:
        del groups[name]
        st.session_state["train_favorite_groups"] = groups


def build_category_preview_options(dataset_root: Path, dataset_yaml_paths):
    options = {}
    if not dataset_yaml_paths:
        return options
    for path in dataset_yaml_paths:
        try:
            root = Path(dataset_root)
            label = str(Path(path).parent.name)
            if root.exists():
                try:
                    rel = Path(path).parent.relative_to(root)
                    label = str(rel)
                except Exception:
                    pass
            if label in options:
                label = f"{label} ({Path(path).parent})"
            options[label] = str(path)
        except Exception:
            continue
    return options


def safe_filename(value: str) -> str:
    if not value:
        return "train"
    cleaned = re.sub(r"[^A-Za-z0-9._-]+", "_", value).strip("_")
    return cleaned or "train"


def build_dir_tree_nodes(root: Path, max_depth: int = 4, max_nodes: int = 2000, include_hidden: bool = False):
    count = 0

    def _label(path: Path) -> str:
        name = path.name or str(path)
        return f"DIR {name}"

    def _children(path: Path, depth: int):
        nonlocal count
        if depth > max_depth or count >= max_nodes:
            return []
        try:
            entries = [
                p for p in path.iterdir()
                if p.is_dir() and (include_hidden or not p.name.startswith("."))
            ]
        except Exception:
            return []
        entries = sorted(entries, key=lambda x: x.name.lower())
        nodes = []
        for p in entries:
            if count >= max_nodes:
                break
            count += 1
            node = {"label": _label(p), "value": str(p)}
            children = _children(p, depth + 1)
            if children:
                node["children"] = children
            nodes.append(node)
        return nodes

    if not root.exists() or not root.is_dir():
        return [], 0
    root_node = {"label": _label(root), "value": str(root)}
    root_children = _children(root, 1)
    if root_children:
        root_node["children"] = root_children
    return [root_node], count


@st.cache_data(show_spinner=False)
def build_dir_tree_nodes_cached(root_str: str, max_depth: int, max_nodes: int, include_hidden: bool):
    return build_dir_tree_nodes(Path(root_str), max_depth=max_depth, max_nodes=max_nodes, include_hidden=include_hidden)


def render_copy_button(path_value: str, key: str):
    escaped = path_value.replace("\\", "\\\\").replace('"', '\\"')
    html = f"""
    <button style="padding:4px 10px;border-radius:8px;border:1px solid #cbd5f5;background:#f8fafc;cursor:pointer;" onclick="navigator.clipboard.writeText('{escaped}')">å¤åˆ¶è·¯å¾„</button>
    """
    components.html(html, height=32, width=110)


def copy_to_clipboard(path_value: str):
    escaped = json.dumps(path_value)
    html = f"""
    <script>
    try {{
      navigator.clipboard.writeText({escaped});
    }} catch (e) {{}}
    </script>
    """
    components.html(html, height=0, width=0)


def add_recent_path(path_str: str, max_items: int = 8):
    if not path_str:
        return
    recent = list(st.session_state.get("train_recent_paths", []))
    if path_str in recent:
        recent.remove(path_str)
    recent.insert(0, path_str)
    st.session_state["train_recent_paths"] = recent[:max_items]


def search_directories(root: Path, query: str, include_hidden: bool, max_results: int = 60):
    if not root.exists() or not query:
        return []
    query_lower = query.lower()
    results = []
    for path in root.rglob("*"):
        if not path.is_dir():
            continue
        if not include_hidden and path.name.startswith("."):
            continue
        if query_lower in path.name.lower():
            results.append(path)
            if len(results) >= max_results:
                break
    return results


def _stable_key(value: str) -> str:
    return hashlib.md5(value.encode("utf-8")).hexdigest()


def collect_dir_paths(root: Path, include_hidden: bool, max_depth: int, max_nodes: int):
    paths = []

    def _walk(path: Path, depth: int):
        if depth > max_depth or len(paths) >= max_nodes:
            return
        try:
            children = [
                p for p in path.iterdir()
                if p.is_dir() and (include_hidden or not p.name.startswith("."))
            ]
        except Exception:
            return
        for child in sorted(children, key=lambda x: x.name.lower()):
            if len(paths) >= max_nodes:
                break
            paths.append(str(child))
            _walk(child, depth + 1)

    _walk(root, 1)
    return paths


def render_icon_tree_custom(root: Path, include_hidden: bool, max_depth: int, max_nodes: int, filter_query: str = ""):
    expanded = set(st.session_state.get("train_icon_tree_expanded", []))
    selected = st.session_state.get("train_icon_tree_selected", "")
    menu_open = st.session_state.get("train_icon_tree_menu_open", "")
    counter = [0]
    filter_key = filter_query.strip().lower()

    def _matches(path: Path) -> bool:
        if not filter_key:
            return True
        return filter_key in path.name.lower()

    def _has_match_descendant(path: Path, depth: int) -> bool:
        if depth > max_depth:
            return False
        try:
            for child in path.iterdir():
                if child.is_dir() and (include_hidden or not child.name.startswith(".")):
                    if _matches(child):
                        return True
                    if _has_match_descendant(child, depth + 1):
                        return True
        except Exception:
            return False
        return False

    def _render(path: Path, depth: int):
        if depth > max_depth or counter[0] >= max_nodes:
            return
        try:
            children = [
                p for p in path.iterdir()
                if p.is_dir() and (include_hidden or not p.name.startswith("."))
            ]
        except Exception:
            return
        children = sorted(children, key=lambda x: x.name.lower())
        for child in children:
            if counter[0] >= max_nodes:
                break
            counter[0] += 1
            child_str = str(child)
            if filter_key and not _matches(child) and not _has_match_descendant(child, depth + 1):
                continue
            is_expanded = child_str in expanded
            selected_class = "fm-selected" if child_str == selected else ""
            indent_px = depth * 16

            cols = st.columns([0.8, 5.4, 1.2, 1.2])
            with cols[0]:
                toggle_label = "â–¾" if is_expanded else "â–¸"
                if st.button(toggle_label, key=f"train_icon_toggle_{_stable_key(child_str)}"):
                    if is_expanded:
                        expanded.discard(child_str)
                    else:
                        expanded.add(child_str)
                    st.session_state["train_icon_tree_expanded"] = list(expanded)
                    trigger_rerun()
            with cols[1]:
                drag_path = child_str.replace("\\", "\\\\").replace("'", "\\'")
                st.markdown(
                    (
                        f"<div class='fm-node fm-depth-{depth} {selected_class}' "
                        f"style='margin-left:{indent_px}px;' "
                        f"draggable='true' "
                        f"ondragstart=\"event.dataTransfer.setData('text/plain', '{drag_path}');\">"
                        "<span class='fm-icon'>"
                        "<svg viewBox='0 0 24 16' aria-hidden='true'>"
                        "<path d='M2 3.5C2 2.7 2.7 2 3.5 2h5.2c.4 0 .8.2 1 .5l.9 1.3c.2.3.6.5 1 .5h8.9c.8 0 1.5.7 1.5 1.5v6.2c0 .8-.7 1.5-1.5 1.5H3.5c-.8 0-1.5-.7-1.5-1.5V3.5z'/>"
                        "</svg>"
                        "</span>"
                        f"<span class='fm-name'>{child.name}</span>"
                        "</div>"
                    ),
                    unsafe_allow_html=True,
                )
            with cols[2]:
                if st.button("é€‰æ‹©", key=f"train_icon_pick_{_stable_key(child_str)}", width='stretch'):
                    st.session_state["train_icon_tree_selected"] = child_str
                    st.session_state["train_preview_path"] = child_str
                    add_recent_path(child_str)
                    st.session_state["train_dataset_root"] = child_str
                    trigger_rerun()
            with cols[3]:
                if st.button("â‹¯", key=f"train_icon_menu_{_stable_key(child_str)}", width='stretch'):
                    st.session_state["train_icon_tree_menu_open"] = "" if menu_open == child_str else child_str
                    trigger_rerun()

            if menu_open == child_str:
                action_cols = st.columns([1.4, 1.8, 1.4])
                with action_cols[0]:
                    if st.button("é¢„è§ˆ", key=f"train_icon_preview_{_stable_key(child_str)}", width='stretch'):
                        st.session_state["train_preview_path"] = child_str
                        trigger_rerun()
                with action_cols[1]:
                    if st.button("è®¾ä¸ºæ ¹ç›®å½•", key=f"train_icon_root_{_stable_key(child_str)}", width='stretch'):
                        st.session_state["train_dataset_root"] = child_str
                        st.session_state["train_browse_root"] = child_str
                        add_recent_path(child_str)
                        trigger_rerun()
                with action_cols[2]:
                    render_copy_button(child_str, f"train_icon_copy_{_stable_key(child_str)}")

            if is_expanded:
                _render(child, depth + 1)

    _render(root, 0)


def format_bytes(value: int):
    if value is None:
        return "-"
    units = ["B", "KB", "MB", "GB", "TB"]
    size = float(value)
    idx = 0
    while size >= 1024 and idx < len(units) - 1:
        size /= 1024
        idx += 1
    return f"{size:.2f} {units[idx]}"


def get_dir_stats(path: Path, recursive: bool = False, max_files: int = 5000, max_depth: int = 6):
    if not path.exists() or not path.is_dir():
        return {"files": 0, "dirs": 0, "bytes": 0, "truncated": False}
    total_bytes = 0
    files = 0
    dirs = 0
    truncated = False

    if not recursive:
        for item in path.iterdir():
            if item.is_dir():
                dirs += 1
            elif item.is_file():
                files += 1
                try:
                    total_bytes += item.stat().st_size
                except Exception:
                    pass
        return {"files": files, "dirs": dirs, "bytes": total_bytes, "truncated": False}

    base_depth = len(path.parts)
    for root, dirnames, filenames in os.walk(path):
        depth = len(Path(root).parts) - base_depth
        if depth > max_depth:
            dirnames[:] = []
            continue
        dirs += len(dirnames)
        for name in filenames:
            files += 1
            if files > max_files:
                truncated = True
                return {"files": files, "dirs": dirs, "bytes": total_bytes, "truncated": True}
            file_path = Path(root) / name
            try:
                total_bytes += file_path.stat().st_size
            except Exception:
                pass
    return {"files": files, "dirs": dirs, "bytes": total_bytes, "truncated": truncated}


def run_yolo_training(model_path: str, data_yaml: str, train_kwargs: dict, env_vars: dict):
    buffer = io.StringIO()
    model = None
    error = None
    results = None
    save_dir = None
    with redirect_stdout(buffer), redirect_stderr(buffer):
        try:
            if env_vars:
                for key, value in env_vars.items():
                    if value:
                        os.environ[str(key)] = str(value)
            from ultralytics import YOLO

            model = YOLO(model_path)
            results = model.train(data=data_yaml, **train_kwargs)
            trainer = getattr(model, "trainer", None)
            save_dir = getattr(trainer, "save_dir", None) if trainer else None
            if save_dir is None and hasattr(results, "save_dir"):
                save_dir = getattr(results, "save_dir")
        except Exception as exc:
            error = exc
    return results, buffer.getvalue(), save_dir, error


LOG_DONE = object()


class StreamQueueWriter:
    def __init__(self, log_queue: "queue.Queue[str]"):
        self.log_queue = log_queue
        self._buffer = ""

    def write(self, data):
        if not data:
            return
        self._buffer += data
        while "\n" in self._buffer:
            line, self._buffer = self._buffer.split("\n", 1)
            self.log_queue.put(line)

    def flush(self):
        if self._buffer:
            self.log_queue.put(self._buffer)
            self._buffer = ""


def _extract_epoch_info(line: str):
    if not line:
        return None
    match = re.search(r"[Ee]poch\s*(\d+)\s*/\s*(\d+)", line)
    if match:
        return int(match.group(1)), int(match.group(2))
    return None


def run_yolo_training_stream(model_path: str, data_yaml: str, train_kwargs: dict, env_vars: dict, log_queue: "queue.Queue", result_holder: dict):
    writer = StreamQueueWriter(log_queue)
    with redirect_stdout(writer), redirect_stderr(writer):
        try:
            if env_vars:
                for key, value in env_vars.items():
                    if value:
                        os.environ[str(key)] = str(value)
            from ultralytics import YOLO

            model = YOLO(model_path)
            results = model.train(data=data_yaml, **train_kwargs)
            trainer = getattr(model, "trainer", None)
            save_dir = getattr(trainer, "save_dir", None) if trainer else None
            if save_dir is None and hasattr(results, "save_dir"):
                save_dir = getattr(results, "save_dir")
            result_holder["save_dir"] = save_dir
            result_holder["results"] = results
        except Exception as exc:
            result_holder["error"] = exc
        finally:
            writer.flush()
            log_queue.put(LOG_DONE)


@st.cache_data(show_spinner=False)
def collect_run_dirs(root_str: str):
    root = Path(root_str) if root_str else None
    if not root or not root.exists():
        return []
    run_dirs = []
    for result_csv in root.rglob("results.csv"):
        run_dirs.append(result_csv.parent)
    unique = sorted({p.resolve() for p in run_dirs}, key=lambda p: p.stat().st_mtime, reverse=True)
    return unique


def render_run_visualization(run_dir: Path):
    if not run_dir or not run_dir.exists():
        st.info("æœªæ‰¾åˆ°è®­ç»ƒç»“æœç›®å½•ã€‚")
        return
    st.write(f"ç»“æœç›®å½•ï¼š`{run_dir}`")
    results_csv = run_dir / "results.csv"
    if results_csv.exists():
        try:
            df = pd.read_csv(results_csv)
            safe_dataframe(df.tail(20), width='stretch')
            numeric_cols = df.select_dtypes(include="number").columns.tolist()
            if "epoch" in df.columns:
                df_plot = df.set_index("epoch")[numeric_cols]
            else:
                df_plot = df[numeric_cols]
            if numeric_cols:
                st.line_chart(df_plot)
        except Exception as exc:
            st.warning(f"è¯»å– results.csv å¤±è´¥ï¼š{exc}")

    image_names = [
        "results.png",
        "confusion_matrix.png",
        "PR_curve.png",
        "F1_curve.png",
        "P_curve.png",
        "R_curve.png",
        "labels.jpg",
        "labels_correlogram.jpg",
        "train_batch0.jpg",
        "train_batch1.jpg",
        "train_batch2.jpg",
    ]
    images = []
    captions = []
    for name in image_names:
        path = run_dir / name
        if path.exists():
            images.append(str(path))
            captions.append(name)
    if images:
        st.image(images, caption=captions, width='stretch')

    weights_dir = run_dir / "weights"
    if weights_dir.exists():
        best = weights_dir / "best.pt"
        last = weights_dir / "last.pt"
        if best.exists() or last.exists():
            st.markdown("**æƒé‡æ–‡ä»¶**")
        if best.exists():
            st.download_button(
                "ä¸‹è½½ best.pt",
                data=best.read_bytes(),
                file_name=best.name,
                mime="application/octet-stream",
            )
        if last.exists():
            st.download_button(
                "ä¸‹è½½ last.pt",
                data=last.read_bytes(),
                file_name=last.name,
                mime="application/octet-stream",
            )


def render_training_platform():
    st.markdown("<div class='hero-title'>YOLO å¯è§†åŒ–è®­ç»ƒå¹³å°</div>", unsafe_allow_html=True)
    st.caption("é€‰æ‹©æ•°æ®é›†ã€è®¾ç½®è®­ç»ƒå‚æ•°ã€è¾“å‡ºå¯è§†åŒ–ç»“æœã€‚")

    try:
        from streamlit_tree_select import tree_select  # type: ignore
    except Exception:
        tree_select = None

    missing = check_train_dependencies()
    if missing:
        st.warning(f"è®­ç»ƒä¾èµ–æœªå®‰è£…ï¼š{', '.join(missing)}ã€‚è¯·å…ˆå®‰è£…ç›¸å…³åº“ã€‚")

    if "train_name" not in st.session_state:
        st.session_state.train_name = datetime.now().strftime("train_%Y%m%d_%H%M%S")
    if "train_project" not in st.session_state:
        st.session_state.train_project = str(Path.cwd() / "runs" / "train_platform")
    if "train_logs" not in st.session_state:
        st.session_state.train_logs = ""
    if "train_last_run" not in st.session_state:
        st.session_state.train_last_run = ""
    if "train_log_lines" not in st.session_state:
        st.session_state.train_log_lines = []
    if "train_log_file" not in st.session_state:
        st.session_state.train_log_file = ""

    templates_dir = Path.cwd() / "runs" / "train_platform" / "templates"
    templates_dir.mkdir(parents=True, exist_ok=True)
    template_files = sorted(templates_dir.glob("*.json"))
    logs_dir = Path.cwd() / "runs" / "train_platform" / "logs"
    logs_dir.mkdir(parents=True, exist_ok=True)

    with st.sidebar:
        st.markdown("<div class='sidebar-title'>è®­ç»ƒé…ç½®</div>", unsafe_allow_html=True)

        st.markdown("**å‚æ•°æ¨¡æ¿**")
        template_name = st.text_input("æ¨¡æ¿åç§°", value="", key="train_template_name")
        template_labels = [p.stem for p in template_files]
        template_pick = st.selectbox(
            "å·²æœ‰æ¨¡æ¿",
            options=["(æ— )"] + template_labels,
            index=0,
            key="train_template_pick",
        )
        template_cols = st.columns(3)
        with template_cols[0]:
            save_template = st.button("ä¿å­˜", width='stretch', key="train_template_save")
        with template_cols[1]:
            load_template = st.button("åŠ è½½", width='stretch', key="train_template_load")
        with template_cols[2]:
            delete_template = st.button("åˆ é™¤", width='stretch', key="train_template_delete")

        if save_template:
            name = template_name.strip() or datetime.now().strftime("template_%Y%m%d_%H%M%S")
            target = templates_dir / f"{name}.json"
            payload = build_train_template_payload()
            save_template_file(target, payload)
            st.success(f"å·²ä¿å­˜æ¨¡æ¿ï¼š{name}")

        if load_template and template_pick != "(æ— )":
            target = templates_dir / f"{template_pick}.json"
            try:
                payload = json.loads(target.read_text(encoding="utf-8"))
                st.session_state["train_dataset_root"] = payload.get("dataset_root", st.session_state.get("train_dataset_root"))
                st.session_state["train_dataset_manual"] = payload.get("data_yaml", "")
                st.session_state["train_model_path"] = payload.get("model_path", st.session_state.get("train_model_path"))
                st.session_state["train_project_input"] = payload.get("project", st.session_state.get("train_project_input"))
                st.session_state["train_name_input"] = payload.get("name", st.session_state.get("train_name_input"))
                st.session_state["train_exist_ok"] = payload.get("exist_ok", st.session_state.get("train_exist_ok"))
                st.session_state["train_epochs"] = payload.get("epochs", st.session_state.get("train_epochs"))
                st.session_state["train_imgsz"] = payload.get("imgsz", st.session_state.get("train_imgsz"))
                st.session_state["train_batch"] = payload.get("batch", st.session_state.get("train_batch"))
                st.session_state["train_workers"] = payload.get("workers", st.session_state.get("train_workers"))
                st.session_state["train_device"] = payload.get("device", st.session_state.get("train_device"))
                st.session_state["train_amp"] = payload.get("amp", st.session_state.get("train_amp"))
                st.session_state["train_cache"] = payload.get("cache", st.session_state.get("train_cache"))
                st.session_state["train_resume"] = payload.get("resume", st.session_state.get("train_resume"))
                st.session_state["train_optimizer"] = payload.get("optimizer", st.session_state.get("train_optimizer"))
                st.session_state["train_seed"] = payload.get("seed", st.session_state.get("train_seed"))
                st.session_state["train_patience"] = payload.get("patience", st.session_state.get("train_patience"))
                st.session_state["train_cos_lr"] = payload.get("cos_lr", st.session_state.get("train_cos_lr"))
                st.session_state["train_close_mosaic"] = payload.get("close_mosaic", st.session_state.get("train_close_mosaic"))
                st.session_state["train_save_period"] = payload.get("save_period", st.session_state.get("train_save_period"))
                st.session_state["train_advanced"] = payload.get("advanced_text", st.session_state.get("train_advanced"))
                st.session_state["train_cuda_visible"] = payload.get("cuda_visible_devices", st.session_state.get("train_cuda_visible"))
                st.session_state["train_scan_yaml"] = payload.get("scan_yaml", st.session_state.get("train_scan_yaml"))
                st.success(f"å·²åŠ è½½æ¨¡æ¿ï¼š{template_pick}")
                trigger_rerun()
            except Exception as exc:
                st.error(f"åŠ è½½æ¨¡æ¿å¤±è´¥ï¼š{exc}")

        if delete_template and template_pick != "(æ— )":
            target = templates_dir / f"{template_pick}.json"
            try:
                target.unlink(missing_ok=True)
                st.success(f"å·²åˆ é™¤æ¨¡æ¿ï¼š{template_pick}")
                trigger_rerun()
            except Exception as exc:
                st.error(f"åˆ é™¤æ¨¡æ¿å¤±è´¥ï¼š{exc}")

        st.markdown("---")

        dataset_default = Path.cwd() / "runs" / "latest" / "yolo_datasets"
        dataset_root_input = st.text_input(
            "è®­ç»ƒæ•°æ®é›†æ–‡ä»¶å¤¹",
            value=str(dataset_default) if dataset_default.exists() else str(Path.cwd()),
            key="train_dataset_root",
        )
        dataset_root_suggestions = get_path_suggestions(dataset_root_input, include_hidden=False)
        if dataset_root_suggestions:
            dataset_root = st.selectbox(
                "è·¯å¾„è‡ªåŠ¨è¡¥å…¨",
                options=dataset_root_suggestions,
                index=0,
                key="train_dataset_root_suggest",
            )
            if dataset_root != dataset_root_input:
                st.session_state["train_dataset_root"] = dataset_root
        else:
            dataset_root = dataset_root_input
        if not Path(dataset_root).exists():
            st.warning("å½“å‰è·¯å¾„ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥æˆ–ä½¿ç”¨ä¸‹æ–¹æµè§ˆå™¨é€‰æ‹©ã€‚")
        with st.expander("æµè§ˆç›®å½•", expanded=False):
            browse_root_input = st.text_input(
                "æµè§ˆèµ·ç‚¹",
                value=str(Path.cwd()),
                key="train_browse_root",
            )
            browse_suggestions = get_path_suggestions(browse_root_input, include_hidden=False)
            if browse_suggestions:
                browse_root = st.selectbox(
                    "è‡ªåŠ¨è¡¥å…¨",
                    options=browse_suggestions,
                    index=0,
                    key="train_browse_root_suggest",
                )
                if browse_root != browse_root_input:
                    st.session_state["train_browse_root"] = browse_root
            else:
                browse_root = browse_root_input
            show_hidden = st.checkbox("æ˜¾ç¤ºéšè—ç›®å½•", value=False, key="train_browse_hidden")
            st.caption("æ¨èï¼šä½¿ç”¨æ ‘çŠ¶é€‰æ‹©å¿«é€Ÿå®šä½ç›®å½•ã€‚")
            use_tree = st.checkbox("å¯ç”¨æ ‘çŠ¶ç›®å½•é€‰æ‹©", value=True, key="train_browse_use_tree")
            use_file_manager = st.checkbox("æ–‡ä»¶ç®¡ç†å™¨é£æ ¼ï¼ˆæœç´¢/é¢åŒ…å±‘/æœ€è¿‘è·¯å¾„ï¼‰", value=True, key="train_use_file_manager")
            if use_file_manager:
                current_path = Path(st.session_state.get("train_dataset_root") or browse_root)
                if not current_path.exists():
                    current_path = Path(browse_root)
                st.markdown("<div class='file-manager'>", unsafe_allow_html=True)
                shortcut_enabled = st.checkbox("å¯ç”¨å¿«æ·é”®", value=True, key="train_shortcuts_enable")
                st.caption("å¿«æ·é”®ï¼šAlt+E å±•å¼€ | Alt+W æ”¶èµ· | Alt+F æ”¶è— | Alt+S è®¾ä¸ºæ ¹ | Alt+P é¢„è§ˆ | Alt+U ä¸Šä¸€çº§ | Alt+C å¤åˆ¶è·¯å¾„")
                if shortcut_enabled:
                    key_event = components.html(
                        """
                        <script>
                        document.addEventListener('keydown', function(e) {
                          if (!e.altKey || e.repeat) return;
                          const key = e.key;
                          let value = '';
                          if (key === 'ArrowUp') value = 'alt+u';
                          else value = 'alt+' + key.toLowerCase();
                          window.parent.postMessage({
                            isStreamlitMessage: true,
                            type: 'streamlit:setComponentValue',
                            value
                          }, '*');
                          e.preventDefault();
                        }, { once: false });
                        </script>
                        """,
                        height=0,
                    )
                    if isinstance(key_event, str) and key_event:
                        needs_rerun = False
                        selected_path = st.session_state.get("train_preview_path") or str(current_path)
                        active_group = st.session_state.get("train_fav_group", "é»˜è®¤")
                        if key_event == "alt+e":
                            all_paths = collect_dir_paths(current_path, show_hidden, st.session_state.get("train_icon_tree_depth", 3), st.session_state.get("train_icon_tree_nodes", 300))
                            st.session_state["train_icon_tree_expanded"] = all_paths
                            needs_rerun = True
                        elif key_event == "alt+w":
                            st.session_state["train_icon_tree_expanded"] = []
                            needs_rerun = True
                        elif key_event == "alt+f":
                            add_favorite_path(selected_path, group=active_group)
                            needs_rerun = True
                        elif key_event == "alt+s":
                            st.session_state["train_dataset_root"] = selected_path
                            add_recent_path(selected_path)
                            needs_rerun = True
                        elif key_event == "alt+p":
                            st.session_state["train_preview_path"] = selected_path
                            needs_rerun = True
                        elif key_event == "alt+u":
                            parent = str(Path(selected_path).parent)
                            st.session_state["train_dataset_root"] = parent
                            st.session_state["train_preview_path"] = parent
                            add_recent_path(parent)
                            needs_rerun = True
                        elif key_event == "alt+c":
                            copy_to_clipboard(selected_path)
                        if needs_rerun:
                            trigger_rerun()
                left_col, right_col = st.columns([1.1, 1.4])
                with left_col:
                    st.caption("å½“å‰ä½ç½®")
                    st.markdown(f"<div class='fm-path'>{current_path}</div>", unsafe_allow_html=True)
                    crumbs = []
                    accum = Path(current_path.anchor) if current_path.anchor else Path("/")
                    for part in current_path.parts:
                        if part == current_path.anchor:
                            crumbs.append((part, str(accum)))
                            continue
                        accum = accum / part
                        crumbs.append((part, str(accum)))
                    if crumbs:
                        st.caption("é¢åŒ…å±‘")
                        crumb_cols = st.columns(min(len(crumbs), 6))
                        for idx, (label, path_value) in enumerate(crumbs[:6]):
                            with crumb_cols[idx]:
                                if st.button(label if label else "/", key=f"train_crumb_{idx}", width='stretch'):
                                    st.session_state["train_dataset_root"] = path_value
                                    add_recent_path(path_value)
                                    trigger_rerun()
                    groups = ensure_favorite_groups()
                    group_names = list(groups.keys())
                    if not group_names:
                        group_names = ["é»˜è®¤"]
                    active_group = st.selectbox("æ”¶è—ç»„", options=group_names, index=0, key="train_fav_group")
                    new_group = st.text_input("æ–°å»ºæ”¶è—ç»„", value="", key="train_fav_new_group")
                    group_cols = st.columns(2)
                    with group_cols[0]:
                        if st.button("æ·»åŠ ç»„", key="train_fav_add_group", width='stretch'):
                            add_favorite_group(new_group.strip())
                            trigger_rerun()
                    with group_cols[1]:
                        if st.button("åˆ é™¤ç»„", key="train_fav_del_group", width='stretch'):
                            delete_favorite_group(active_group)
                            trigger_rerun()

                    if groups:
                        st.caption("æ”¶è—è·¯å¾„ï¼ˆåˆ†ç»„ï¼‰")
                        for group_name, paths in groups.items():
                            with st.expander(f"{group_name} ({len(paths)})", expanded=False):
                                for idx, path_value in enumerate(paths):
                                    fav_cols = st.columns([3.2, 1])
                                    with fav_cols[0]:
                                        if st.button(path_value, key=f"train_fav_{group_name}_{idx}", width='stretch'):
                                            st.session_state["train_dataset_root"] = path_value
                                            trigger_rerun()
                                    with fav_cols[1]:
                                        if st.button("ç§»é™¤", key=f"train_fav_remove_{group_name}_{idx}", width='stretch'):
                                            remove_favorite_path(path_value, group_name)
                                            trigger_rerun()

                    recent = st.session_state.get("train_recent_paths", [])
                    if recent:
                        st.caption("æœ€è¿‘è·¯å¾„")
                        for idx, path_value in enumerate(recent):
                            if st.button(path_value, key=f"train_recent_{idx}", width='stretch'):
                                st.session_state["train_dataset_root"] = path_value
                                trigger_rerun()

                    search_query = st.text_input("æœç´¢ç›®å½•", value="", key="train_dir_search")
                    if search_query.strip():
                        results = search_directories(current_path, search_query.strip(), show_hidden, max_results=40)
                        if results:
                            st.caption(f"æ‰¾åˆ° {len(results)} ä¸ªç›®å½•")
                            for idx, path_value in enumerate(results):
                                if st.button(str(path_value), key=f"train_search_{idx}", width='stretch'):
                                    st.session_state["train_dataset_root"] = str(path_value)
                                    add_recent_path(str(path_value))
                                    trigger_rerun()
                        else:
                            st.info("æœªæ‰¾åˆ°åŒ¹é…ç›®å½•ã€‚")

                    st.caption("ç›®å½•æ ‘æ¨¡å¼")
                    tree_mode = st.radio(
                        "é€‰æ‹©ç›®å½•æ ‘",
                        options=["å›¾æ ‡æ ‘(æ‹–æ‹½)", "é«˜çº§æ ‘(å³é”®èœå•)"],
                        index=0,
                        key="train_tree_mode",
                        horizontal=True,
                    )
                    tree_filter = st.text_input("ç›®å½•è¿‡æ»¤", value="", key="train_tree_filter")
                    icon_tree_depth = st.slider("å›¾æ ‡æ ‘æ·±åº¦", min_value=1, max_value=6, value=3, step=1, key="train_icon_tree_depth")
                    icon_tree_nodes = st.slider("å›¾æ ‡æ ‘èŠ‚ç‚¹ä¸Šé™", min_value=50, max_value=1200, value=300, step=50, key="train_icon_tree_nodes")
                    tree_action_cols = st.columns(3)
                    with tree_action_cols[0]:
                        if st.button("å±•å¼€å…¨éƒ¨", key="train_icon_expand_all", width='stretch'):
                            all_paths = collect_dir_paths(current_path, show_hidden, icon_tree_depth, icon_tree_nodes)
                            st.session_state["train_icon_tree_expanded"] = all_paths
                            trigger_rerun()
                    with tree_action_cols[1]:
                        if st.button("æ”¶èµ·å…¨éƒ¨", key="train_icon_collapse_all", width='stretch'):
                            st.session_state["train_icon_tree_expanded"] = []
                            trigger_rerun()
                    with tree_action_cols[2]:
                        if st.button("æ¸…é™¤é€‰ä¸­", key="train_icon_clear_sel", width='stretch'):
                            st.session_state["train_icon_tree_selected"] = ""
                            trigger_rerun()

                    if tree_mode == "é«˜çº§æ ‘(å³é”®èœå•)":
                        nodes, root_id = build_tree_flat(current_path, show_hidden, icon_tree_depth, icon_tree_nodes)
                        nodes = filter_tree_nodes(nodes, tree_filter.strip())
                        adv_value = render_advanced_tree_component(
                            nodes,
                            root_id,
                            st.session_state.get("train_adv_tree_expanded", []),
                            st.session_state.get("train_adv_tree_selected", ""),
                        )
                        if isinstance(adv_value, str) and adv_value:
                            try:
                                payload = json.loads(adv_value)
                            except Exception:
                                payload = {}
                            action = payload.get("action")
                            path = payload.get("path")
                            expanded = payload.get("expanded")
                            selected = payload.get("selected")
                            if expanded is not None:
                                st.session_state["train_adv_tree_expanded"] = expanded
                            if selected:
                                st.session_state["train_adv_tree_selected"] = selected
                            if action in {"select", "preview"} and path:
                                st.session_state["train_preview_path"] = path
                                add_recent_path(path)
                                trigger_rerun()
                            if action == "set_root" and path:
                                st.session_state["train_dataset_root"] = path
                                st.session_state["train_browse_root"] = path
                                add_recent_path(path)
                                trigger_rerun()
                    else:
                        render_icon_tree_custom(current_path, show_hidden, icon_tree_depth, icon_tree_nodes, filter_query=tree_filter)
                    st.markdown("---")
                    if st.button("æ”¶è—å½“å‰è·¯å¾„", key="train_fav_add", width='stretch'):
                        add_favorite_path(str(current_path), group=active_group)
                        trigger_rerun()
                with right_col:
                    st.caption("ç›®å½•ä¿¡æ¯")
                    selected_path = st.session_state.get("train_preview_path") or str(current_path)
                    selected_dir = Path(selected_path)
                    st.markdown(f"<div class='fm-path'>{selected_dir}</div>", unsafe_allow_html=True)
                    if selected_dir.exists():
                        subdir_count = len([p for p in selected_dir.iterdir() if p.is_dir()])
                        file_count = len([p for p in selected_dir.iterdir() if p.is_file()])
                        st.write(f"å­ç›®å½•ï¼š{subdir_count}")
                        st.write(f"æ–‡ä»¶ï¼š{file_count}")
                        stats_recursive = st.checkbox("é€’å½’ç»Ÿè®¡å¤§å°", value=False, key="train_stats_recursive")
                        stats_depth = st.slider("ç»Ÿè®¡æ·±åº¦", min_value=1, max_value=10, value=6, step=1, key="train_stats_depth")
                        stats_limit = st.number_input("ç»Ÿè®¡æ–‡ä»¶ä¸Šé™", min_value=500, max_value=50000, value=5000, step=500, key="train_stats_limit")
                        stats = get_dir_stats(
                            selected_dir,
                            recursive=stats_recursive,
                            max_files=int(stats_limit),
                            max_depth=int(stats_depth),
                        )
                        st.write(f"å¤§å°ï¼š{format_bytes(stats.get('bytes'))}")
                        st.write(f"ç»Ÿè®¡æ–‡ä»¶æ•°ï¼š{stats.get('files')} Â· ç›®å½•æ•°ï¼š{stats.get('dirs')}")
                        if stats.get("truncated"):
                            st.caption("å·²è¾¾åˆ°ç»Ÿè®¡ä¸Šé™ï¼Œç»“æœä¸ºè¿‘ä¼¼å€¼ã€‚")
                        st.caption("å¤§å°å æ¯”ï¼ˆéé€’å½’ï¼ŒæŒ‰å½“å‰å±‚ï¼‰")
                        dir_sizes, file_sizes = get_immediate_children_sizes(selected_dir, max_items=6)
                        if dir_sizes:
                            max_dir = max(size for _, size in dir_sizes) or 1
                            for name, size in dir_sizes:
                                st.write(f"{name} Â· {format_bytes(size)}")
                                st.progress(min(size / max_dir, 1.0))
                        if file_sizes:
                            max_file = max(size for _, size in file_sizes) or 1
                            for name, size in file_sizes:
                                st.write(f"{name} Â· {format_bytes(size)}")
                                st.progress(min(size / max_file, 1.0))
                    st.caption("é¢„è§ˆé€‰ä¸­ç›®å½•ç¼©ç•¥å›¾")
                    drop_target = components.html(
                        """
                        <div class="drop-zone" id="drop-zone">æ‹–æ‹½ç›®å½•åˆ°è¿™é‡Œé¢„è§ˆ</div>
                        <script>
                        const dz = document.getElementById('drop-zone');
                        dz.addEventListener('dragover', (e) => { e.preventDefault(); dz.style.background = 'rgba(59,130,246,0.12)'; });
                        dz.addEventListener('dragleave', (e) => { dz.style.background = 'rgba(59,130,246,0.05)'; });
                        dz.addEventListener('drop', (e) => {
                          e.preventDefault();
                          dz.style.background = 'rgba(59,130,246,0.05)';
                          const path = e.dataTransfer.getData('text/plain');
                          window.parent.postMessage({
                            isStreamlitMessage: true,
                            type: 'streamlit:setComponentValue',
                            value: path
                          }, '*');
                        });
                        </script>
                        """,
                        height=70,
                    )
                    if isinstance(drop_target, str) and drop_target:
                        st.session_state["train_preview_path"] = drop_target
                        if st.checkbox("æ‹–æ‹½åŒæ—¶æ›´æ–°æ ¹ç›®å½•", value=False, key="train_drop_set_root"):
                            st.session_state["train_dataset_root"] = drop_target
                            add_recent_path(drop_target)
                        trigger_rerun()

                    preview_recursive = st.checkbox("é€’å½’æœç´¢å›¾ç‰‡", value=False, key="train_preview_recursive")
                    preview_scan_limit = st.number_input("æ‰«æä¸Šé™", min_value=50, max_value=10000, value=600, step=50, key="train_preview_scan_limit")
                    preview_page_size = st.slider("æ¯é¡µæ•°é‡", min_value=4, max_value=48, value=12, step=4, key="train_preview_page_size")
                    st.caption(f"é¢„è§ˆç›®å½•ï¼š`{selected_dir}`")
                    image_files = list_image_files_for_preview(str(selected_dir), preview_recursive, int(preview_scan_limit))
                    search_name = st.text_input("æ–‡ä»¶åæœç´¢", value="", key="train_preview_search")
                    sort_by = st.selectbox("æ’åºæ–¹å¼", options=["åç§°", "ä¿®æ”¹æ—¶é—´", "å¤§å°"], index=0, key="train_preview_sort")
                    sort_order = st.selectbox("é¡ºåº", options=["å‡åº", "é™åº"], index=0, key="train_preview_order")
                    filtered = image_files
                    if search_name.strip():
                        key = search_name.strip().lower()
                        filtered = [item for item in filtered if key in Path(item["path"]).name.lower()]
                    reverse = sort_order == "é™åº"
                    if sort_by == "åç§°":
                        filtered = sorted(filtered, key=lambda x: Path(x["path"]).name.lower(), reverse=reverse)
                    elif sort_by == "ä¿®æ”¹æ—¶é—´":
                        filtered = sorted(filtered, key=lambda x: x.get("mtime", 0), reverse=reverse)
                    else:
                        filtered = sorted(filtered, key=lambda x: x.get("size", 0), reverse=reverse)
                    total_images = len(filtered)
                    total_pages = max(1, math.ceil(total_images / preview_page_size)) if total_images else 1
                    page_cols = st.columns([1, 1, 2])
                    with page_cols[0]:
                        if st.button("ä¸Šä¸€é¡µ", key="train_preview_prev", width='stretch'):
                            current_page = max(1, st.session_state.get("train_preview_page", 1) - 1)
                            st.session_state["train_preview_page"] = current_page
                            trigger_rerun()
                    with page_cols[1]:
                        if st.button("ä¸‹ä¸€é¡µ", key="train_preview_next", width='stretch'):
                            current_page = min(total_pages, st.session_state.get("train_preview_page", 1) + 1)
                            st.session_state["train_preview_page"] = current_page
                            trigger_rerun()
                    with page_cols[2]:
                        current_page = st.number_input("é¡µç ", min_value=1, max_value=total_pages, value=int(st.session_state.get("train_preview_page", 1)), step=1, key="train_preview_page_input")
                        st.session_state["train_preview_page"] = current_page

                    lazy_scroll = st.checkbox("æ»šåŠ¨åŠ è½½ä¸‹ä¸€é¡µ", value=False, key="train_preview_lazy_scroll")
                    if lazy_scroll:
                        scroll_event = components.html(
                            """
                            <script>
                            let ticking = false;
                            function onScroll() {
                              if (ticking) return;
                              ticking = true;
                              window.requestAnimationFrame(() => {
                                const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
                                const scrollHeight = document.documentElement.scrollHeight || document.body.scrollHeight;
                                const clientHeight = document.documentElement.clientHeight || document.body.clientHeight;
                                if (scrollTop + clientHeight >= scrollHeight - 120) {
                                  window.parent.postMessage({
                                    isStreamlitMessage: true,
                                    type: 'streamlit:setComponentValue',
                                    value: 'next'
                                  }, '*');
                                }
                                ticking = false;
                              });
                            }
                            window.addEventListener('scroll', onScroll, { passive: true });
                            </script>
                            """,
                            height=0,
                        )
                        if isinstance(scroll_event, str) and scroll_event == "next":
                            if current_page < total_pages:
                                st.session_state["train_preview_page"] = current_page + 1
                                trigger_rerun()

                    start = (current_page - 1) * preview_page_size
                    end = start + preview_page_size
                    page_files = filtered[start:end]
                    if total_images == 0:
                        st.info("æœªæ‰¾åˆ°å¯é¢„è§ˆå›¾ç‰‡ã€‚")
                    else:
                        st.caption(f"å·²æ‰«æ {total_images} å¼ ï¼Œæ˜¾ç¤º {start + 1}-{min(end, total_images)}")
                        view_mode = st.radio("é¢„è§ˆæ¨¡å¼", options=["ç½‘æ ¼", "åˆ—è¡¨"], index=0, key="train_preview_mode", horizontal=True)
                        if view_mode == "ç½‘æ ¼":
                            grid_mode_type = st.selectbox("åˆ—æ•°æ¨¡å¼", options=["è‡ªé€‚åº”", "æ‰‹åŠ¨"], index=0, key="train_preview_grid_mode")
                            if grid_mode_type == "è‡ªé€‚åº”":
                                cols_count = min(6, max(2, int(math.sqrt(preview_page_size))))
                            else:
                                cols_count = st.slider("åˆ—æ•°", min_value=2, max_value=8, value=3, step=1, key="train_preview_grid_cols")
                            cols = st.columns(cols_count)
                            for idx, item in enumerate(page_files):
                                with cols[idx % cols_count]:
                                    st.image(item["path"], caption=Path(item["path"]).name, width='stretch')
                        else:
                            rows = []
                            for item in page_files:
                                rows.append({
                                    "æ–‡ä»¶å": Path(item["path"]).name,
                                    "å¤§å°": format_bytes(item.get("size", 0)),
                                    "ä¿®æ”¹æ—¶é—´": datetime.fromtimestamp(item.get("mtime", 0)).strftime("%Y-%m-%d %H:%M:%S"),
                                    "è·¯å¾„": item["path"],
                                })
                            safe_dataframe(pd.DataFrame(rows), width='stretch')
                        if total_images >= int(preview_scan_limit) and st.button("åŠ è½½æ›´å¤š", key="train_preview_load_more", width='stretch'):
                            st.session_state["train_preview_scan_limit"] = int(preview_scan_limit) + 200
                            list_image_files_for_preview.clear()
                            trigger_rerun()
                st.markdown("</div>", unsafe_allow_html=True)
            if use_tree and tree_select:
                tree_depth = st.slider("æ ‘çŠ¶æ·±åº¦", min_value=1, max_value=8, value=4, step=1, key="train_tree_depth")
                tree_nodes_limit = st.slider("æœ€å¤§èŠ‚ç‚¹æ•°", min_value=200, max_value=5000, value=2000, step=200, key="train_tree_nodes")
                if st.button("åˆ·æ–°æ ‘", key="train_tree_refresh", width='stretch'):
                    build_dir_tree_nodes_cached.clear()
                    trigger_rerun()
                nodes, total_nodes = build_dir_tree_nodes_cached(browse_root, tree_depth, tree_nodes_limit, show_hidden)
                if not nodes:
                    st.caption("æ ‘çŠ¶ç›®å½•ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æµè§ˆèµ·ç‚¹ã€‚")
                else:
                    st.caption(f"å·²åŠ è½½ {total_nodes} ä¸ªç›®å½•èŠ‚ç‚¹")
                    tree_state = tree_select(
                        nodes,
                        check_model="all",
                        only_leaf_checkboxes=False,
                        no_cascade=True,
                        expand_on_click=True,
                        show_expand_all=True,
                        expanded=st.session_state.get("train_tree_expanded", []),
                        checked=st.session_state.get("train_tree_checked", []),
                    )
                    checked = tree_state.get("checked", []) if isinstance(tree_state, dict) else []
                    expanded = tree_state.get("expanded", []) if isinstance(tree_state, dict) else []
                    st.session_state["train_tree_expanded"] = expanded
                    if checked:
                        st.session_state["train_tree_checked"] = [checked[-1]]
                        selected_path = checked[-1]
                        st.caption(f"æ ‘çŠ¶é€‰æ‹©ï¼š`{selected_path}`")
                        if st.button("ä½¿ç”¨æ ‘çŠ¶è·¯å¾„", key="train_use_tree_path", width='stretch'):
                            st.session_state["train_dataset_root"] = selected_path
                            add_recent_path(selected_path)
                            trigger_rerun()
            elif use_tree and not tree_select:
                st.info("æ ‘çŠ¶ç›®å½•éœ€è¦å®‰è£… streamlit-tree-selectã€‚")
            base_path = Path(browse_root)
            if not base_path.exists() or not base_path.is_dir():
                st.caption("æµè§ˆèµ·ç‚¹ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•ã€‚")
            else:
                if st.session_state.get("train_browse_base_cache") != str(base_path):
                    st.session_state["train_browse_base_cache"] = str(base_path)
                    st.session_state["train_browse_stack"] = []
                stack = list(st.session_state.get("train_browse_stack", []))
                max_depth = st.slider("ç›®å½•å±‚çº§æ·±åº¦", min_value=1, max_value=8, value=4, step=1, key="train_browse_depth")
                current_path = str(base_path)
                for level in range(max_depth):
                    parent = Path(current_path)
                    subdirs = list_subdirectories(str(parent), include_hidden=show_hidden)
                    if not subdirs:
                        break
                    options = ["(å½“å‰)"] + [str(p) for p in subdirs]
                    current_value = stack[level] if level < len(stack) else "(å½“å‰)"
                    index = options.index(current_value) if current_value in options else 0
                    choice = st.selectbox(
                        f"ç¬¬ {level + 1} çº§",
                        options=options,
                        index=index,
                        key=f"train_browse_level_{level}",
                    )
                    if choice == "(å½“å‰)":
                        stack = stack[:level]
                        break
                    if len(stack) <= level:
                        stack.append(choice)
                    else:
                        stack[level] = choice
                        stack = stack[: level + 1]
                    current_path = choice
                st.session_state["train_browse_stack"] = stack
                selected_path = stack[-1] if stack else str(base_path)
                st.caption(f"å½“å‰é€‰æ‹©ï¼š`{selected_path}`")
                browse_cols = st.columns(3)
                with browse_cols[0]:
                    if st.button("ä½¿ç”¨å½“å‰è·¯å¾„", key="train_use_browse", width='stretch'):
                        st.session_state["train_dataset_root"] = selected_path
                        trigger_rerun()
                with browse_cols[1]:
                    if st.button("ä¸Šä¸€çº§", key="train_browse_up", width='stretch'):
                        if stack:
                            stack = stack[:-1]
                            st.session_state["train_browse_stack"] = stack
                        trigger_rerun()
                with browse_cols[2]:
                    if st.button("é‡ç½®", key="train_browse_reset", width='stretch'):
                        st.session_state["train_browse_stack"] = []
                        trigger_rerun()
        scan_yaml = st.checkbox("æ‰«æ data.yaml / dataset.yaml", value=True, key="train_scan_yaml")
        if st.button("é‡æ–°æ‰«æ", key="train_rescan", width='stretch'):
            scan_dataset_configs.clear()
            trigger_rerun()
        dataset_root_path = Path(dataset_root)
        if dataset_root_path.is_file() and dataset_root_path.suffix.lower() in {".yaml", ".yml"}:
            dataset_yaml_options = [dataset_root_path]
        else:
            dataset_yaml_options = scan_dataset_configs(dataset_root) if scan_yaml else []
        dataset_yaml_choice = None
        if dataset_yaml_options:
            dataset_yaml_choice = st.selectbox(
                "é€‰æ‹©æ•°æ®é›†é…ç½®æ–‡ä»¶",
                options=[str(p) for p in dataset_yaml_options],
                key="train_dataset_choice",
            )
        else:
            st.caption("æœªæ‰¾åˆ° data.yaml / dataset.yamlï¼Œè¯·æ£€æŸ¥ç›®å½•æˆ–æ‰‹åŠ¨è¾“å…¥è·¯å¾„ã€‚")
        manual_yaml = st.text_input("æˆ–æ‰‹åŠ¨è¾“å…¥ data.yaml è·¯å¾„", value="", key="train_dataset_manual")
        if manual_yaml.strip():
            st.info("å·²å¡«å†™æ‰‹åŠ¨è·¯å¾„ï¼Œå°†ä¼˜å…ˆä½¿ç”¨ã€‚æ¸…ç©ºåæ‰èƒ½ä½¿ç”¨ä¸‹æ‹‰é€‰æ‹©ã€‚")
            if st.button("æ¸…ç©ºæ‰‹åŠ¨è·¯å¾„", key="train_clear_manual", width='stretch'):
                st.session_state["train_dataset_manual"] = ""
                trigger_rerun()
        with st.expander("æµè§ˆ data.yaml", expanded=False):
            yaml_files = list_yaml_files(dataset_root)
            if yaml_files:
                pick_yaml = st.selectbox(
                    "é€‰æ‹© data.yaml æ–‡ä»¶",
                    options=[str(p) for p in yaml_files],
                    key="train_browse_yaml",
                )
                if st.button("ä½¿ç”¨é€‰ä¸­æ–‡ä»¶", key="train_use_yaml", width='stretch'):
                    st.session_state["train_dataset_manual"] = pick_yaml
                    trigger_rerun()
            else:
                st.caption("å½“å‰ç›®å½•æœªæ‰¾åˆ° data.yaml / dataset.yamlã€‚")
        data_yaml = manual_yaml.strip() or dataset_yaml_choice or ""

        st.markdown("---")
        st.markdown("<div class='sidebar-title'>æ¨¡å‹ä¸è¾“å‡º</div>", unsafe_allow_html=True)
        model_path = st.text_input(
            "æ¨¡å‹/æƒé‡è·¯å¾„",
            value="ultralytics/cfg/models/11/yolo11.yaml",
            key="train_model_path",
        )
        project = st.text_input(
            "è¾“å‡ºç›®å½• project",
            value=st.session_state.train_project,
            key="train_project_input",
        )
        name = st.text_input(
            "è®­ç»ƒåç§° name",
            value=st.session_state.train_name,
            key="train_name_input",
        )
        exist_ok = st.checkbox("exist_okï¼ˆè¦†ç›–åŒåç»“æœï¼‰", value=False, key="train_exist_ok")

        st.markdown("---")
        st.markdown("<div class='sidebar-title'>åŸºç¡€å‚æ•°</div>", unsafe_allow_html=True)
        epochs = st.number_input("epochs", min_value=1, max_value=5000, value=50, step=1, key="train_epochs")
        imgsz = st.number_input("imgsz", min_value=320, max_value=4096, value=640, step=32, key="train_imgsz")
        batch = st.number_input("batch", min_value=1, max_value=1024, value=16, step=1, key="train_batch")
        workers = st.number_input("workers", min_value=0, max_value=64, value=4, step=1, key="train_workers")
        device = st.text_input("deviceï¼ˆå¦‚ 0 / 0,1 / cpuï¼‰", value="0", key="train_device")
        amp = st.checkbox("AMP æ··åˆç²¾åº¦", value=True, key="train_amp")
        cache_choice = st.selectbox("cache", options=["False", "True", "ram", "disk"], index=0, key="train_cache")
        resume = st.checkbox("resumeï¼ˆæ–­ç‚¹ç»­è®­ï¼‰", value=False, key="train_resume")

        st.markdown("---")
        st.markdown("<div class='sidebar-title'>è¿›é˜¶å‚æ•°</div>", unsafe_allow_html=True)
        optimizer = st.text_input("optimizer", value="auto", key="train_optimizer")
        seed = st.number_input("seed", min_value=0, max_value=999999, value=0, step=1, key="train_seed")
        patience = st.number_input("patience", min_value=0, max_value=500, value=50, step=1, key="train_patience")
        cos_lr = st.checkbox("cos_lr", value=False, key="train_cos_lr")
        close_mosaic = st.number_input("close_mosaic", min_value=0, max_value=200, value=10, step=1, key="train_close_mosaic")
        save_period = st.number_input("save_period", min_value=-1, max_value=200, value=-1, step=1, key="train_save_period")

        st.markdown("---")
        st.markdown("<div class='sidebar-title'>é«˜çº§å‚æ•°</div>", unsafe_allow_html=True)
        advanced_text = st.text_area(
            "key=valueï¼ˆæ”¯æŒæ•°å­—/true/false/[]/{})",
            value="",
            height=140,
            key="train_advanced",
        )
        cuda_visible_devices = st.text_input(
            "CUDA_VISIBLE_DEVICESï¼ˆå¯é€‰ï¼‰",
            value="",
            key="train_cuda_visible",
        )

        st.markdown("---")
        st.markdown("<div class='sidebar-title'>è®­ç»ƒç›‘æ§</div>", unsafe_allow_html=True)
        stream_logs = st.checkbox("å®æ—¶æ—¥å¿—æµ", value=True, key="train_stream_logs")
        max_log_lines = st.number_input("æ—¥å¿—ä¿ç•™è¡Œæ•°", min_value=200, max_value=5000, value=1200, step=100, key="train_log_lines_limit")

    dataset_summary = summarize_dataset(data_yaml) if data_yaml else {"error": "æœªé€‰æ‹©æ•°æ®é›†é…ç½®"}
    cuda_info = get_cuda_summary()

    info_left, info_right = st.columns([2, 1])
    with info_left:
        st.markdown("**æ•°æ®é›†æ¦‚è§ˆ**")
        if dataset_summary.get("error"):
            st.warning(dataset_summary["error"])
        else:
            st.write(f"data.yamlï¼š`{data_yaml}`")
            st.write(f"æ•°æ®é›†æ ¹ç›®å½•ï¼š`{dataset_summary.get('path')}`")
            st.write(f"ç±»åˆ«æ•°ï¼š{dataset_summary.get('nc')}")
            names = dataset_summary.get("names")
            if names:
                st.write(f"ç±»åˆ«ï¼š{', '.join([str(n) for n in names])}")
            st.write(f"è®­ç»ƒé›†å›¾ç‰‡ï¼š{format_int_safe(dataset_summary.get('train_images'))}")
            st.write(f"éªŒè¯é›†å›¾ç‰‡ï¼š{format_int_safe(dataset_summary.get('val_images'))}")
            st.write(f"æµ‹è¯•é›†å›¾ç‰‡ï¼š{format_int_safe(dataset_summary.get('test_images'))}")

    with info_right:
        st.markdown("**ç®—åŠ›ä¿¡æ¯**")
        if cuda_info.get("available"):
            st.write(f"CUDA å¯ç”¨ï¼šæ˜¯ï¼ˆ{cuda_info.get('detail')}ï¼‰")
            for idx, name in enumerate(cuda_info.get("devices", [])):
                st.write(f"GPU {idx}: {name}")
        else:
            st.write(f"CUDA å¯ç”¨ï¼šå¦ï¼ˆ{cuda_info.get('detail')}ï¼‰")

    st.markdown("---")
    with st.expander("æ•°æ®é›†å¯è§†åŒ–æµè§ˆå™¨", expanded=False):
        if dataset_summary.get("error"):
            st.info("è¯·å…ˆé€‰æ‹©æœ‰æ•ˆçš„ data.yaml åå†æµè§ˆã€‚")
        else:
            preview_yaml = data_yaml
            category_options = build_category_preview_options(Path(dataset_root), dataset_yaml_options)
            if category_options:
                category_choice = st.selectbox(
                    "ç±»åˆ«è¿‡æ»¤å­ç›®å½•",
                    options=["(å½“å‰æ•°æ®é›†)"] + list(category_options.keys()),
                    index=0,
                    key="train_preview_category",
                )
                if category_choice != "(å½“å‰æ•°æ®é›†)":
                    preview_yaml = category_options.get(category_choice, preview_yaml)

            preview_summary = summarize_dataset(preview_yaml) if preview_yaml else {"error": "æœªé€‰æ‹© data.yaml"}
            if preview_summary.get("error"):
                st.warning(preview_summary["error"])
            else:
                st.caption(f"ä½¿ç”¨ data.yamlï¼š`{preview_yaml}`")
                split_choice = st.selectbox("é€‰æ‹© split", options=["train", "val", "test"], index=0, key="train_preview_split")
                max_preview = st.slider("ç¼©ç•¥å›¾æ•°é‡", min_value=4, max_value=64, value=16, step=4, key="train_preview_count")
                shuffle_preview = st.checkbox("éšæœºæŠ½æ ·", value=True, key="train_preview_shuffle")
                dir_map = {
                    "train": Path(preview_summary.get("train_dir", "")),
                    "val": Path(preview_summary.get("val_dir", "")),
                    "test": Path(preview_summary.get("test_dir", "")),
                }
                target_dir = dir_map.get(split_choice)
                st.caption(f"ç›®å½•ï¼š`{target_dir}`")
                images = collect_image_files(target_dir, max_images=int(max_preview), shuffle=shuffle_preview)
                if images:
                    st.image([str(p) for p in images], caption=[p.name for p in images], width='stretch')
                else:
                    st.info("è¯¥ split æœªæ‰¾åˆ°å¯é¢„è§ˆå›¾ç‰‡ã€‚")

    st.markdown("---")
    train_btn = st.button("å¼€å§‹è®­ç»ƒ", type="primary", width='stretch', disabled=bool(missing))

    if train_btn:
        if not data_yaml:
            st.error("è¯·å…ˆé€‰æ‹©æˆ–è¾“å…¥ data.yaml è·¯å¾„ã€‚")
            st.stop()
        if not Path(data_yaml).exists():
            st.error("data.yaml ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
            st.stop()

        cache_value = {"False": False, "True": True}.get(cache_choice, cache_choice)
        train_kwargs = {
            "epochs": int(epochs),
            "imgsz": int(imgsz),
            "batch": int(batch),
            "workers": int(workers),
            "device": device.strip() or None,
            "project": project.strip() or None,
            "name": name.strip() or None,
            "exist_ok": bool(exist_ok),
            "amp": bool(amp),
            "cache": cache_value,
            "resume": bool(resume),
            "optimizer": optimizer.strip() or "auto",
            "seed": int(seed),
            "patience": int(patience),
            "cos_lr": bool(cos_lr),
            "close_mosaic": int(close_mosaic),
            "save_period": int(save_period),
        }
        if train_kwargs.get("device") is None:
            train_kwargs.pop("device", None)
        if train_kwargs.get("project") is None:
            train_kwargs.pop("project", None)
        if train_kwargs.get("name") is None:
            train_kwargs.pop("name", None)

        advanced_opts, errors = parse_kv_lines(advanced_text)
        if errors:
            st.warning("é«˜çº§å‚æ•°è§£ææç¤ºï¼š" + "ï¼›".join(errors))
        if advanced_opts:
            train_kwargs.update(advanced_opts)

        env_vars = {}
        if cuda_visible_devices.strip():
            env_vars["CUDA_VISIBLE_DEVICES"] = cuda_visible_devices.strip()

        progress_bar = st.progress(0.0)
        log_placeholder = st.empty()
        status_placeholder = st.empty()
        run_stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file_name = f"{safe_filename(name.strip() if name else 'train')}_{run_stamp}.log"
        log_file_path = logs_dir / log_file_name
        st.session_state.train_log_file = str(log_file_path)

        if stream_logs:
            log_queue = queue.Queue()
            result_holder = {}
            worker = threading.Thread(
                target=run_yolo_training_stream,
                args=(model_path, data_yaml, train_kwargs, env_vars, log_queue, result_holder),
                daemon=True,
            )
            worker.start()

            log_lines = list(st.session_state.train_log_lines or [])
            max_lines = int(max_log_lines)
            total_epochs = int(train_kwargs.get("epochs", 0)) or None
            current_epoch = 0
            log_file = None
            try:
                log_file = open(log_file_path, "a", encoding="utf-8")
            except Exception as exc:
                st.warning(f"æ— æ³•å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼š{exc}")

            status_placeholder.info("è®­ç»ƒè¿›è¡Œä¸­ï¼ˆå®æ—¶æ—¥å¿—æµå·²å¼€å¯ï¼‰â€¦")
            done = False
            while not done:
                try:
                    item = log_queue.get(timeout=0.2)
                except queue.Empty:
                    item = None

                if item is LOG_DONE:
                    done = True
                elif isinstance(item, str):
                    if item.strip():
                        log_lines.append(item)
                        if len(log_lines) > max_lines:
                            log_lines = log_lines[-max_lines:]
                        st.session_state.train_log_lines = log_lines
                        st.session_state.train_logs = "\n".join(log_lines)
                        if log_file:
                            try:
                                log_file.write(item + "\n")
                                log_file.flush()
                            except Exception:
                                log_file = None
                        epoch_info = _extract_epoch_info(item)
                        if epoch_info:
                            current_epoch, total_epochs = epoch_info
                if total_epochs:
                    progress_bar.progress(min(current_epoch / total_epochs, 1.0))
                log_placeholder.text_area("è®­ç»ƒè¾“å‡ºï¼ˆå®æ—¶ï¼‰", st.session_state.train_logs, height=260)

                if item is None and not worker.is_alive():
                    done = True
            if log_file:
                log_file.close()

            save_dir = result_holder.get("save_dir")
            error = result_holder.get("error")
            st.session_state.train_last_run = str(save_dir) if save_dir else ""
            if error:
                st.error(f"è®­ç»ƒå¤±è´¥ï¼š{error}")
            else:
                st.success("è®­ç»ƒå®Œæˆï¼")
                payload = build_train_template_payload()
                try:
                    save_template_file(templates_dir / "last_success.json", payload)
                except Exception as exc:
                    st.warning(f"å†™å…¥ last_success æ¨¡æ¿å¤±è´¥ï¼š{exc}")
                collect_run_dirs.clear()
        else:
            status_placeholder.info("è®­ç»ƒè¿›è¡Œä¸­ï¼ˆå®æ—¶æ—¥å¿—æµå·²å…³é—­ï¼‰â€¦")
            with st.spinner("è®­ç»ƒä¸­ï¼Œè¯·è€å¿ƒç­‰å¾…â€¦â€¦"):
                _, logs, save_dir, error = run_yolo_training(model_path, data_yaml, train_kwargs, env_vars)
            try:
                log_file_path.write_text(logs, encoding="utf-8")
            except Exception as exc:
                st.warning(f"æ— æ³•å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼š{exc}")
            lines = logs.splitlines()
            max_lines = int(max_log_lines)
            st.session_state.train_log_lines = lines[-max_lines:] if len(lines) > max_lines else lines
            st.session_state.train_logs = "\n".join(st.session_state.train_log_lines)
            st.session_state.train_last_run = str(save_dir) if save_dir else ""
            if error:
                st.error(f"è®­ç»ƒå¤±è´¥ï¼š{error}")
            else:
                st.success("è®­ç»ƒå®Œæˆï¼")
                payload = build_train_template_payload()
                try:
                    save_template_file(templates_dir / "last_success.json", payload)
                except Exception as exc:
                    st.warning(f"å†™å…¥ last_success æ¨¡æ¿å¤±è´¥ï¼š{exc}")
                collect_run_dirs.clear()

    st.markdown("---")
    st.markdown("**è®­ç»ƒæ—¥å¿—**")
    if st.session_state.train_log_file:
        log_path = Path(st.session_state.train_log_file)
        if log_path.exists():
            st.write(f"æ—¥å¿—æ–‡ä»¶ï¼š`{log_path}`")
            st.download_button(
                "ä¸‹è½½æ—¥å¿—æ–‡ä»¶",
                data=log_path.read_bytes(),
                file_name=log_path.name,
                mime="text/plain",
            )
    if st.session_state.train_logs:
        st.text_area("è®­ç»ƒè¾“å‡º", st.session_state.train_logs, height=260)
    else:
        st.info("æš‚æ— æ—¥å¿—è¾“å‡ºã€‚")

    st.markdown("---")
    st.markdown("**è®­ç»ƒç»“æœå¯è§†åŒ–**")
    run_root = Path(project) if project else Path.cwd() / "runs"
    run_dirs = collect_run_dirs(str(run_root))
    default_run = st.session_state.train_last_run or (str(run_dirs[0]) if run_dirs else "")
    selected_run = None
    if run_dirs:
        index = 0
        if default_run:
            for idx, path in enumerate(run_dirs):
                if str(path) == str(default_run):
                    index = idx
                    break
        selected_run = st.selectbox(
            "é€‰æ‹©è®­ç»ƒç»“æœç›®å½•",
            options=[str(p) for p in run_dirs],
            index=index,
        )
    elif default_run:
        selected_run = default_run
    else:
        st.info("æœªæ‰¾åˆ°è®­ç»ƒç»“æœç›®å½•ã€‚")

    if selected_run:
        render_run_visualization(Path(selected_run))


inject_style()

BUILD_VERSION = "2026-02-06.1"
st.caption(f"Build: {BUILD_VERSION}")

mode = st.sidebar.radio("åŠŸèƒ½æ¨¡å¼", ["æ•°æ®å¤„ç†", "YOLOè®­ç»ƒå¹³å°"], index=0)
if mode == "YOLOè®­ç»ƒå¹³å°":
    render_training_platform()
    st.stop()

st.markdown("<div class='hero-title'>YOLO æ•°æ®å¤„ç†æµæ°´çº¿</div>", unsafe_allow_html=True)
st.caption("åˆå¹¶CSV â†’ æŒ‰sourceå»é‡ â†’ å‚è€ƒå»é‡ â†’ æ›¿æ¢ptList â†’ IoUç­›é€‰ â†’ æ ‡ç­¾æ›¿æ¢ â†’ å›¾ç‰‡æ ‡æ³¨")



STEP_ORDER = [
    "merge",
    "dedup",
    "ref_filter",
    "replace_ptlist",
    "iou_filter",
    "label_replace",
    "split",
    "yolo",
    "download",
]


def init_state():
    if "run_id" not in st.session_state:
        st.session_state.run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    fixed_output_root = Path.cwd() / "runs" / "latest"
    if "output_root" not in st.session_state:
        st.session_state.output_root = str(fixed_output_root)
    if "outputs" not in st.session_state:
        st.session_state.outputs = {}
    if "logs" not in st.session_state:
        st.session_state.logs = {}
    if "step_done" not in st.session_state:
        st.session_state.step_done = {}
    if "input_ready" not in st.session_state:
        st.session_state.input_ready = False
    if "config" not in st.session_state:
        st.session_state.config = {}
    if "preview_path" not in st.session_state:
        st.session_state.preview_path = None


init_state()
FIXED_OUTPUT_ROOT = Path(st.session_state.output_root)


# def save_upload(uploaded_file, dest_path: Path):
#     dest_path.parent.mkdir(parents=True, exist_ok=True)
#     with open(dest_path, "wb") as f:
#         f.write(uploaded_file.getbuffer())
#     return dest_path
# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def save_upload(uploaded_file, dest_path: Path):
    """
    ä¿®å¤ç‰ˆï¼šä¸Šä¼ æ–‡ä»¶ä¿å­˜æ–¹æ³•ï¼Œå¢åŠ å¼‚å¸¸æ•è·ã€æ ¡éªŒå’Œæ—¥å¿—
    """
    # å‰ç½®æ ¡éªŒï¼šä¸Šä¼ æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
    if uploaded_file is None:
        logger.error("ä¸Šä¼ æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
        raise ValueError("ä¸Šä¼ æ–‡ä»¶ä¸èƒ½ä¸ºç©º")

    # æ ¡éªŒæ–‡ä»¶å¤§å°
    file_size = uploaded_file.size
    if file_size == 0:
        logger.error(f"ä¸Šä¼ æ–‡ä»¶ {uploaded_file.name} ä¸ºç©ºæ–‡ä»¶ï¼ˆå¤§å°ï¼š0å­—èŠ‚ï¼‰")
        raise ValueError(f"ä¸Šä¼ æ–‡ä»¶ {uploaded_file.name} ä¸ºç©º")

    # åˆ›å»ºç›®å½•ï¼ˆå¸¦æƒé™æ£€æŸ¥ï¼‰
    try:
        dest_path.parent.mkdir(parents=True, exist_ok=True, mode=0o755)  # Linux/macOS æƒé™
    except PermissionError as e:
        logger.error(f"åˆ›å»ºç›®å½•å¤±è´¥ï¼š{dest_path.parent}ï¼Œæƒé™ä¸è¶³ï¼š{e}")
        raise PermissionError(f"æ— å†™å…¥æƒé™ï¼š{dest_path.parent}") from e

    # å†™å…¥æ–‡ä»¶ï¼ˆå¸¦æ ¡éªŒï¼‰
    try:
        with open(dest_path, "wb") as f:
            # åˆ†å—å†™å…¥å¤§æ–‡ä»¶ï¼Œé¿å…ç¼“å†²åŒºæº¢å‡º
            chunk_size = 1024 * 1024  # 1MB åˆ†å—
            buffer = uploaded_file.getbuffer()
            f.write(buffer)

        # æ ¡éªŒå†™å…¥åæ–‡ä»¶å¤§å°
        saved_size = dest_path.stat().st_size
        if saved_size != file_size:
            logger.warning(
                f"æ–‡ä»¶ {uploaded_file.name} å†™å…¥ä¸å®Œæ•´ï¼åŸå¤§å°ï¼š{file_size} å­—èŠ‚ï¼Œä¿å­˜åï¼š{saved_size} å­—èŠ‚"
            )
            raise RuntimeError(f"æ–‡ä»¶å†™å…¥ä¸å®Œæ•´ï¼Œä¸¢å¤± {file_size - saved_size} å­—èŠ‚æ•°æ®")

        logger.info(f"æ–‡ä»¶ {uploaded_file.name} ä¿å­˜æˆåŠŸï¼Œè·¯å¾„ï¼š{dest_path}ï¼Œå¤§å°ï¼š{saved_size} å­—èŠ‚")
        return dest_path

    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥ï¼š{e}", exc_info=True)
        # æ¸…ç†ä¸å®Œæ•´æ–‡ä»¶
        if dest_path.exists():
            dest_path.unlink()
        raise

def check_requirements():
    req_path = Path(__file__).resolve().parent / "requirements.txt"
    if not req_path.exists():
        return ["requirements.txt æœªæ‰¾åˆ°"]
    mapping = {
        "Pillow": "PIL",
        "opencv-python": "cv2",
        "scikit-learn": "sklearn",
    }
    missing = []
    for line in req_path.read_text(encoding="utf-8").splitlines():
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        pkg = line.split("==")[0].strip()
        module = mapping.get(pkg, pkg)
        if importlib.util.find_spec(module) is None:
            missing.append(pkg)
    return missing


def get_csv_columns(file_obj_or_path):
    try:
        if hasattr(file_obj_or_path, "getbuffer"):
            data = io.BytesIO(file_obj_or_path.getbuffer())
            df = pd.read_csv(data, nrows=1, encoding="utf-8-sig")
        else:
            df = pd.read_csv(file_obj_or_path, nrows=1, encoding="utf-8-sig")
        return list(df.columns)
    except Exception:
        return None


missing_pkgs = check_requirements()
if missing_pkgs:
    st.warning(f"ç¯å¢ƒä¾èµ–ç¼ºå¤±ï¼š{', '.join(missing_pkgs)}ã€‚è¯·å…ˆå®‰è£… requirements.txtã€‚")


def save_uploads(uploaded_files, dest_dir: Path):
    dest_dir.mkdir(parents=True, exist_ok=True)
    paths = []
    for item in uploaded_files:
        out_path = dest_dir / item.name
        save_upload(item, out_path)
        paths.append(out_path)
    return paths


def file_info_from_upload(uploaded_file):
    size = getattr(uploaded_file, "size", None)
    if size is None:
        try:
            size = len(uploaded_file.getbuffer())
        except Exception:
            size = 0
    return {
        "name": uploaded_file.name,
        "size_kb": size / 1024,
        "type": getattr(uploaded_file, "type", "æœªçŸ¥ç±»å‹") or "æœªçŸ¥ç±»å‹",
    }


def file_info_from_path(path: Path):
    try:
        size = path.stat().st_size
    except Exception:
        size = 0
    suffix = path.suffix.lower().lstrip(".")
    file_type = suffix if suffix else "æ–‡ä»¶"
    return {
        "name": path.name,
        "size_kb": size / 1024,
        "type": file_type,
    }


def render_file_tiles(title, file_infos, columns=3):
    if not file_infos:
        return
    st.markdown(f"**{title}**")
    cols = st.columns(columns)
    for idx, info in enumerate(file_infos):
        with cols[idx % columns]:
            st.markdown(
                f"""
                <div class="file-card">
                  <div class="file-name">{info['name']}</div>
                  <div class="file-meta">{info['size_kb']:.1f} KB</div>
                  <div class="file-meta">ç±»å‹ï¼š{info['type']}</div>
                </div>
                """,
                unsafe_allow_html=True,
            )


def run_step(step_key, step_name, func, *args, **kwargs):
    buffer = io.StringIO()
    busy = st.empty()
    busy.markdown(
        f"<div class='busy-indicator'>æ­£åœ¨æ‰§è¡Œï¼š{step_name} <span class='busy-dots'><span></span><span></span><span></span></span></div>",
        unsafe_allow_html=True,
    )
    with st.spinner(""):
        with redirect_stdout(buffer):
            result = func(*args, **kwargs)
    busy.empty()
    st.session_state.logs[step_key] = buffer.getvalue()
    st.success(f"{step_name} å®Œæˆ")
    return result


def show_logs(step_key, step_name):
    logs = st.session_state.logs.get(step_key)
    if logs:
        st.text_area(f"{step_name} æ—¥å¿—", logs, height=180)


def preview_csv(path: Path, label: str):
    if path and path.exists():
        st.write(f"{label}ï¼š`{path}`")
        try:
            if str(path).lower().endswith((".xlsx", ".xls")):
                df = pd.read_excel(path, nrows=200)
            else:
                df = pd.read_csv(path, nrows=200, encoding="utf-8-sig")
            safe_dataframe(df.head(200))
        except Exception as exc:
            st.warning(f"é¢„è§ˆå¤±è´¥ï¼š{exc}")


def download_file(path: Path, label: str):
    if path and path.exists():
        st.download_button(
            label=label,
            data=path.read_bytes(),
            file_name=path.name,
            mime="text/csv",
    )


def dataframe_to_excel_bytes(df: pd.DataFrame) -> bytes:
    buffer = io.BytesIO()
    with pd.ExcelWriter(buffer) as writer:
        df.to_excel(writer, index=False)
    buffer.seek(0)
    return buffer.getvalue()


def download_dataframe_excel(df: pd.DataFrame, file_name: str, label: str, key: str = None):
    if df is None:
        return
    data = dataframe_to_excel_bytes(df)
    st.download_button(
        label=label,
        data=data,
        file_name=file_name,
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        key=key,
    )


def reset_downstream(from_step):
    if from_step not in STEP_ORDER:
        return
    from_index = STEP_ORDER.index(from_step)
    downstream = STEP_ORDER[from_index + 1:]
    for step in downstream:
        st.session_state.step_done.pop(step, None)
        st.session_state.logs.pop(step, None)
    if from_step in ["merge", "dedup"]:
        st.session_state.logs.pop("update_ref", None)
    for key in [
        "dedup",
        "filtered",
        "processed",
        "processed_excluded",
        "high_iou",
        "other",
        "label_replaced",
        "label_replace_summary",
        "label_replace_diff",
        "label_replace_unmatched",
        "label_replace_sample_diff",
        "split_dir",
        "split_counts",
        "unclassified",
        "unclassified_summary",
        "category_files",
        "yolo_dir",
        "yolo_datasets",
        "yolo_skipped",
        "yolo_stats",
        "yolo_progress",
        "download_dir",
        "annotated_dir",
    ]:
        if from_step == "merge":
            st.session_state.outputs.pop(key, None)
        if from_step == "dedup" and key in ["filtered", "processed", "processed_excluded", "high_iou", "other", "download_dir", "annotated_dir", "split_dir", "split_counts", "unclassified", "unclassified_summary", "category_files", "yolo_dir", "yolo_datasets", "yolo_skipped"]:
            st.session_state.outputs.pop(key, None)
        if from_step == "ref_filter" and key in ["processed", "processed_excluded", "high_iou", "other", "split_dir", "split_counts", "unclassified", "unclassified_summary", "category_files", "download_dir", "annotated_dir"]:
            st.session_state.outputs.pop(key, None)
        if from_step == "replace_ptlist" and key in ["high_iou", "other", "label_replaced", "label_replace_summary", "label_replace_diff", "label_replace_unmatched", "label_replace_sample_diff", "split_dir", "split_counts", "unclassified", "unclassified_summary", "category_files", "download_dir", "annotated_dir"]:
            st.session_state.outputs.pop(key, None)
        if from_step == "iou_filter" and key in ["label_replaced", "label_replace_summary", "label_replace_diff", "label_replace_unmatched", "label_replace_sample_diff", "split_dir", "split_counts", "unclassified", "unclassified_summary", "category_files", "download_dir", "annotated_dir"]:
            st.session_state.outputs.pop(key, None)
        if from_step == "label_replace" and key in ["split_dir", "split_counts", "unclassified", "unclassified_summary", "category_files", "download_dir", "annotated_dir"]:
            st.session_state.outputs.pop(key, None)
        if from_step == "split" and key in ["yolo_dir", "yolo_datasets", "yolo_skipped", "yolo_stats", "yolo_progress", "download_dir", "annotated_dir"]:
            st.session_state.outputs.pop(key, None)
        if from_step == "yolo" and key in ["download_dir", "annotated_dir", "yolo_stats", "yolo_progress"]:
            st.session_state.outputs.pop(key, None)


def step_status_chip(step_key, label):
    if st.session_state.step_done.get(step_key):
        chip_class = "chip-done"
        status = "å·²å®Œæˆ"
    else:
        chip_class = "chip-wait"
        status = "å¾…æ‰§è¡Œ"
    return f"<span class=\"chip {chip_class}\">{label} Â· {status}</span>"


def build_steps(config):
    label_enabled = bool(st.session_state.outputs.get("label_map_path"))
    return [
        ("merge", "åˆå¹¶CSV", False, True),
        ("dedup", "æŒ‰sourceå»é‡", False, True),
        ("ref_filter", "å‚è€ƒCSVå»é‡", True, config.get("use_reference")),
        ("replace_ptlist", "æ›¿æ¢ptList", False, True),
        ("iou_filter", "IoUç­›é€‰", False, True),
        ("label_replace", "æ ‡ç­¾æ›¿æ¢", True, label_enabled),
        ("split", "è§„åˆ™åˆ†ç±»æ‹†åˆ†", False, True),
        ("yolo", "ç”ŸæˆYOLOæ•°æ®é›†", False, True),
        ("download", "ä¸‹è½½å¹¶ç»˜åˆ¶æ ‡æ³¨", True, config.get("run_download")),
    ]


def render_stepper(config):
    steps = build_steps(config)
    html = "<div class='stepper'>"
    ready = True
    for idx, (key, label, optional, enabled) in enumerate(steps):
        if optional and not enabled:
            status = "skipped"
        elif st.session_state.step_done.get(key):
            status = "done"
        else:
            status = "active" if ready else "locked"
            ready = False
        if status == "done":
            ready = True
        if status == "skipped":
            ready = True
        html += (
            "<div class='step'>"
            f"<div class='step-circle {status}'>{idx + 1}</div>"
            f"<div class='step-label'>{label}</div>"
            "</div>"
        )
        if idx < len(steps) - 1:
            if status == "done":
                line_class = "line-done"
            elif status == "skipped":
                line_class = "line-skip"
            else:
                line_class = "line-lock"
            html += f"<div class='step-line {line_class}'></div>"
    html += "</div>"
    st.markdown(html, unsafe_allow_html=True)


def render_dependency_graph(config):
    steps = build_steps(config)
    width = 160 * len(steps) + 40
    height = 120
    rect_w = 130
    rect_h = 40
    start_x = 20
    y = 30
    parts = [
        f"<svg class='dependency-card' viewBox='0 0 {width} {height}' width='100%' height='120' xmlns='http://www.w3.org/2000/svg'>",
        "<defs>",
        "<marker id='arrow' markerWidth='10' markerHeight='10' refX='9' refY='3' orient='auto'>",
        "<path d='M0,0 L10,3 L0,6 Z' fill='#94a3b8'></path>",
        "</marker>",
        "</defs>",
    ]

    for idx, (_, label, optional, enabled) in enumerate(steps):
        x = start_x + idx * 160
        stroke = "#2563eb" if enabled or not optional else "#cbd5f5"
        fill = "#ffffff" if enabled or not optional else "#f8fafc"
        dash = "" if enabled or not optional else "stroke-dasharray='4 4'"
        parts.append(
            f"<rect x='{x}' y='{y}' width='{rect_w}' height='{rect_h}' rx='10' fill='{fill}' stroke='{stroke}' {dash} />"
        )
        parts.append(
            f"<text x='{x + rect_w / 2}' y='{y + 24}' text-anchor='middle' font-size='12' fill='#0f172a'>{label}</text>"
        )
        if idx < len(steps) - 1:
            next_optional = steps[idx + 1][2]
            next_enabled = steps[idx + 1][3]
            dashed = (optional and not enabled) or (next_optional and not next_enabled)
            line_color = "#94a3b8" if dashed else "#2563eb"
            dash_attr = "stroke-dasharray='4 4'" if dashed else ""
            x1 = x + rect_w
            x2 = start_x + (idx + 1) * 160
            parts.append(
                f"<line x1='{x1}' y1='{y + rect_h / 2}' x2='{x2}' y2='{y + rect_h / 2}' stroke='{line_color}' stroke-width='2' marker-end='url(#arrow)' {dash_attr} />"
            )

    parts.append("</svg>")
    st.markdown("\n".join(parts), unsafe_allow_html=True)


def compute_progress(config):
    active = ["merge", "dedup", "replace_ptlist", "iou_filter", "split", "yolo"]
    if config.get("use_reference"):
        active.insert(2, "ref_filter")
    if st.session_state.outputs.get("label_map_path"):
        active.insert(active.index("split"), "label_replace")
    if config.get("run_download"):
        active.append("download")
    done = sum(1 for s in active if st.session_state.step_done.get(s))
    total = len(active) if active else 1
    return done, total


@st.cache_data(show_spinner=False)
def get_row_count_cached(path_str, mtime):
    try:
        path_lower = str(path_str).lower()
        if path_lower.endswith((".xlsx", ".xls")):
            df = pd.read_excel(path_str)
            return len(df)
        if path_lower.endswith(".csv"):
            # Fast, low-memory line count to avoid reading large CSVs into pandas.
            line_count = 0
            with open(path_str, "r", encoding="utf-8-sig", errors="ignore") as f:
                for _ in f:
                    line_count += 1
            return max(line_count - 1, 0)
        df = pd.read_csv(path_str, encoding="utf-8-sig")
        return len(df)
    except Exception:
        return None


def get_row_count(path):
    if not path:
        return None
    p = Path(path)
    if not p.exists():
        return None
    try:
        return get_row_count_cached(str(p), p.stat().st_mtime)
    except Exception:
        return None


def get_image_count(path):
    if not path:
        return None
    p = Path(path)
    if not p.exists():
        return None
    try:
        return len([f for f in p.iterdir() if f.is_file()])
    except Exception:
        return None


def summarize_yolo_label_counts(dataset_dirs):
    """ç»Ÿè®¡æ¯ä¸ªYOLOæ•°æ®é›†ä¸­ train/val/test å„æ ‡ç­¾â€œå›¾ç‰‡æ•°é‡/æ ‡æ³¨æ¡†æ•°é‡/å æ¯”â€ï¼Œå¹¶æä¾›æ±‡æ€»ã€‚"""
    stats = {}
    flat_rows = []
    for dataset_dir in dataset_dirs or []:
        if not dataset_dir:
            continue
        dataset_path = Path(dataset_dir)
        if not dataset_path.exists():
            continue
        names = []
        data_yaml = dataset_path / "data.yaml"
        if data_yaml.exists():
            try:
                data = yaml.safe_load(data_yaml.read_text(encoding="utf-8"))
                names = data.get("names") or []
            except Exception:
                names = []
        dataset_key = dataset_path.name
        split_stats = {}
        total_images_all = 0
        total_img_counts = {}
        total_box_counts = {}
        for split in ["train", "val", "test"]:
            label_dir = dataset_path / "labels" / split
            img_counts = {}
            box_counts = {}
            total_images = 0
            if label_dir.exists():
                for txt_path in label_dir.glob("*.txt"):
                    total_images += 1
                    try:
                        lines = txt_path.read_text(encoding="utf-8", errors="ignore").splitlines()
                    except Exception:
                        continue
                    labels_in_image = set()
                    for line in lines:
                        parts = line.strip().split()
                        if not parts:
                            continue
                        try:
                            class_id = int(float(parts[0]))
                        except Exception:
                            continue
                        label_name = names[class_id] if class_id < len(names) else str(class_id)
                        labels_in_image.add(label_name)
                        box_counts[label_name] = box_counts.get(label_name, 0) + 1
                    for label in labels_in_image:
                        img_counts[label] = img_counts.get(label, 0) + 1
            split_stats[split] = {
                "total_images": total_images,
                "label_counts": img_counts,
                "box_counts": box_counts,
            }
            total_images_all += total_images
            for label, count in img_counts.items():
                total_img_counts[label] = total_img_counts.get(label, 0) + count
            for label, count in box_counts.items():
                total_box_counts[label] = total_box_counts.get(label, 0) + count

            all_labels = set(img_counts) | set(box_counts)
            for label in all_labels:
                img_count = img_counts.get(label, 0)
                box_count = box_counts.get(label, 0)
                ratio = (img_count / total_images) if total_images else 0.0
                flat_rows.append({
                    "æ•°æ®é›†": dataset_key,
                    "split": split,
                    "æ ‡ç­¾": label,
                    "å›¾ç‰‡æ•°é‡": img_count,
                    "æ ‡æ³¨æ¡†æ•°é‡": box_count,
                    "å æ¯”%": f"{ratio * 100:.1f}%",
                    "splitæ€»å›¾ç‰‡æ•°": total_images,
                })

        split_stats["all"] = {
            "total_images": total_images_all,
            "label_counts": total_img_counts,
            "box_counts": total_box_counts,
        }
        all_labels = set(total_img_counts) | set(total_box_counts)
        for label in all_labels:
            img_count = total_img_counts.get(label, 0)
            box_count = total_box_counts.get(label, 0)
            ratio = (img_count / total_images_all) if total_images_all else 0.0
            flat_rows.append({
                "æ•°æ®é›†": dataset_key,
                "split": "all",
                "æ ‡ç­¾": label,
                "å›¾ç‰‡æ•°é‡": img_count,
                "æ ‡æ³¨æ¡†æ•°é‡": box_count,
                "å æ¯”%": f"{ratio * 100:.1f}%",
                "splitæ€»å›¾ç‰‡æ•°": total_images_all,
            })

        stats[dataset_key] = split_stats
    df = pd.DataFrame(flat_rows)
    return stats, df


def format_int(value):
    return "-" if value is None else f"{value:,}"


def format_ratio(numerator, denominator):
    if numerator is None or denominator in (None, 0):
        return "-"
    return f"{(numerator / denominator) * 100:.1f}%"


def format_duration(seconds: float) -> str:
    if seconds is None or seconds < 0:
        return "-"
    seconds = int(seconds)
    hours = seconds // 3600
    minutes = (seconds % 3600) // 60
    secs = seconds % 60
    if hours > 0:
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"
    return f"{minutes:02d}:{secs:02d}"


def render_merge_eta_card(elapsed, eta, speed, total_bytes, read_bytes, file_idx, total_files):
    progress = min(read_bytes / total_bytes, 1.0) if total_bytes else 0.0
    html = f"""
    <div class="glow-frame">
      <div class="glow-inner">
        <div class="kpi">åˆå¹¶è¿›åº¦</div>
        <div style="font-size:1.1rem;font-weight:700;">{progress*100:.1f}% Â· {file_idx}/{total_files} æ–‡ä»¶</div>
        <div class="kpi" style="margin-top:8px;">é€Ÿåº¦ / å‰©ä½™</div>
        <div style="font-size:0.95rem;">{format_bytes(int(speed))}/s Â· é¢„è®¡å‰©ä½™ {format_duration(eta)}</div>
        <div class="kpi" style="margin-top:8px;">å·²ç”¨æ—¶</div>
        <div style="font-size:0.95rem;">{format_duration(elapsed)}</div>
      </div>
    </div>
    """
    return html


def render_stats_cards(items):
    if not items:
        return
    cards = ""
    for label, value, hint in items:
        hint_html = f"<div class='stat-hint'>{hint}</div>" if hint else ""
        cards += (
            "<div class='stat-card'>"
            f"<div class='stat-label'>{label}</div>"
            f"<div class='stat-value'>{value}</div>"
            f"{hint_html}"
            "</div>"
        )
    st.markdown(f"<div class='stat-grid'>{cards}</div>", unsafe_allow_html=True)


def collect_counts(outputs):
    return {
        "merged": get_row_count(outputs.get("merged")),
        "dedup": get_row_count(outputs.get("dedup")),
        "filtered": get_row_count(outputs.get("filtered")),
        "processed": get_row_count(outputs.get("processed")),
        "processed_excluded": get_row_count(outputs.get("processed_excluded")),
        "high_iou": get_row_count(outputs.get("high_iou")),
        "other": get_row_count(outputs.get("other")),
        "label_replaced": get_row_count(outputs.get("label_replaced")),
        "unclassified": get_row_count(outputs.get("unclassified")),
        "unclassified_summary": get_row_count(outputs.get("unclassified_summary")),
        "split_counts": get_row_count(outputs.get("split_counts")),
    }


def get_summary_metrics(counts):
    total = counts.get("merged")
    processed = counts.get("processed")
    high_iou = counts.get("high_iou")
    other = counts.get("other")
    final_total = None
    if high_iou is not None and other is not None:
        final_total = high_iou + other
    final_retention = format_ratio(final_total, total)
    hit_rate = format_ratio(high_iou, processed)
    return [
        ("æœ€ç»ˆè¾“å‡ºè¡Œæ•°", format_int(final_total), "é«˜IoU + å…¶ä»–"),
        ("æœ€ç»ˆä¿ç•™ç‡", final_retention, "æœ€ç»ˆè¾“å‡º/åˆå¹¶ç»“æœ"),
        ("é«˜IoUå‘½ä¸­ç‡", hit_rate, "é«˜IoU/ptListæ›¿æ¢ç»“æœ"),
    ]


def list_excel_files(folder_path):
    if not folder_path:
        return []
    folder = Path(folder_path)
    if not folder.exists():
        return []
    files = list(folder.glob("*.xlsx")) + list(folder.glob("*.xls"))
    return sorted(files)


def build_export_zip(outputs, include_images=False, only_classification=False):
    buffer = io.BytesIO()
    with zipfile.ZipFile(buffer, "w", zipfile.ZIP_DEFLATED) as zf:
        if not only_classification:
            csv_keys = [
                "merged",
                "dedup",
                "filtered",
                "processed",
                "high_iou",
                "other",
                "label_replaced",
                "label_replace_diff",
                "label_replace_unmatched",
                "unclassified",
                "unclassified_summary",
                "split_counts",
            ]
            for key in csv_keys:
                path = outputs.get(key)
                if path and Path(path).exists():
                    zf.write(path, arcname=f"csv/{Path(path).name}")
        else:
            path = outputs.get("unclassified")
            if path and Path(path).exists():
                zf.write(path, arcname=f"categories/{Path(path).name}")
            path = outputs.get("unclassified_summary")
            if path and Path(path).exists():
                zf.write(path, arcname=f"categories/{Path(path).name}")
            path = outputs.get("split_counts")
            if path and Path(path).exists():
                zf.write(path, arcname=f"categories/{Path(path).name}")

        category_files = outputs.get("category_files") or []
        for path in category_files:
            if path and Path(path).exists():
                zf.write(path, arcname=f"categories/{Path(path).name}")

        if include_images:
            annotated_dir = outputs.get("annotated_dir")
            download_dir = outputs.get("download_dir")
            for folder, prefix in [(download_dir, "images/downloaded"), (annotated_dir, "images/annotated")]:
                if folder and Path(folder).exists():
                    for file_path in Path(folder).glob("*"):
                        if file_path.is_file():
                            zf.write(file_path, arcname=f"{prefix}/{file_path.name}")

    buffer.seek(0)
    return buffer


def build_yolo_zip(yolo_dir):
    if not yolo_dir:
        return None
    yolo_dir = Path(yolo_dir)
    if not yolo_dir.exists():
        return None
    buffer = io.BytesIO()
    with zipfile.ZipFile(buffer, "w", zipfile.ZIP_DEFLATED) as zf:
        for path in yolo_dir.rglob("*"):
            if path.is_file():
                zf.write(path, arcname=str(path.relative_to(yolo_dir)))
    buffer.seek(0)
    return buffer


def ensure_empty_reference_csv(path_str, template_csv_path=None):
    if not path_str:
        return False, "å‚è€ƒCSVè·¯å¾„ä¸ºç©º"
    path = Path(path_str)
    if path.exists():
        return True, None
    try:
        path.parent.mkdir(parents=True, exist_ok=True)
        columns = ["source"]
        if template_csv_path and Path(template_csv_path).exists():
            try:
                columns = list(pd.read_csv(template_csv_path, nrows=0, encoding="utf-8-sig").columns)
            except Exception:
                columns = ["source"]
        pd.DataFrame(columns=columns).to_csv(path, index=False, encoding="utf-8-sig")
        return True, f"å·²è‡ªåŠ¨åˆ›å»ºç©ºå‚è€ƒæ–‡ä»¶ï¼ˆç»§æ‰¿ä¸»CSVåˆ—ï¼‰ï¼š{path}"
    except Exception as exc:
        return False, f"è‡ªåŠ¨åˆ›å»ºå‚è€ƒæ–‡ä»¶å¤±è´¥ï¼š{exc}"


def render_output_preview(outputs):
    preview_items = [
        ("åˆå¹¶ç»“æœ", outputs.get("merged")),
        ("å»é‡ç»“æœ", outputs.get("dedup")),
        ("å‚è€ƒå»é‡ç»“æœ", outputs.get("filtered")),
        ("ptListæ›¿æ¢ç»“æœ", outputs.get("processed")),
        ("ptListæœªç­›é€‰", outputs.get("processed_excluded")),
        ("é«˜IoUç»“æœ", outputs.get("high_iou")),
        ("å…¶ä»–æ•°æ®", outputs.get("other")),
        ("æ ‡ç­¾æ›¿æ¢ç»“æœ", outputs.get("label_replaced")),
        ("æ ‡ç­¾æ›¿æ¢å·®å¼‚", outputs.get("label_replace_diff")),
        ("æ ‡ç­¾æ›¿æ¢æœªåŒ¹é…", outputs.get("label_replace_unmatched")),
        ("æ— æ³•åˆ†ç±»æ•°æ®", outputs.get("unclassified")),
        ("æ— æ³•åˆ†ç±»æ±‡æ€»", outputs.get("unclassified_summary")),
        ("æ‹†åˆ†æ¡æ•°ç»Ÿè®¡", outputs.get("split_counts")),
        ("YOLOè·³è¿‡æ¸…å•", outputs.get("yolo_skipped")),
    ]
    available = [(label, path) for label, path in preview_items if path and Path(path).exists()]
    if not available:
        st.info("æš‚æ— å¯é¢„è§ˆçš„è¾“å‡ºæ–‡ä»¶ã€‚")
        return

    st.markdown("**è¾“å‡ºé¢„è§ˆï¼ˆç‚¹å‡»æŸ¥çœ‹ï¼‰**")
    cols = st.columns(2)
    with cols[0]:
        for label, path in available[: (len(available) + 1) // 2]:
            if st.button(f"é¢„è§ˆ {label}", key=f"preview_{label}"):
                st.session_state.preview_path = path
    with cols[1]:
        for label, path in available[(len(available) + 1) // 2:]:
            if st.button(f"é¢„è§ˆ {label}", key=f"preview_{label}"):
                st.session_state.preview_path = path

    if st.session_state.preview_path:
        preview_csv(Path(st.session_state.preview_path), "å½“å‰é¢„è§ˆ")


existing_input_csvs = []
ref_fallback_path = None
rule_fallback_path = None
label_map_fallback_path = None

with st.sidebar:
    st.markdown("<div class='sidebar-title'>é…ç½®ä¸­å¿ƒ</div>", unsafe_allow_html=True)
    st.caption("è¾“å‡ºç›®å½•ï¼ˆå›ºå®šï¼Œè¦†ç›–æ—§ç»“æœï¼‰")
    st.code(str(FIXED_OUTPUT_ROOT))

    uploaded_csvs = st.file_uploader(
        "ä¸Šä¼ å¾…å¤„ç†CSVï¼ˆæ”¯æŒå¤šæ–‡ä»¶ï¼‰",
        type=["csv"],
        accept_multiple_files=True,
    )
    input_dir_default = FIXED_OUTPUT_ROOT / "input_csvs"
    if not uploaded_csvs and input_dir_default.exists():
        existing_input_csvs = sorted(input_dir_default.glob("*.csv"))

    if uploaded_csvs:
        render_file_tiles("å·²ä¸Šä¼ ä¸»CSV", [file_info_from_upload(f) for f in uploaded_csvs], columns=4)
    elif existing_input_csvs:
        render_file_tiles("å·²ä¿å­˜ä¸»CSV", [file_info_from_path(p) for p in existing_input_csvs], columns=4)
        st.caption("æœªé‡æ–°ä¸Šä¼ ï¼Œé»˜è®¤ä½¿ç”¨å·²ä¿å­˜çš„ä¸»CSVæ–‡ä»¶ã€‚")
        if st.button("åˆ é™¤å†å²ä¸»CSV", key="clear_saved_csvs", width='stretch'):
            st.session_state["confirm_clear_saved_csvs"] = True
        if st.session_state.get("confirm_clear_saved_csvs"):
            keep_files = []
            for name in ["reference.csv", "classification_rules.xlsx", "label_mapping.xlsx"]:
                if (FIXED_OUTPUT_ROOT / name).exists():
                    keep_files.append(name)
            def _do_clear_csvs():
                try:
                    for p in existing_input_csvs:
                        try:
                            p.unlink()
                        except Exception:
                            pass
                    try:
                        input_dir_default.rmdir()
                    except Exception:
                        pass
                    clear_output_root(FIXED_OUTPUT_ROOT, keep_inputs=False, keep_files=keep_files)
                    if st.session_state.outputs.get("input_dir") and Path(st.session_state.outputs.get("input_dir")) == input_dir_default:
                        st.session_state.input_ready = False
                        st.session_state.outputs["uploaded_info"] = []
                        st.session_state.outputs["input_dir"] = str(input_dir_default)
                        st.session_state.step_done = {}
                        st.session_state.logs = {}
                    st.success("å·²åˆ é™¤å†å²ä¸Šä¼ ä¸»CSVï¼Œå¹¶æ¸…ç†ç›¸å…³è¾“å‡ºã€‚")
                except Exception as exc:
                    st.error(f"åˆ é™¤å†å²ä¸»CSVå¤±è´¥ï¼š{exc}")
            show_confirm_dialog(
                "confirm_clear_saved_csvs",
                "ç¡®è®¤åˆ é™¤å†å²ä¸»CSV",
                "å°†åˆ é™¤å·²ä¿å­˜çš„ä¸»CSVæ–‡ä»¶ï¼Œå¹¶æ¸…ç† runs/latest ä¸‹çš„ç›¸å…³è¾“å‡ºã€‚æ­¤æ“ä½œä¸å¯æ¢å¤ã€‚",
                _do_clear_csvs,
            )

    sample_csv = None
    if uploaded_csvs:
        sample_csv = uploaded_csvs[0]
    elif existing_input_csvs:
        sample_csv = existing_input_csvs[0]
    if sample_csv:
        cols = get_csv_columns(sample_csv)
        if cols is not None:
            required_cols = ["source", "æ˜¯å¦åºŸå¼ƒ", "ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®"]
            missing_cols = [c for c in required_cols if c not in cols]
            if missing_cols:
                st.warning(f"ä¸»CSVç¼ºå°‘å¿…è¦åˆ—ï¼š{', '.join(missing_cols)}")
        else:
            st.info("ä¸»CSVåˆ—è¯»å–å¤±è´¥ï¼Œè¯·ç¡®è®¤æ–‡ä»¶ç¼–ç æˆ–æ ¼å¼ã€‚")

    use_reference = st.checkbox("å¯ç”¨å‚è€ƒCSVå»é‡", value=True)
    ref_mode = st.radio("å‚è€ƒCSVæ¥æº", ["ä¸Šä¼ å‚è€ƒCSV", "ä½¿ç”¨å·²æœ‰è·¯å¾„"], horizontal=True)
    ref_path = None
    ref_uploaded = None

    if ref_mode == "ä¸Šä¼ å‚è€ƒCSV":
        ref_uploaded = st.file_uploader("ä¸Šä¼ å‚è€ƒCSV", type=["csv"], key="ref_csv")
        if ref_uploaded:
            render_file_tiles("å·²ä¸Šä¼ å‚è€ƒCSV", [file_info_from_upload(ref_uploaded)])
        else:
            candidate = FIXED_OUTPUT_ROOT / "reference.csv"
            if candidate.exists():
                ref_fallback_path = candidate
                render_file_tiles("å·²ä¿å­˜å‚è€ƒCSV", [file_info_from_path(candidate)])
                st.caption(f"æœªé‡æ–°ä¸Šä¼ ï¼Œé»˜è®¤ä½¿ç”¨ï¼š{candidate.name}")
                if st.button("åˆ é™¤å‚è€ƒCSV", key="clear_ref_csv", width='stretch'):
                    st.session_state["confirm_clear_ref"] = True
                if st.session_state.get("confirm_clear_ref"):
                    keep_files = []
                    for name in ["classification_rules.xlsx", "label_mapping.xlsx"]:
                        if (FIXED_OUTPUT_ROOT / name).exists():
                            keep_files.append(name)
                    def _do_clear_ref():
                        try:
                            candidate.unlink(missing_ok=True)
                            clear_output_root(FIXED_OUTPUT_ROOT, keep_inputs=True, keep_files=keep_files)
                            st.session_state.outputs["ref_path"] = None
                            st.session_state.outputs["ref_info"] = []
                            st.session_state.input_ready = False
                            st.session_state.step_done = {}
                            st.session_state.logs = {}
                            st.success("å·²åˆ é™¤å‚è€ƒCSVï¼Œå¹¶æ¸…ç†ç›¸å…³è¾“å‡ºã€‚")
                        except Exception as exc:
                            st.error(f"åˆ é™¤å‚è€ƒCSVå¤±è´¥ï¼š{exc}")
                    show_confirm_dialog(
                        "confirm_clear_ref",
                        "ç¡®è®¤åˆ é™¤å‚è€ƒCSV",
                        "å°†åˆ é™¤å‚è€ƒCSVæ–‡ä»¶ï¼Œå¹¶æ¸…ç† runs/latest ä¸‹çš„ç›¸å…³è¾“å‡ºã€‚æ­¤æ“ä½œä¸å¯æ¢å¤ã€‚",
                        _do_clear_ref,
                    )
        if ref_uploaded and use_reference:
            ref_cols = get_csv_columns(ref_uploaded)
            if ref_cols is not None and "source" not in ref_cols:
                st.warning("å‚è€ƒCSVç¼ºå°‘ source åˆ—ï¼Œå»é‡å¯èƒ½å¤±è´¥ã€‚")
    else:
        ref_path = st.text_input("å‚è€ƒCSVè·¯å¾„", value=str(Path.cwd() / "reference.csv"))
        if use_reference and ref_path and not Path(ref_path).exists():
            st.info("å‚è€ƒCSVè·¯å¾„ä¸å­˜åœ¨ï¼Œå°†åœ¨ç¡®è®¤è¾“å…¥æ—¶è‡ªåŠ¨åˆ›å»ºç©ºæ–‡ä»¶ã€‚")
        if use_reference and ref_path and Path(ref_path).exists():
            ref_cols = get_csv_columns(ref_path)
            if ref_cols is not None and "source" not in ref_cols:
                st.warning("å‚è€ƒCSVç¼ºå°‘ source åˆ—ï¼Œå»é‡å¯èƒ½å¤±è´¥ã€‚")

    st.markdown("---")
    st.markdown("<div class='sidebar-title'>åˆå¹¶è®¾ç½®</div>", unsafe_allow_html=True)
    merge_chunk_size = st.number_input("åˆå¹¶åˆ†å—è¡Œæ•°", min_value=1000, max_value=500000, value=100000, step=1000)
    keep_outputs = st.checkbox("ä¿ç•™æ—§è¾“å‡ºç”¨äºè·³è¿‡", value=True)

    st.markdown("---")
    st.markdown("<div class='sidebar-title'>å¤„ç†å‚æ•°</div>", unsafe_allow_html=True)
    min_boxes = st.number_input("æœ€å°æ ‡æ³¨æ¡†æ•°é‡", min_value=1, max_value=50, value=2, step=1)
    iou_threshold = st.number_input("IoUé˜ˆå€¼", min_value=0.0, max_value=1.0, value=0.98, step=0.01)
    update_reference = st.checkbox("è¦†ç›–æ›´æ–°reference.csv", value=False)
    backup_reference = st.checkbox("æ›´æ–°æ—¶å¤‡ä»½reference.csv", value=True)

    st.markdown("---")
    st.markdown("<div class='sidebar-title'>åˆ†ç±»è§„åˆ™</div>", unsafe_allow_html=True)
    rule_source = st.radio("è§„åˆ™æ¥æº", ["ä¸Šä¼ è§„åˆ™Excel", "æŒ‡å®šæ–‡ä»¶å¤¹"], horizontal=True)
    rule_upload = None
    rule_folder = None
    rule_file_path = None

    if rule_source == "ä¸Šä¼ è§„åˆ™Excel":
        rule_upload = st.file_uploader("ä¸Šä¼ åˆ†ç±»è§„åˆ™Excel", type=["xlsx", "xls"], key="rule_excel")
        if rule_upload:
            st.caption(f"å·²é€‰æ‹©ï¼š{rule_upload.name}")
        else:
            candidate = FIXED_OUTPUT_ROOT / "classification_rules.xlsx"
            if candidate.exists():
                rule_fallback_path = candidate
                st.caption(f"æœªé‡æ–°ä¸Šä¼ ï¼Œé»˜è®¤ä½¿ç”¨ï¼š{candidate.name}")
                if st.button("åˆ é™¤è§„åˆ™æ–‡ä»¶", key="clear_rule_excel", width='stretch'):
                    st.session_state["confirm_clear_rule"] = True
                if st.session_state.get("confirm_clear_rule"):
                    keep_files = []
                    for name in ["reference.csv", "label_mapping.xlsx"]:
                        if (FIXED_OUTPUT_ROOT / name).exists():
                            keep_files.append(name)
                    def _do_clear_rule():
                        try:
                            candidate.unlink(missing_ok=True)
                            clear_output_root(FIXED_OUTPUT_ROOT, keep_inputs=True, keep_files=keep_files)
                            st.session_state.outputs["rule_path"] = None
                            st.session_state.input_ready = False
                            st.session_state.step_done = {}
                            st.session_state.logs = {}
                            st.success("å·²åˆ é™¤è§„åˆ™æ–‡ä»¶ï¼Œå¹¶æ¸…ç†ç›¸å…³è¾“å‡ºã€‚")
                        except Exception as exc:
                            st.error(f"åˆ é™¤è§„åˆ™æ–‡ä»¶å¤±è´¥ï¼š{exc}")
                    show_confirm_dialog(
                        "confirm_clear_rule",
                        "ç¡®è®¤åˆ é™¤è§„åˆ™æ–‡ä»¶",
                        "å°†åˆ é™¤åˆ†ç±»è§„åˆ™æ–‡ä»¶ï¼Œå¹¶æ¸…ç† runs/latest ä¸‹çš„ç›¸å…³è¾“å‡ºã€‚æ­¤æ“ä½œä¸å¯æ¢å¤ã€‚",
                        _do_clear_rule,
                    )
    else:
        rule_folder = st.text_input("è§„åˆ™æ–‡ä»¶å¤¹è·¯å¾„", value=str(Path.cwd()))
        excel_files = list_excel_files(rule_folder)
        if not excel_files:
            st.info("è¯¥ç›®å½•æœªæ‰¾åˆ°Excelæ–‡ä»¶ã€‚")
        else:
            rule_file_path = st.selectbox(
                "é€‰æ‹©è§„åˆ™æ–‡ä»¶",
                options=[str(p) for p in excel_files],
                format_func=lambda x: Path(x).name,
            )

    rule_mode = st.radio("è§£ææ–¹å¼", ["å®½è¡¨(ç±»åˆ«ä¸ºåˆ—)", "ä¸¤åˆ—æ˜ å°„"], horizontal=True)
    st.caption("å®½è¡¨æ¨¡å¼ï¼šæ¯åˆ—æ˜¯å¤§ç±»ï¼Œæ¯æ ¼æ˜¯å­æ ‡ç­¾ï¼›æ”¯æŒé€—å·/é¡¿å·/åˆ†å·/æ¢è¡Œåˆ†éš”ï¼Œæ ‡ç­¾ä¸­çš„â€œ|â€ä¸ä¼šæ‹†åˆ†ã€‚")
    rule_sheet = None
    rule_label_col = None
    rule_category_col = None
    rule_columns = []

    if rule_source == "ä¸Šä¼ è§„åˆ™Excel":
        rules_source_obj = rule_upload if rule_upload else rule_fallback_path
    else:
        rules_source_obj = rule_file_path if rule_file_path else None
    if rules_source_obj:
        try:
            excel_file = pd.ExcelFile(rules_source_obj)
            rule_sheet = st.selectbox("è§„åˆ™Sheet", options=excel_file.sheet_names)
            preview_df = excel_file.parse(rule_sheet, nrows=5)
            rule_columns = list(preview_df.columns)
            if rule_mode == "ä¸¤åˆ—æ˜ å°„" and rule_columns:
                rule_label_col = st.selectbox("æ ‡ç­¾åˆ—", options=rule_columns, index=0)
                rule_category_col = st.selectbox(
                    "ç±»åˆ«åˆ—",
                    options=rule_columns,
                    index=1 if len(rule_columns) > 1 else 0,
                )
            with st.expander("è§„åˆ™é¢„è§ˆï¼ˆå‰50è¡Œï¼‰", expanded=False):
                preview_full = excel_file.parse(rule_sheet, nrows=50)
                safe_dataframe(preview_full, width='stretch')
        except Exception as exc:
            st.warning(f"è§„åˆ™æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{exc}")
    train_ratio = st.number_input("è®­ç»ƒé›†æ¯”ä¾‹", min_value=0.0, max_value=1.0, value=0.8, step=0.05)
    val_ratio = st.number_input("éªŒè¯é›†æ¯”ä¾‹", min_value=0.0, max_value=1.0, value=0.1, step=0.05)
    test_ratio = st.number_input("æµ‹è¯•é›†æ¯”ä¾‹", min_value=0.0, max_value=1.0, value=0.1, step=0.05)
    random_seed = st.number_input("æ‹†åˆ†éšæœºç§å­", min_value=0, max_value=9999, value=42, step=1)
    ratio_sum = train_ratio + val_ratio + test_ratio
    if ratio_sum <= 0:
        st.warning("è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ¯”ä¾‹ä¹‹å’Œå¿…é¡»å¤§äº0ã€‚")
    elif abs(ratio_sum - 1.0) > 0.01:
        st.info("æ¯”ä¾‹ä¹‹å’Œä¸ä¸º1ï¼Œå°†åœ¨æ‰§è¡Œæ—¶è‡ªåŠ¨å½’ä¸€åŒ–ã€‚")

    st.markdown("---")
    st.markdown("<div class='sidebar-title'>æ ‡ç­¾æ›¿æ¢</div>", unsafe_allow_html=True)
    label_map_upload = st.file_uploader("ä¸Šä¼ æ–°æ—§æ ‡ç­¾å¯¹ç…§è¡¨Excel", type=["xlsx", "xls"], key="label_map_excel")
    label_map_sheet = None
    label_map_old_col = None
    label_map_new_col = None
    if label_map_upload is None:
        candidate = FIXED_OUTPUT_ROOT / "label_mapping.xlsx"
        if candidate.exists():
            label_map_fallback_path = candidate
            st.caption(f"æœªé‡æ–°ä¸Šä¼ ï¼Œé»˜è®¤ä½¿ç”¨ï¼š{candidate.name}")
            if st.button("åˆ é™¤æ ‡ç­¾å¯¹ç…§è¡¨", key="clear_label_map", width='stretch'):
                st.session_state["confirm_clear_label"] = True
            if st.session_state.get("confirm_clear_label"):
                keep_files = []
                for name in ["reference.csv", "classification_rules.xlsx"]:
                    if (FIXED_OUTPUT_ROOT / name).exists():
                        keep_files.append(name)
                def _do_clear_label():
                    try:
                        candidate.unlink(missing_ok=True)
                        clear_output_root(FIXED_OUTPUT_ROOT, keep_inputs=True, keep_files=keep_files)
                        st.session_state.outputs["label_map_path"] = None
                        st.session_state.input_ready = False
                        st.session_state.step_done = {}
                        st.session_state.logs = {}
                        st.success("å·²åˆ é™¤æ ‡ç­¾å¯¹ç…§è¡¨ï¼Œå¹¶æ¸…ç†ç›¸å…³è¾“å‡ºã€‚")
                    except Exception as exc:
                        st.error(f"åˆ é™¤æ ‡ç­¾å¯¹ç…§è¡¨å¤±è´¥ï¼š{exc}")
                show_confirm_dialog(
                    "confirm_clear_label",
                    "ç¡®è®¤åˆ é™¤æ ‡ç­¾å¯¹ç…§è¡¨",
                    "å°†åˆ é™¤æ ‡ç­¾å¯¹ç…§è¡¨æ–‡ä»¶ï¼Œå¹¶æ¸…ç† runs/latest ä¸‹çš„ç›¸å…³è¾“å‡ºã€‚æ­¤æ“ä½œä¸å¯æ¢å¤ã€‚",
                    _do_clear_label,
                )
    label_map_source_obj = label_map_upload if label_map_upload else label_map_fallback_path
    if label_map_source_obj:
        try:
            label_excel = pd.ExcelFile(label_map_source_obj)
            label_map_sheet = st.selectbox("å¯¹ç…§è¡¨Sheet", options=label_excel.sheet_names, key="label_map_sheet")
            preview_df = label_excel.parse(label_map_sheet, nrows=5)
            map_columns = list(preview_df.columns)
            if map_columns:
                label_map_old_col = st.selectbox("æ—§æ ‡ç­¾åˆ—", options=map_columns, index=0, key="label_map_old_col")
                label_map_new_col = st.selectbox(
                    "æ–°æ ‡ç­¾åˆ—",
                    options=map_columns,
                    index=1 if len(map_columns) > 1 else 0,
                    key="label_map_new_col",
                )
            with st.expander("å¯¹ç…§è¡¨é¢„è§ˆï¼ˆå‰50è¡Œï¼‰", expanded=False):
                preview_full = label_excel.parse(label_map_sheet, nrows=50)
                safe_dataframe(preview_full, width='stretch')
        except Exception as exc:
            st.warning(f"æ ‡ç­¾å¯¹ç…§è¡¨è¯»å–å¤±è´¥ï¼š{exc}")

    st.markdown("---")
    st.markdown("<div class='sidebar-title'>æ ‡æ³¨è¾“å‡º</div>", unsafe_allow_html=True)
    run_download = st.checkbox("ä¸‹è½½å¹¶ç»˜åˆ¶æ ‡æ³¨å›¾ç‰‡", value=False)
    max_images = st.number_input("æœ€å¤šå¤„ç†å›¾ç‰‡æ•°ï¼ˆ0è¡¨ç¤ºä¸é™ï¼‰", min_value=0, max_value=100000, value=0, step=10)
    max_images = None if max_images == 0 else int(max_images)

    st.markdown("---")
    st.markdown("<div class='sidebar-title'>YOLOè®¾ç½®</div>", unsafe_allow_html=True)
    class_order_text = st.text_area("YOLOç±»é¡ºåºï¼ˆæ¯è¡Œä¸€ä¸ªæ ‡ç­¾ï¼‰", value="", height=120)
    st.caption("ç•™ç©ºåˆ™æŒ‰ç±»åˆ«å†…æ ‡ç­¾å­—æ¯åºï¼›å¡«å†™åä¼šä¼˜å…ˆæŒ‰æ­¤é¡ºåºç”Ÿæˆ class idã€‚")

    confirm_input = st.button("ç¡®è®¤è¾“å…¥å¹¶ä¿å­˜", width='stretch')

if confirm_input:
    has_uploaded_csvs = bool(uploaded_csvs)
    has_existing_csvs = bool(existing_input_csvs)
    if not has_uploaded_csvs and not has_existing_csvs:
        st.error("è¯·å…ˆä¸Šä¼ è‡³å°‘ä¸€ä¸ªCSVæ–‡ä»¶ï¼Œæˆ–ä¿ç•™è¾“å‡ºç›®å½•ä¸­å·²ä¿å­˜çš„è¾“å…¥CSVã€‚")
    elif use_reference and ref_mode == "ä¸Šä¼ å‚è€ƒCSV" and ref_uploaded is None and ref_fallback_path is None:
        st.error("å·²å¯ç”¨å‚è€ƒCSVå»é‡ï¼Œè¯·ä¸Šä¼ å‚è€ƒCSVæˆ–ä¿ç•™å·²æœ‰ reference.csvã€‚")
    elif use_reference and ref_mode == "ä½¿ç”¨å·²æœ‰è·¯å¾„" and not ref_path:
        st.error("å‚è€ƒCSVè·¯å¾„ä¸ºç©ºï¼Œè¯·æä¾›æœ‰æ•ˆè·¯å¾„æˆ–æ”¹ä¸ºä¸Šä¼ å‚è€ƒCSVã€‚")
    else:
        st.session_state.output_root = str(FIXED_OUTPUT_ROOT)
        output_root_path = FIXED_OUTPUT_ROOT
        if output_root_path.exists() and not keep_outputs:
            try:
                shutil.rmtree(output_root_path)
            except Exception as exc:
                st.error(f"æ¸…ç†æ—§è¾“å‡ºå¤±è´¥ï¼š{exc}")
                st.stop()
        output_root_path.mkdir(parents=True, exist_ok=True)
        input_dir = output_root_path / "input_csvs"
        if has_uploaded_csvs:
            if input_dir.exists():
                try:
                    shutil.rmtree(input_dir)
                except Exception as exc:
                    st.error(f"æ¸…ç†æ—§è¾“å…¥å¤±è´¥ï¼š{exc}")
                    st.stop()
            saved_csvs = save_uploads(uploaded_csvs, input_dir)
            st.success(f"å·²ä¿å­˜ {len(saved_csvs)} ä¸ªCSVåˆ°ï¼š{input_dir}")
        else:
            input_dir.mkdir(parents=True, exist_ok=True)
            saved_csvs = existing_input_csvs
            st.success(f"ä½¿ç”¨å·²ä¿å­˜ {len(saved_csvs)} ä¸ªCSVï¼š{input_dir}")

        ref_path_value = None
        if use_reference:
            if ref_mode == "ä¸Šä¼ å‚è€ƒCSV":
                if ref_uploaded:
                    ref_path_value = output_root_path / "reference.csv"
                    save_upload(ref_uploaded, ref_path_value)
                else:
                    ref_path_value = ref_fallback_path
            else:
                ref_path_value = Path(ref_path)
                template_csv = str(saved_csvs[0]) if saved_csvs else None
                ok, msg = ensure_empty_reference_csv(str(ref_path_value), template_csv)
                if not ok:
                    st.error(msg)
                    st.stop()
                if msg:
                    st.success(msg)
        st.session_state.outputs["ref_path"] = ref_path_value
        st.session_state.outputs["input_dir"] = input_dir
        if has_uploaded_csvs:
            st.session_state.outputs["uploaded_info"] = [file_info_from_upload(f) for f in uploaded_csvs]
        else:
            st.session_state.outputs["uploaded_info"] = [file_info_from_path(p) for p in existing_input_csvs]
        if ref_uploaded:
            st.session_state.outputs["ref_info"] = [file_info_from_upload(ref_uploaded)]
        elif ref_path_value and Path(ref_path_value).exists():
            st.session_state.outputs["ref_info"] = [file_info_from_path(Path(ref_path_value))]
        else:
            st.session_state.outputs["ref_info"] = []
        label_map_path_value = None
        if label_map_upload is not None:
            label_map_path_value = output_root_path / "label_mapping.xlsx"
            save_upload(label_map_upload, label_map_path_value)
        elif label_map_fallback_path:
            label_map_path_value = label_map_fallback_path
        st.session_state.outputs["label_map_path"] = label_map_path_value
        rule_path_value = None
        if rule_source == "ä¸Šä¼ è§„åˆ™Excel" and rule_upload is not None:
            rule_path_value = output_root_path / "classification_rules.xlsx"
            save_upload(rule_upload, rule_path_value)
        elif rule_source == "ä¸Šä¼ è§„åˆ™Excel" and rule_fallback_path:
            rule_path_value = rule_fallback_path
        elif rule_source == "æŒ‡å®šæ–‡ä»¶å¤¹" and rule_file_path:
            rule_path_value = Path(rule_file_path)
        st.session_state.outputs["rule_path"] = rule_path_value
        st.session_state.input_ready = True
        st.session_state.step_done = {}
        st.session_state.logs = {}
        st.session_state.config = {
            "use_reference": use_reference,
            "update_reference": update_reference,
            "backup_reference": backup_reference,
            "merge_chunk_size": int(merge_chunk_size),
            "keep_outputs": bool(keep_outputs),
            "min_boxes": int(min_boxes),
            "iou_threshold": float(iou_threshold),
            "run_download": run_download,
            "max_images": max_images,
            "ref_mode": ref_mode,
            "rule_mode": rule_mode,
            "rule_sheet": rule_sheet,
            "rule_label_col": rule_label_col,
            "rule_category_col": rule_category_col,
            "label_map_sheet": label_map_sheet,
            "label_map_old_col": label_map_old_col,
            "label_map_new_col": label_map_new_col,
            "train_ratio": float(train_ratio),
            "val_ratio": float(val_ratio),
            "test_ratio": float(test_ratio),
            "random_seed": int(random_seed),
            "class_order": [line.strip() for line in class_order_text.splitlines() if line.strip()],
        }

if not st.session_state.input_ready:
    st.info("è¯·åœ¨å·¦ä¾§é…ç½®åŒºå®Œæˆè¾“å…¥å¹¶ç‚¹å‡»â€œç¡®è®¤è¾“å…¥å¹¶ä¿å­˜â€ã€‚")
    st.stop()
else:
    st.caption("å¦‚éœ€ä¿®æ”¹è¾“å…¥æˆ–å‚æ•°ï¼Œè¯·é‡æ–°ç‚¹å‡»â€œç¡®è®¤è¾“å…¥å¹¶ä¿å­˜â€ã€‚")

output_root_path = Path(st.session_state.output_root)
config = st.session_state.config
counts = collect_counts(st.session_state.outputs)

st.markdown("---")

st.markdown("**è¿è¡Œæ¦‚è§ˆ**")
summary_left, summary_right = st.columns([2, 1])
with summary_left:
    st.markdown(
        f"""
        <div class="glow-frame">
          <div class="glow-inner">
            <div class="kpi">è¿è¡ŒID</div>
            <div style="font-size:1.1rem;font-weight:700;">{st.session_state.run_id}</div>
            <div class="kpi" style="margin-top:8px;">è¾“å‡ºç›®å½•</div>
            <div style="font-size:0.9rem;">{output_root_path}</div>
          </div>
        </div>
        """,
        unsafe_allow_html=True,
    )
with summary_right:
    done, total = compute_progress(config)
    st.markdown(
        f"""
        <div class="glow-frame">
          <div class="glow-inner">
            <div class="kpi">æµç¨‹è¿›åº¦</div>
            <div style="font-size:1.1rem;font-weight:700;">{done} / {total}</div>
          </div>
        </div>
        """,
        unsafe_allow_html=True,
    )
    st.progress(done / total)

st.markdown("**æ­¥éª¤è¿›åº¦æ¡**")
render_stepper(config)

st.markdown("**æµç¨‹ä¾èµ–å›¾**")
render_dependency_graph(config)

st.markdown("**ç»“æœæŒ‡æ ‡æ€»è§ˆ**")
render_stats_cards(get_summary_metrics(counts))

render_output_preview(st.session_state.outputs)

if st.session_state.outputs.get("uploaded_info"):
    st.markdown("**è¾“å…¥æ–‡ä»¶**")
    render_file_tiles("ä¸»CSV", st.session_state.outputs.get("uploaded_info", []), columns=4)
    if config.get("use_reference"):
        render_file_tiles("å‚è€ƒCSV", st.session_state.outputs.get("ref_info", []), columns=4)

st.markdown("**åˆ†ç±»è§„åˆ™ Excel é¢„è§ˆ**")
rule_path = st.session_state.outputs.get("rule_path")
if rule_path and Path(rule_path).exists():
    try:
        rule_sheet = config.get("rule_sheet")
        preview_rules = pd.read_excel(rule_path, sheet_name=rule_sheet, nrows=200) if rule_sheet else pd.read_excel(rule_path, nrows=200)
        safe_dataframe(preview_rules)
    except Exception as exc:
        st.warning(f"è§„åˆ™é¢„è§ˆå¤±è´¥ï¼š{exc}")
else:
    st.info("å°šæœªé€‰æ‹©åˆ†ç±»è§„åˆ™æ–‡ä»¶ã€‚")

st.markdown("---")


with st.expander("Step 1 åˆå¹¶CSV", expanded=True):
    st.markdown(step_status_chip("merge", "åˆå¹¶CSV"), unsafe_allow_html=True)
    input_dir_dbg = st.session_state.outputs.get("input_dir")
    input_dir_path_dbg = Path(input_dir_dbg) if input_dir_dbg else None
    input_csv_count_dbg = None
    if input_dir_path_dbg and input_dir_path_dbg.exists():
        input_csv_count_dbg = len(list(input_dir_path_dbg.glob("*.csv")))
    st.caption(
        f"è°ƒè¯•æç¤ºï¼šinput_ready = {st.session_state.input_ready} | "
        f"input_dir = {input_dir_dbg or '-'} | "
        f"csvæ–‡ä»¶æ•° = {input_csv_count_dbg if input_csv_count_dbg is not None else '-'}"
    )
    merge_btn = st.button(
        "ç¡®è®¤å¹¶æ‰§è¡Œ Step 1",
        disabled=not st.session_state.input_ready,
        key="run_merge",
        width='stretch',
    )
    if merge_btn:
        st.info("Step 1 å·²è¿›å…¥æ‰§è¡Œåˆ†æ”¯")
        reset_downstream("merge")
        merged_csv = output_root_path / "merged_result.csv"
        input_dir = Path(st.session_state.outputs["input_dir"])
        input_files = sorted(input_dir.glob("*.csv"))
        log_dir = output_root_path / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        log_path = log_dir / f"merge_{st.session_state.run_id}.log"
        st.session_state.outputs["merge_log"] = log_path
        try:
            with open(log_path, "a", encoding="utf-8") as f:
                f.write(f"==== åˆå¹¶å¼€å§‹ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ====\n")
                f.write("æ ¡éªŒï¼šå·²ç‚¹å‡» Step 1 å¹¶è¿›å…¥æ‰§è¡Œåˆ†æ”¯ã€‚\n")
                f.write(f"input_dir: {input_dir}\n")
                f.write(f"input_csv_count: {len(input_files)}\n")
                if input_files:
                    preview_names = [p.name for p in input_files[:20]]
                    f.write(f"input_csv_preview: {', '.join(preview_names)}\n")
            st.info(f"åˆå¹¶å·²å¯åŠ¨ï¼Œæ—¥å¿—è·¯å¾„ï¼š`{log_path}`")
        except Exception:
            pass

        skip_merge = False
        if merged_csv.exists() and input_files:
            latest_input = max(f.stat().st_mtime for f in input_files)
            if merged_csv.stat().st_mtime >= latest_input:
                st.info("æ£€æµ‹åˆ° merged_result.csv å·²æ˜¯æœ€æ–°ï¼Œå·²è·³è¿‡åˆå¹¶ã€‚")
                st.session_state.outputs["merged"] = merged_csv
                st.session_state.step_done["merge"] = True
                st.session_state.logs["merge"] = "å¿«é€Ÿè·³è¿‡ï¼šmerged_result.csv å·²æ˜¯æœ€æ–°ã€‚"
                counts["merged"] = get_row_count(merged_csv)
                skip_merge = True
                try:
                    with open(log_path, "a", encoding="utf-8") as f:
                        f.write("å¿«é€Ÿè·³è¿‡ï¼šmerged_result.csv å·²æ˜¯æœ€æ–°ã€‚\n")
                except Exception:
                    pass

        if not skip_merge:
            progress_bar = st.progress(0.0)
            eta_card = st.empty()
            log_box = st.empty()
            heartbeat_box = st.empty()
            log_lines = []
            start_time = time.time()
            merge_state = {
                "rows": 0,
                "bytes": 0,
                "bytes_read": 0,
                "file_idx": 0,
                "files": len(input_files),
            }
            last_heartbeat = {"ts": 0.0}
            log_file = open(log_path, "a", encoding="utf-8")
            ui_tick = {"ts": 0.0}

            def _progress_cb(file_idx, total_files, filename, total_rows, file_rows, chunk_idx, file_size, file_bytes, total_bytes, total_bytes_read):
                if total_files:
                    progress_bar.progress(min(file_idx / total_files, 1.0))
                elapsed = max(time.time() - start_time, 0.001)
                speed = total_bytes_read / elapsed
                eta = (total_bytes - total_bytes_read) / speed if speed > 0 else None
                file_pct = (file_bytes / file_size * 100) if file_size else 0
                message = (
                    f"å¤„ç†ç¬¬ {file_idx}/{total_files} ä¸ªæ–‡ä»¶ï¼š{filename} | "
                    f"å½“å‰æ–‡ä»¶ {file_rows} è¡Œ | å·²åˆå¹¶ {total_rows} è¡Œ\\n"
                    f"å½“å‰æ–‡ä»¶å¤§å° {format_bytes(file_size)} | è¯»å– {file_pct:.1f}% | "
                    f"é€Ÿåº¦ {format_bytes(int(speed))}/s | é¢„è®¡å‰©ä½™ {format_duration(eta)}"
                )
                log_lines.append(message)
                if len(log_lines) > 200:
                    log_lines[:] = log_lines[-200:]
                try:
                    log_file.write(message + "\\n\\n")
                    log_file.flush()
                except Exception:
                    pass
                if time.time() - ui_tick["ts"] >= 0.3 or chunk_idx == 1:
                    log_box.text_area("åˆå¹¶æ—¥å¿—ï¼ˆå®æ—¶ï¼‰", "\\n\\n".join(log_lines), height=220)
                    ui_tick["ts"] = time.time()
                if time.time() - last_heartbeat["ts"] >= 2:
                    heartbeat_box.caption(f"å¿ƒè·³ï¼šä»åœ¨åˆå¹¶ä¸­â€¦ {datetime.now().strftime('%H:%M:%S')}")
                    last_heartbeat["ts"] = time.time()
                if time.time() - ui_tick["ts"] >= 0.3 or chunk_idx == 1:
                    eta_card.markdown(
                        render_merge_eta_card(
                            elapsed,
                            eta,
                            speed,
                            total_bytes,
                            total_bytes_read,
                            file_idx,
                            total_files,
                        ),
                        unsafe_allow_html=True,
                    )
                merge_state["rows"] = total_rows
                merge_state["bytes"] = total_bytes
                merge_state["bytes_read"] = total_bytes_read
                merge_state["file_idx"] = file_idx
                merge_state["files"] = total_files

            try:
                with st.spinner("åˆå¹¶ä¸­ï¼Œè¯·è€å¿ƒç­‰å¾…â€¦"):
                    merge_all_csv_in_folder(
                        str(input_dir),
                        str(merged_csv),
                        "utf-8-sig",
                        int(config.get("merge_chunk_size", 100000)),
                        _progress_cb,
                    )
            finally:
                try:
                    log_file.close()
                except Exception:
                    pass

            st.session_state.outputs["merged"] = merged_csv
            st.session_state.step_done["merge"] = True
            st.session_state.logs["merge"] = "\n".join(log_lines)
            counts["merged"] = get_row_count(merged_csv)
            st.success("åˆå¹¶CSV å®Œæˆ")

            elapsed_total = max(time.time() - start_time, 0.001)
            avg_speed = merge_state["bytes_read"] / elapsed_total if elapsed_total else 0
            summary = {
                "files": merge_state["files"],
                "rows": merge_state["rows"],
                "bytes": merge_state["bytes"],
                "elapsed": elapsed_total,
                "avg_speed": avg_speed,
            }
            st.session_state.outputs["merge_summary"] = summary

            log_dir = output_root_path / "logs"
            log_dir.mkdir(parents=True, exist_ok=True)
            log_path = log_dir / f"merge_{st.session_state.run_id}.log"
            summary_lines = [
                "==== åˆå¹¶æ€»ç»“ ====",
                f"æ–‡ä»¶æ•°: {summary['files']}",
                f"æ€»è¡Œæ•°: {summary['rows']}",
                f"æ€»å¤§å°: {format_bytes(int(summary['bytes']))}",
                f"è€—æ—¶: {format_duration(summary['elapsed'])}",
                f"å¹³å‡é€Ÿåº¦: {format_bytes(int(summary['avg_speed']))}/s",
            ]
            with open(log_path, "a", encoding="utf-8") as f:
                f.write("\n")
                f.write("\n".join(summary_lines))
                f.write("\n")
            st.session_state.outputs["merge_log"] = log_path

    render_stats_cards([
        ("è¾“å…¥æ–‡ä»¶", format_int(len(st.session_state.outputs.get("uploaded_info", []))), "ä¸»CSVæ•°é‡"),
        ("åˆå¹¶è¡Œæ•°", format_int(counts.get("merged")), "è¾“å‡ºè¡Œæ•°"),
    ])

    merge_summary = st.session_state.outputs.get("merge_summary")
    if merge_summary:
        render_stats_cards([
            ("åˆå¹¶è€—æ—¶", format_duration(merge_summary.get("elapsed")), "æ€»è€—æ—¶"),
            ("åˆå¹¶æ€»å¤§å°", format_bytes(int(merge_summary.get("bytes") or 0)), "è¾“å…¥CSVæ€»å¤§å°"),
            ("å¹³å‡é€Ÿåº¦", f"{format_bytes(int(merge_summary.get('avg_speed') or 0))}/s", "å¹³å‡åå"),
        ])
        merge_log = st.session_state.outputs.get("merge_log")
        if merge_log and Path(merge_log).exists():
            st.download_button(
                label="ä¸‹è½½åˆå¹¶æ—¥å¿—",
                data=Path(merge_log).read_bytes(),
                file_name=Path(merge_log).name,
                mime="text/plain",
                width='stretch',
            )

    show_logs("merge", "åˆå¹¶CSV")
    preview_csv(st.session_state.outputs.get("merged"), "åˆå¹¶ç»“æœ")
    download_file(st.session_state.outputs.get("merged"), "ä¸‹è½½ merged_result.csv")

with st.expander("Step 2 æŒ‰sourceå»é‡", expanded=False):
    st.markdown(step_status_chip("dedup", "æŒ‰sourceå»é‡"), unsafe_allow_html=True)
    dedup_btn = st.button(
        "ç¡®è®¤å¹¶æ‰§è¡Œ Step 2",
        disabled=not st.session_state.step_done.get("merge"),
        key="run_dedup",
        width='stretch',
    )
    if dedup_btn:
        reset_downstream("dedup")
        dedup_csv = output_root_path / "deduplicate_result.csv"
        run_step("dedup", "æŒ‰sourceå»é‡", deduplicate_csv_by_source, str(st.session_state.outputs["merged"]), str(dedup_csv))
        st.session_state.outputs["dedup"] = dedup_csv
        st.session_state.step_done["dedup"] = True
        counts["dedup"] = get_row_count(dedup_csv)
        if not config.get("use_reference"):
            st.session_state.outputs["filtered"] = dedup_csv
            st.session_state.step_done["ref_filter"] = True

    removed = None
    if counts.get("merged") is not None and counts.get("dedup") is not None:
        removed = counts["merged"] - counts["dedup"]

    render_stats_cards([
        ("è¾“å…¥è¡Œæ•°", format_int(counts.get("merged")), "åˆå¹¶ç»“æœ"),
        ("å»é‡åè¡Œæ•°", format_int(counts.get("dedup")), "å»é‡è¾“å‡º"),
        ("å»é™¤é‡å¤", format_int(removed), "å‡å°‘è¡Œæ•°"),
        ("ä¿ç•™ç‡", format_ratio(counts.get("dedup"), counts.get("merged")), "å»é‡å/å»é‡å‰"),
    ])

    show_logs("dedup", "æŒ‰sourceå»é‡")
    preview_csv(st.session_state.outputs.get("dedup"), "å»é‡ç»“æœ")
    download_file(st.session_state.outputs.get("dedup"), "ä¸‹è½½ deduplicate_result.csv")

with st.expander("Step 3 å‚è€ƒCSVå»é‡", expanded=False):
    if config.get("use_reference"):
        st.markdown(step_status_chip("ref_filter", "å‚è€ƒCSVå»é‡"), unsafe_allow_html=True)
        ref_exists = True
        ref_path_value = st.session_state.outputs.get("ref_path")
        if config.get("ref_mode") == "ä½¿ç”¨å·²æœ‰è·¯å¾„":
            ref_exists = bool(ref_path_value) and Path(ref_path_value).exists()
            if not ref_exists:
                st.warning("å‚è€ƒCSVè·¯å¾„ä¸å­˜åœ¨ï¼Œè¯·å›åˆ°å·¦ä¾§é…ç½®åŒºä¿®æ­£åé‡æ–°ç¡®è®¤è¾“å…¥ã€‚")
        ref_btn = st.button(
            "ç¡®è®¤å¹¶æ‰§è¡Œ Step 3",
            disabled=not st.session_state.step_done.get("dedup") or not ref_exists,
            key="run_ref",
            width='stretch',
        )
        if ref_btn:
            reset_downstream("ref_filter")
            filtered_csv = output_root_path / "filtered_main.csv"
            run_step(
                "ref_filter",
                "å‚è€ƒCSVå»é‡",
                remove_duplicates_between_csv,
                str(st.session_state.outputs["dedup"]),
                str(st.session_state.outputs["ref_path"]),
                str(filtered_csv),
            )
            st.session_state.outputs["filtered"] = filtered_csv
            st.session_state.step_done["ref_filter"] = True
            counts["filtered"] = get_row_count(filtered_csv)

        removed = None
        if counts.get("dedup") is not None and counts.get("filtered") is not None:
            removed = counts["dedup"] - counts["filtered"]

        render_stats_cards([
            ("è¾“å…¥è¡Œæ•°", format_int(counts.get("dedup")), "å»é‡ç»“æœ"),
            ("è¿‡æ»¤åè¡Œæ•°", format_int(counts.get("filtered")), "å‚è€ƒå»é‡è¾“å‡º"),
            ("å‰”é™¤è¡Œæ•°", format_int(removed), "ä¸å‚è€ƒé›†é‡å¤"),
            ("ä¿ç•™ç‡", format_ratio(counts.get("filtered"), counts.get("dedup")), "è¿‡æ»¤å/è¿‡æ»¤å‰"),
        ])

        show_logs("ref_filter", "å‚è€ƒCSVå»é‡")
        preview_csv(st.session_state.outputs.get("filtered"), "å‚è€ƒå»é‡ç»“æœ")
        download_file(st.session_state.outputs.get("filtered"), "ä¸‹è½½ filtered_main.csv")

        if config.get("update_reference"):
            st.markdown("**å¯é€‰ï¼šè¦†ç›–æ›´æ–°reference.csv**")
            update_btn = st.button(
                "ç¡®è®¤å¹¶è¦†ç›– reference.csv",
                disabled=not st.session_state.step_done.get("dedup"),
                key="run_update_ref",
                width='stretch',
            )
            if update_btn:
                run_step(
                    "update_ref",
                    "è¦†ç›–æ›´æ–°reference.csv",
                    overwrite_reference_with_result,
                    str(st.session_state.outputs["dedup"]),
                    str(st.session_state.outputs["ref_path"]),
                    "utf-8-sig",
                    config.get("backup_reference", True),
                    True,
                )
            show_logs("update_ref", "è¦†ç›–æ›´æ–°reference.csv")
    else:
        st.markdown("<span class=\"chip chip-skip\">å‚è€ƒCSVå»é‡ Â· å·²è·³è¿‡</span>", unsafe_allow_html=True)
        st.info("å·²å…³é—­å‚è€ƒCSVå»é‡ï¼ŒStep 3 è‡ªåŠ¨è·³è¿‡ã€‚")

with st.expander("Step 4 æ›¿æ¢ptList", expanded=False):
    st.markdown(step_status_chip("replace_ptlist", "æ›¿æ¢ptList"), unsafe_allow_html=True)
    replace_btn = st.button(
        "ç¡®è®¤å¹¶æ‰§è¡Œ Step 4",
        disabled=not st.session_state.step_done.get("ref_filter"),
        key="run_replace",
        width='stretch',
    )
    if replace_btn:
        reset_downstream("replace_ptlist")
        processed_csv = output_root_path / "processed_replaced_ptlist.csv"
        excluded_csv = output_root_path / "processed_replaced_ptlist_excluded.csv"
        run_step(
            "replace_ptlist",
            "æ›¿æ¢ptList",
            process_csv_replace_ptlist,
            str(st.session_state.outputs["filtered"]),
            str(processed_csv),
            str(excluded_csv),
        )
        st.session_state.outputs["processed"] = processed_csv
        st.session_state.outputs["processed_excluded"] = excluded_csv
        st.session_state.step_done["replace_ptlist"] = True
        counts["processed"] = get_row_count(processed_csv)
        counts["processed_excluded"] = get_row_count(excluded_csv)

    render_stats_cards([
        ("è¾“å…¥è¡Œæ•°", format_int(counts.get("filtered")), "å‚è€ƒå»é‡ç»“æœ"),
        ("è¾“å‡ºè¡Œæ•°", format_int(counts.get("processed")), "ptListæ›¿æ¢ç»“æœ"),
        ("æœªç­›é€‰è¡Œæ•°", format_int(counts.get("processed_excluded")), "æœªç­›é€‰æ•°æ®"),
        ("ä¿ç•™ç‡", format_ratio(counts.get("processed"), counts.get("filtered")), "è¾“å‡º/è¾“å…¥"),
    ])

    show_logs("replace_ptlist", "æ›¿æ¢ptList")
    preview_csv(st.session_state.outputs.get("processed"), "ptListæ›¿æ¢ç»“æœ")
    download_file(st.session_state.outputs.get("processed"), "ä¸‹è½½ processed_replaced_ptlist.csv")
    preview_csv(st.session_state.outputs.get("processed_excluded"), "æœªç­›é€‰æ•°æ®ï¼ˆå«åŸå› ï¼‰")
    download_file(st.session_state.outputs.get("processed_excluded"), "ä¸‹è½½ processed_replaced_ptlist_excluded.csv")
    if st.session_state.outputs.get("processed_excluded"):
        st.markdown("**æœªç­›é€‰åŸå› ç»Ÿè®¡**")
        try:
            excluded_df = pd.read_csv(st.session_state.outputs.get("processed_excluded"), encoding="utf-8-sig")
            if "æœªç­›é€‰åŸå› " in excluded_df.columns:
                reason_counts = (
                    excluded_df["æœªç­›é€‰åŸå› "]
                    .fillna("æœªçŸ¥")
                    .value_counts()
                    .reset_index()
                )
                reason_counts.columns = ["æœªç­›é€‰åŸå› ", "æ•°é‡"]
                safe_dataframe(reason_counts, width='stretch')
            else:
                st.info("æœªç­›é€‰æ•°æ®ä¸­æœªæ‰¾åˆ°â€œæœªç­›é€‰åŸå› â€åˆ—ã€‚")
        except Exception as exc:
            st.warning(f"æœªç­›é€‰åŸå› ç»Ÿè®¡è¯»å–å¤±è´¥ï¼š{exc}")

with st.expander("Step 5 IoUç­›é€‰", expanded=False):
    st.markdown(step_status_chip("iou_filter", "IoUç­›é€‰"), unsafe_allow_html=True)
    iou_btn = st.button(
        "ç¡®è®¤å¹¶æ‰§è¡Œ Step 5",
        disabled=not st.session_state.step_done.get("replace_ptlist"),
        key="run_iou",
        width='stretch',
    )
    if iou_btn:
        reset_downstream("iou_filter")
        high_iou_csv = output_root_path / f"high_iou_{config.get('iou_threshold', 0.98):.2f}.csv"
        other_csv = output_root_path / "other_data.csv"
        run_step(
            "iou_filter",
            "IoUç­›é€‰",
            filter_by_box_count_and_iou,
            str(st.session_state.outputs["processed"]),
            str(high_iou_csv),
            str(other_csv),
            int(config.get("min_boxes", 2)),
            float(config.get("iou_threshold", 0.98)),
        )
        st.session_state.outputs["high_iou"] = high_iou_csv
        st.session_state.outputs["other"] = other_csv
        st.session_state.step_done["iou_filter"] = True
        counts["high_iou"] = get_row_count(high_iou_csv)
        counts["other"] = get_row_count(other_csv)

    render_stats_cards([
        ("è¾“å…¥è¡Œæ•°", format_int(counts.get("processed")), "ptListæ›¿æ¢ç»“æœ"),
        ("é«˜IoUè¡Œæ•°", format_int(counts.get("high_iou")), "æ»¡è¶³é˜ˆå€¼"),
        ("å…¶ä»–è¡Œæ•°", format_int(counts.get("other")), "æœªæ»¡è¶³é˜ˆå€¼"),
        ("é«˜IoUå æ¯”", format_ratio(counts.get("high_iou"), counts.get("processed")), "é«˜IoU/è¾“å…¥"),
    ])

    show_logs("iou_filter", "IoUç­›é€‰")
    preview_csv(st.session_state.outputs.get("high_iou"), "é«˜IoUç»“æœ")
    preview_csv(st.session_state.outputs.get("other"), "å…¶ä»–æ•°æ®")
    download_file(st.session_state.outputs.get("high_iou"), "ä¸‹è½½ high_iou.csv")
    download_file(st.session_state.outputs.get("other"), "ä¸‹è½½ other_data.csv")

with st.expander("Step 5.5 æ ‡ç­¾æ›¿æ¢", expanded=False):
    st.markdown(step_status_chip("label_replace", "æ ‡ç­¾æ›¿æ¢"), unsafe_allow_html=True)
    label_map_path = st.session_state.outputs.get("label_map_path")
    label_map_ready = label_map_path is not None and Path(label_map_path).exists()
    if not label_map_ready:
        st.info("æœªä¸Šä¼ æ ‡ç­¾å¯¹ç…§è¡¨ï¼Œæ­¥éª¤å°†è‡ªåŠ¨è·³è¿‡ã€‚")
    replace_label_btn = st.button(
        "ç¡®è®¤å¹¶æ‰§è¡Œ Step 5.5",
        disabled=not st.session_state.step_done.get("iou_filter") or not label_map_ready,
        key="run_label_replace",
        width='stretch',
    )
    if replace_label_btn:
        reset_downstream("label_replace")
        replaced_csv = output_root_path / "other_data_label_replaced.csv"
        diff_path = output_root_path / "label_replace_diff.xlsx"
        unmatched_path = output_root_path / "label_replace_unmatched.xlsx"
        result = run_step(
            "label_replace",
            "æ ‡ç­¾æ›¿æ¢",
            replace_labels_by_mapping,
            str(st.session_state.outputs.get("other")),
            str(label_map_path),
            str(replaced_csv),
            config.get("label_map_sheet"),
            config.get("label_map_old_col"),
            config.get("label_map_new_col"),
            None,
            str(diff_path),
            str(unmatched_path),
            30,
        )
        st.session_state.outputs["label_replaced"] = result.get("output_csv", replaced_csv)
        st.session_state.outputs["label_replace_summary"] = result.get("summary", {})
        st.session_state.outputs["label_replace_diff"] = result.get("diff", diff_path)
        st.session_state.outputs["label_replace_unmatched"] = result.get("unmatched", unmatched_path)
        st.session_state.outputs["label_replace_sample_diff"] = result.get("sample_diff", [])
        st.session_state.step_done["label_replace"] = True
        counts["label_replaced"] = get_row_count(replaced_csv)

    summary = st.session_state.outputs.get("label_replace_summary", {})
    if summary:
        render_stats_cards([
            ("æ˜ å°„æ•°é‡", format_int(summary.get("mapping_size")), "å¯¹ç…§è¡¨æ˜ å°„æ•°"),
            ("æ›¿æ¢è¡Œæ•°", format_int(summary.get("replaced_rows")), "è‡³å°‘åŒ…å«1ä¸ªæ›¿æ¢"),
            ("æ›¿æ¢æ ‡ç­¾æ•°", format_int(summary.get("replaced_labels")), "æ›¿æ¢çš„æ ‡ç­¾æ€»æ•°"),
            ("æ— æ•ˆJSONè¡Œæ•°", format_int(summary.get("invalid_json_rows")), "æ ‡æ³¨å­—æ®µè§£æå¤±è´¥"),
            ("æœªåŒ¹é…æ ‡ç­¾æ•°", format_int(summary.get("unmatched_labels")), "å¯¹ç…§è¡¨æœªè¦†ç›–"),
        ])

    show_logs("label_replace", "æ ‡ç­¾æ›¿æ¢")
    preview_csv(st.session_state.outputs.get("label_replaced"), "æ ‡ç­¾æ›¿æ¢ç»“æœ")
    download_file(st.session_state.outputs.get("label_replaced"), "ä¸‹è½½ other_data_label_replaced.csv")
    if st.session_state.outputs.get("label_replace_unmatched"):
        st.markdown("**æœªåŒ¹é…æ ‡ç­¾ç»Ÿè®¡**")
        preview_csv(st.session_state.outputs.get("label_replace_unmatched"), "æœªåŒ¹é…æ ‡ç­¾ç»Ÿè®¡")
        download_file(st.session_state.outputs.get("label_replace_unmatched"), "ä¸‹è½½ label_replace_unmatched.xlsx")
    if st.session_state.outputs.get("label_replace_diff"):
        st.markdown("**æ ‡ç­¾æ›¿æ¢å·®å¼‚æŠ¥å‘Š**")
        preview_csv(st.session_state.outputs.get("label_replace_diff"), "æ ‡ç­¾æ›¿æ¢å·®å¼‚")
        download_file(st.session_state.outputs.get("label_replace_diff"), "ä¸‹è½½ label_replace_diff.xlsx")
    sample_diff = st.session_state.outputs.get("label_replace_sample_diff") or []
    if sample_diff:
        st.markdown("**æ›¿æ¢å‰åå¯¹æ¯”æŠ½æ ·**")
        safe_dataframe(pd.DataFrame(sample_diff), width='stretch')

with st.expander("Step 6 è§„åˆ™åˆ†ç±»æ‹†åˆ†", expanded=False):
    st.markdown(step_status_chip("split", "è§„åˆ™åˆ†ç±»æ‹†åˆ†"), unsafe_allow_html=True)
    rule_path = st.session_state.outputs.get("rule_path")
    rules_ready = rule_path is not None and Path(rule_path).exists()
    if config.get("rule_mode") == "ä¸¤åˆ—æ˜ å°„" and (not config.get("rule_label_col") or not config.get("rule_category_col")):
        rules_ready = False
        st.warning("ä¸¤åˆ—æ˜ å°„æ¨¡å¼éœ€è¦é€‰æ‹©æ ‡ç­¾åˆ—å’Œç±»åˆ«åˆ—ã€‚")
    if not rules_ready:
        st.warning("æœªæ‰¾åˆ°åˆ†ç±»è§„åˆ™æ–‡ä»¶ï¼Œè¯·åœ¨å·¦ä¾§é€‰æ‹©å¹¶ç¡®è®¤è¾“å…¥ã€‚")
    label_replace_done = st.session_state.step_done.get("label_replace")
    if not label_replace_done:
        st.warning("Step 6 å°†åŸºäº Step 5.5 æ ‡ç­¾æ›¿æ¢ç»“æœæ‰§è¡Œï¼Œè¯·å…ˆå®Œæˆ Step 5.5ã€‚")
    split_btn = st.button(
        "ç¡®è®¤å¹¶æ‰§è¡Œ Step 6",
        disabled=not label_replace_done or not rules_ready,
        key="run_split",
        width='stretch',
    )
    if split_btn:
        reset_downstream("split")
        split_dir = output_root_path / "split_by_category"
        split_input = st.session_state.outputs.get("label_replaced")
        if not split_input:
            st.error("æœªæ‰¾åˆ° Step 5.5 çš„è¾“å‡ºæ–‡ä»¶ï¼Œè¯·å…ˆæ‰§è¡Œæ ‡ç­¾æ›¿æ¢ã€‚")
            st.stop()
        result = run_step(
            "split",
            "è§„åˆ™åˆ†ç±»æ‹†åˆ†",
            split_dataset_by_rules,
            str(split_input),
            str(rule_path),
            str(split_dir),
            "wide" if config.get("rule_mode") == "å®½è¡¨(ç±»åˆ«ä¸ºåˆ—)" else "two_column",
            config.get("rule_sheet"),
            config.get("rule_label_col"),
            config.get("rule_category_col"),
            None,
            float(config.get("train_ratio", 0.8)),
            float(config.get("val_ratio", 0.1)),
            float(config.get("test_ratio", 0.1)),
            int(config.get("random_seed", 42)),
        )
        st.session_state.outputs["split_dir"] = split_dir
        st.session_state.outputs["category_files"] = result.get("category_files")
        st.session_state.outputs["unclassified"] = result.get("unclassified")
        st.session_state.outputs["split_counts"] = result.get("split_counts")
        st.session_state.outputs["classification_summary"] = result.get("summary", {})
        if st.session_state.outputs.get("unclassified"):
            summary_path = run_step(
                "unclassified_summary",
                "æ— æ³•åˆ†ç±»æ±‡æ€»",
                summarize_unclassified,
                str(st.session_state.outputs.get("unclassified")),
                str(split_dir),
                None,
            )
            st.session_state.outputs["unclassified_summary"] = summary_path
        st.session_state.step_done["split"] = True

    summary = st.session_state.outputs.get("classification_summary", {})
    render_stats_cards([
        ("å¯åˆ†ç±»æ¡æ•°", format_int(summary.get("classified")), "å¤šæ ‡ç­¾ä¼šæ‹†åˆ†æˆå¤šæ¡"),
        ("æ— æ³•åˆ†ç±»æ¡æ•°", format_int(summary.get("unclassified")), "è§ unclassified.xlsx"),
        ("ç±»åˆ«æ•°é‡", format_int(summary.get("categories")), "è§„åˆ™ä¸­åŒ¹é…åˆ°çš„ç±»åˆ«"),
    ])

    category_counts = summary.get("category_counts", {})
    if category_counts:
        st.markdown("**ç±»åˆ«æ ·æœ¬æ•°ç»Ÿè®¡**")
        count_df = pd.DataFrame(
            [{"ç±»åˆ«": k, "æ ·æœ¬æ•°": v} for k, v in category_counts.items()]
        ).sort_values("æ ·æœ¬æ•°", ascending=False)
        safe_dataframe(count_df, width='stretch')
    else:
        st.info("æš‚æ— ç±»åˆ«æ ·æœ¬ç»Ÿè®¡ã€‚")

    if st.session_state.outputs.get("category_files"):
        st.markdown("**åˆ†ç±»Excelè¾“å‡º**")
        for path in st.session_state.outputs.get("category_files", []):
            st.write(f"`{path}`")
        st.markdown("**åˆ†ç±»Excelæ¡æ•°ç»Ÿè®¡**")
        stats_rows = []
        for path in st.session_state.outputs.get("category_files", []):
            try:
                xls = pd.ExcelFile(path)
                row_counts = {}
                for split in ["train", "val", "test"]:
                    if split in xls.sheet_names:
                        df_split = pd.read_excel(path, sheet_name=split)
                        row_counts[split] = len(df_split)
                    else:
                        row_counts[split] = 0
                total = row_counts["train"] + row_counts["val"] + row_counts["test"]
                stats_rows.append({
                    "ç±»åˆ«": Path(path).stem,
                    "train": row_counts["train"],
                    "val": row_counts["val"],
                    "test": row_counts["test"],
                    "æ€»è®¡": total,
                })
            except Exception as exc:
                st.warning(f"è¯»å–åˆ†ç±»Excelå¤±è´¥ï¼š{path}ï¼ˆ{exc}ï¼‰")
        if stats_rows:
            stats_df = pd.DataFrame(stats_rows).sort_values("æ€»è®¡", ascending=False)
            safe_dataframe(stats_df, width='stretch')
    if st.session_state.outputs.get("unclassified"):
        download_file(st.session_state.outputs.get("unclassified"), "ä¸‹è½½ unclassified.xlsx")

    if st.session_state.outputs.get("split_counts"):
        download_file(st.session_state.outputs.get("split_counts"), "ä¸‹è½½ split_counts.xlsx")
        st.markdown("**æ‹†åˆ†æ¡æ•°ç»Ÿè®¡é¢„è§ˆ**")
        try:
            split_counts_df = pd.read_excel(st.session_state.outputs.get("split_counts"))
            safe_dataframe(split_counts_df, width='stretch')
            min_split = st.number_input("ä»…æ˜¾ç¤ºæ‹†åˆ†æ¡æ•° â‰¥ X çš„å›¾åƒ", min_value=1, max_value=500, value=1, step=1, key="split_min_threshold")
            chart_df = split_counts_df[["source", "æ‹†åˆ†æ¡æ•°"]].copy()
            chart_df["source"] = chart_df["source"].astype(str)
            chart_df = chart_df.sort_values("æ‹†åˆ†æ¡æ•°", ascending=False)
            filtered_chart_df = chart_df[chart_df["æ‹†åˆ†æ¡æ•°"] >= int(min_split)]
            if filtered_chart_df.empty:
                st.info("æ²¡æœ‰æ»¡è¶³æ¡ä»¶çš„å›¾åƒã€‚")

            st.markdown("**æ‹†åˆ†æœ€å¤šçš„ TOP N å›¾åƒ**")
            top_n = st.number_input("TOP N", min_value=5, max_value=50, value=10, step=5, key="split_top_n")
            top_df = filtered_chart_df.head(int(top_n)) if not filtered_chart_df.empty else chart_df.head(int(top_n))
            safe_dataframe(top_df, width='stretch')
        except Exception as exc:
            st.warning(f"split_counts è¯»å–å¤±è´¥ï¼š{exc}")

    if st.session_state.outputs.get("unclassified_summary"):
        download_file(st.session_state.outputs.get("unclassified_summary"), "ä¸‹è½½ unclassified_summary.xlsx")
        sheet_choice = st.selectbox(
            "é€‰æ‹©æ±‡æ€»è¡¨",
            options=["label_summary", "reason_summary", "reason_label"],
            index=0,
            key="unclassified_sheet_choice",
        )
        try:
            summary_path = st.session_state.outputs.get("unclassified_summary")
            summary_df = pd.read_excel(summary_path, sheet_name=sheet_choice)
            download_dataframe_excel(
                summary_df,
                f"{sheet_choice}.xlsx",
                f"ä¸‹è½½ {sheet_choice}.xlsx",
                key=f"download_{sheet_choice}",
            )
            st.caption(f"å½“å‰è¡¨ï¼š{len(summary_df)} è¡Œ Â· {len(summary_df.columns)} åˆ—")
        except Exception as exc:
            st.warning(f"æ±‡æ€»è¡¨è¯»å–å¤±è´¥ï¼š{exc}")
        st.markdown("**æ— æ³•åˆ†ç±»æ±‡æ€»é¢„è§ˆ**")
        try:
            safe_dataframe(summary_df, width='stretch')
        except Exception as exc:
            st.warning(f"{sheet_choice} è¯»å–å¤±è´¥ï¼š{exc}")

with st.expander("Step 7 ç”ŸæˆYOLOæ•°æ®é›†", expanded=False):
    st.markdown(step_status_chip("yolo", "ç”ŸæˆYOLOæ•°æ®é›†"), unsafe_allow_html=True)
    yolo_ready = st.session_state.step_done.get("split")
    progress_bar = st.progress(0.0)
    progress_text = st.empty()
    progress_text.caption("ç­‰å¾…å¼€å§‹â€¦")
    yolo_resume = st.checkbox(
        "æ–­ç‚¹ç»­å­˜ï¼ˆè·³è¿‡å·²æœ‰å›¾ç‰‡ä¸æ ‡ç­¾ï¼‰",
        value=True,
        key="yolo_resume",
    )
    yolo_btn = st.button(
        "ç¡®è®¤å¹¶æ‰§è¡Œ Step 7",
        disabled=not yolo_ready,
        key="run_yolo",
        width='stretch',
    )
    if yolo_btn:
        reset_downstream("yolo")
        yolo_dir = output_root_path / "yolo_datasets"
        def _progress_cb(done, total, downloaded, category=None, split=None, filename=None, label=None, excel=None, row_idx=None):
            if total and total > 0:
                progress_bar.progress(min(done / total, 1.0))
                extra = ""
                if category:
                    extra += f" | å½“å‰ç±»åˆ«ï¼š{category}"
                if split:
                    extra += f" | å½“å‰splitï¼š{split}"
                if filename:
                    extra += f" | å½“å‰æ–‡ä»¶ï¼š{filename}"
                if label:
                    extra += f" | å½“å‰æ ‡ç­¾ï¼š{label}"
                if excel:
                    extra += f" | å½“å‰Excelï¼š{excel}"
                if row_idx is not None:
                    extra += f" | å½“å‰è¡Œï¼š{row_idx}"
                progress_text.markdown(f"å·²å¤„ç† {done}/{total} æ¡ï¼Œå·²ä¸‹è½½ {downloaded} å¼ {extra}")
            else:
                progress_bar.progress(0.0)
                progress_text.markdown("æœªæ‰¾åˆ°å¯å¤„ç†çš„æ•°æ®")
        result = run_step(
            "yolo",
            "ç”ŸæˆYOLOæ•°æ®é›†",
            generate_yolo_datasets_from_excels,
            st.session_state.outputs.get("category_files", []),
            str(yolo_dir),
            str(yolo_dir / "image_cache"),
            "source",
            "åˆ†ç±»æ ‡ç­¾",
            "æ–°_ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®",
            "ç»“æœå­—æ®µ-ç›®æ ‡æ£€æµ‹æ ‡ç­¾é…ç½®",
            "width",
            "height",
            True,
            int(config.get("random_seed", 42)),
            config.get("class_order") or None,
            yolo_resume,
            _progress_cb,
        )
        st.session_state.outputs["yolo_dir"] = yolo_dir
        st.session_state.outputs["yolo_datasets"] = result.get("datasets")
        st.session_state.outputs["yolo_skipped"] = result.get("skipped")
        st.session_state.outputs["yolo_stats"] = result.get("stats", {})
        st.session_state.outputs["yolo_progress"] = {
            "total": result.get("total"),
            "processed": result.get("processed"),
            "downloaded": result.get("downloaded"),
        }
        st.session_state.outputs["yolo_dataset_name_map"] = result.get("dataset_name_map", {})
        yolo_label_stats, yolo_label_df = summarize_yolo_label_counts(result.get("datasets"))
        st.session_state.outputs["yolo_label_stats"] = yolo_label_stats
        st.session_state.outputs["yolo_label_df"] = yolo_label_df
        st.session_state.step_done["yolo"] = True

    if st.session_state.outputs.get("yolo_dir"):
        st.write(f"YOLOæ•°æ®é›†è¾“å‡ºç›®å½•ï¼š`{st.session_state.outputs.get('yolo_dir')}`")
    if st.session_state.outputs.get("yolo_datasets"):
        st.markdown("**å·²ç”Ÿæˆçš„æ•°æ®é›†**")
        for path in st.session_state.outputs.get("yolo_datasets", []):
            st.write(f"`{path}`")
    yolo_progress = st.session_state.outputs.get("yolo_progress")
    if yolo_progress:
        render_stats_cards([
            ("æ€»æ¡æ•°", format_int(yolo_progress.get("total")), "å¾…å¤„ç†æ•°æ®"),
            ("å·²å¤„ç†", format_int(yolo_progress.get("processed")), "å·²å®Œæˆè½¬æ¢"),
            ("å·²ä¸‹è½½", format_int(yolo_progress.get("downloaded")), "å›¾åƒæˆåŠŸå†™å…¥"),
        ])
    yolo_stats = st.session_state.outputs.get("yolo_stats", {})
    if yolo_stats:
        st.markdown("**å„ç±»åˆ«æ‹†åˆ†ç»Ÿè®¡**")
        stats_rows = []
        for category, splits in yolo_stats.items():
            stats_rows.append({
                "ç±»åˆ«": category,
                "train": splits.get("train", 0),
                "val": splits.get("val", 0),
                "test": splits.get("test", 0),
                "æ€»è®¡": splits.get("train", 0) + splits.get("val", 0) + splits.get("test", 0),
            })
        stats_df = pd.DataFrame(stats_rows).sort_values("æ€»è®¡", ascending=False)
        safe_dataframe(stats_df, width='stretch')
    yolo_label_stats = st.session_state.outputs.get("yolo_label_stats", {})
    yolo_label_df = st.session_state.outputs.get("yolo_label_df")
    yolo_dataset_name_map = st.session_state.outputs.get("yolo_dataset_name_map", {})
    if yolo_label_stats:
        st.markdown("**YOLOæ•°æ®é›†æ ‡ç­¾ç»Ÿè®¡ï¼ˆæŒ‰å›¾ç‰‡æ•°ï¼‰**")
        if yolo_label_df is not None and not yolo_label_df.empty:
            st.markdown("**å¯¼å‡ºæ ‡ç­¾ç»Ÿè®¡**")
            download_dataframe_excel(
                yolo_label_df,
                f"yolo_label_stats_{st.session_state.run_id}.xlsx",
                "ä¸‹è½½ æ ‡ç­¾ç»Ÿè®¡ Excel",
                key="download_yolo_label_excel",
            )
            csv_bytes = yolo_label_df.to_csv(index=False, encoding="utf-8-sig")
            st.download_button(
                label="ä¸‹è½½ æ ‡ç­¾ç»Ÿè®¡ CSV",
                data=csv_bytes.encode("utf-8-sig"),
                file_name=f"yolo_label_stats_{st.session_state.run_id}.csv",
                mime="text/csv",
            )
        for dataset_name, split_stats in yolo_label_stats.items():
            display_name = yolo_dataset_name_map.get(dataset_name) or dataset_name
            title = f"{display_name} æ ‡ç­¾ç»Ÿè®¡"
            if display_name != dataset_name:
                title = f"{display_name}ï¼ˆ{dataset_name}ï¼‰ æ ‡ç­¾ç»Ÿè®¡"
            with st.expander(title, expanded=False):
                for split in ["train", "val", "test", "all"]:
                    split_info = split_stats.get(split, {})
                    total_images = split_info.get("total_images", 0)
                    img_counts = split_info.get("label_counts", {})
                    box_counts = split_info.get("box_counts", {})
                    split_label = "æ±‡æ€»" if split == "all" else split
                    st.markdown(f"**{split_label} æ ‡ç­¾ç»Ÿè®¡**")
                    st.caption(f"splitæ€»å›¾ç‰‡æ•°ï¼š{total_images}")
                    if img_counts or box_counts:
                        rows = []
                        all_labels = set(img_counts) | set(box_counts)
                        for label in all_labels:
                            img_count = img_counts.get(label, 0)
                            box_count = box_counts.get(label, 0)
                            ratio = (img_count / total_images) if total_images else 0.0
                            rows.append({
                                "æ ‡ç­¾": label,
                                "å›¾ç‰‡æ•°é‡": img_count,
                                "æ ‡æ³¨æ¡†æ•°é‡": box_count,
                                "å æ¯”": f"{ratio * 100:.1f}%",
                            })
                        df = pd.DataFrame(rows).sort_values("å›¾ç‰‡æ•°é‡", ascending=False)
                        safe_dataframe(df, width='stretch')
                    else:
                        st.info("æš‚æ— æ ‡ç­¾æ•°æ®ã€‚")
    if st.session_state.outputs.get("yolo_skipped"):
        download_file(st.session_state.outputs.get("yolo_skipped"), "ä¸‹è½½ yolo_skipped.xlsx")

with st.expander("Step 8 ä¸‹è½½å¹¶ç»˜åˆ¶æ ‡æ³¨å›¾ç‰‡", expanded=False):
    if config.get("run_download"):
        st.markdown(step_status_chip("download", "ä¸‹è½½å¹¶ç»˜åˆ¶æ ‡æ³¨å›¾ç‰‡"), unsafe_allow_html=True)
        download_btn = st.button(
            "ç¡®è®¤å¹¶æ‰§è¡Œ Step 8",
            disabled=not st.session_state.step_done.get("yolo"),
            key="run_download",
            width='stretch',
        )
        if download_btn:
            draw_input = st.session_state.outputs.get("label_replaced") or st.session_state.outputs.get("other")
            run_step(
                "download",
                "ä¸‹è½½å¹¶ç»˜åˆ¶æ ‡æ³¨å›¾ç‰‡",
                download_and_draw_annotations,
                str(draw_input),
                str(output_root_path),
                None,
                None,
                config.get("max_images"),
                15,
            )
            st.session_state.outputs["download_dir"] = output_root_path / "downloaded_images"
            st.session_state.outputs["annotated_dir"] = output_root_path / "annotated_images"
            st.session_state.step_done["download"] = True

        image_count = get_image_count(st.session_state.outputs.get("annotated_dir"))
        render_stats_cards([
            ("è¾“å‡ºå›¾ç‰‡æ•°", format_int(image_count), "æ ‡æ³¨å›¾ç‰‡"),
            ("é™åˆ¶æ•°é‡", format_int(config.get("max_images") or 0), "0è¡¨ç¤ºä¸é™"),
        ])

        show_logs("download", "ä¸‹è½½å¹¶ç»˜åˆ¶æ ‡æ³¨å›¾ç‰‡")

        annotated_dir = st.session_state.outputs.get("annotated_dir")
        if annotated_dir and annotated_dir.exists():
            images = list(annotated_dir.glob("*"))[:12]
            if images:
                st.image([str(p) for p in images], caption=[p.name for p in images])
            else:
                st.info("æš‚æ— æ ‡æ³¨å›¾ç‰‡å¯é¢„è§ˆã€‚")
    else:
        st.markdown("<span class=\"chip chip-skip\">å›¾ç‰‡æ ‡æ³¨ Â· å·²è·³è¿‡</span>", unsafe_allow_html=True)
        st.info("å·²å…³é—­å›¾ç‰‡æ ‡æ³¨æ­¥éª¤ï¼ŒStep 8 è‡ªåŠ¨è·³è¿‡ã€‚")

st.markdown("---")

st.markdown("**ç»“æœå¯¼å‡ºåŒº**")
export_left, export_right = st.columns([2, 1])
with export_left:
    only_classification = st.checkbox("åªæ‰“åŒ…åˆ†ç±»ç»“æœ", value=False)
    include_yolo = st.checkbox("æ‰“åŒ…YOLOæ•°æ®é›†", value=False)
    include_images = st.checkbox(
        "æ‰“åŒ…åŒ…å«å›¾ç‰‡ï¼ˆä¸‹è½½åŸå›¾ä¸æ ‡æ³¨å›¾ï¼‰",
        value=False,
        disabled=not config.get("run_download") or only_classification,
    )
    if only_classification:
        st.info("å½“å‰ä»…å¯¼å‡ºåˆ†ç±»ç»“æœï¼ˆç±»åˆ«Excel + unclassifiedï¼‰ã€‚")
    elif include_images and not config.get("run_download"):
        st.info("æœªå¯ç”¨å›¾ç‰‡æ ‡æ³¨æ­¥éª¤ï¼Œæ— æ³•åŒ…å«å›¾ç‰‡ã€‚")
with export_right:
    zip_buffer = build_export_zip(
        st.session_state.outputs,
        include_images=include_images,
        only_classification=only_classification,
    )
    zip_name = "classification_only" if only_classification else "yolo_pipeline"
    st.download_button(
        label="ä¸‹è½½å…¨éƒ¨ç»“æœ ZIP",
        data=zip_buffer,
        file_name=f"{zip_name}_{st.session_state.run_id}.zip",
        mime="application/zip",
        width='stretch',
    )
    if include_yolo:
        yolo_zip = build_yolo_zip(st.session_state.outputs.get("yolo_dir"))
        if yolo_zip:
            st.download_button(
                label="ä¸‹è½½ YOLO æ•°æ®é›† ZIP",
                data=yolo_zip,
                file_name=f"yolo_dataset_{st.session_state.run_id}.zip",
                mime="application/zip",
                width='stretch',
            )
        else:
            st.info("å°šæœªç”Ÿæˆ YOLO æ•°æ®é›†ã€‚")

st.markdown("---")
st.markdown("**æµç¨‹æ—¥å¿—æ±‡æ€»**")
log_steps = [
    ("merge", "åˆå¹¶CSV"),
    ("dedup", "æŒ‰sourceå»é‡"),
    ("ref_filter", "å‚è€ƒCSVå»é‡"),
    ("update_ref", "è¦†ç›–reference.csv"),
    ("replace_ptlist", "æ›¿æ¢ptList"),
    ("iou_filter", "IoUç­›é€‰"),
    ("split", "è§„åˆ™åˆ†ç±»æ‹†åˆ†"),
    ("unclassified_summary", "æ— æ³•åˆ†ç±»æ±‡æ€»"),
    ("yolo", "ç”ŸæˆYOLOæ•°æ®é›†"),
    ("download", "ä¸‹è½½å¹¶ç»˜åˆ¶æ ‡æ³¨å›¾ç‰‡"),
]
visible_tabs = []
visible_labels = []
for key, label in log_steps:
    if key == "ref_filter" and not config.get("use_reference"):
        continue
    if key == "update_ref" and not config.get("update_reference"):
        continue
    if key == "download" and not config.get("run_download"):
        continue
    visible_tabs.append(key)
    visible_labels.append(label)

if visible_tabs:
    tabs = st.tabs(visible_labels)
    for idx, key in enumerate(visible_tabs):
        with tabs[idx]:
            log_text = st.session_state.logs.get(key)
            if log_text:
                st.text_area("æ—¥å¿—è¾“å‡º", log_text, height=220)
            else:
                st.info("æš‚æ— æ—¥å¿—ã€‚")
